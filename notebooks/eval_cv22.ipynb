{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4fbd961",
   "metadata": {},
   "source": [
    "# Eval Cv22\n",
    "- now legacy code as the code of this notebook is refactored into eval scripts\n",
    "- this notebook is for initial exploration of cv22 dataset and building eval pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0361487",
   "metadata": {},
   "source": [
    "## Variables, file utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "062df8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok. Reading from .env file\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "REPO_ROOT = Path('..').resolve()\n",
    "if str(REPO_ROOT) not in sys.path:\n",
    "    sys.path.append(str(REPO_ROOT))\n",
    "\n",
    "load_dotenv(override=True)\n",
    "print(os.getenv(\"TEST\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22793bc",
   "metadata": {},
   "source": [
    "## CV22 manager class\n",
    "- simple class to easily work with Common Voice dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "422c83f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from audio_eval import CommonVoiceDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f7800987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 languages: ['ru', 'it', 'en', 'es', 'ja']...\n",
      "ru: {'total_samples': 10244, 'available_splits': ['train', 'dev', 'test', 'validated', 'invalidated', 'other', 'reported'], 'unique_speakers': 2033, 'avg_up_votes': 2.0807301835220615, 'high_quality_samples': 10244}\n",
      "it: {'total_samples': 15177, 'available_splits': ['train', 'dev', 'test', 'validated', 'invalidated', 'other', 'reported'], 'unique_speakers': 3848, 'avg_up_votes': 2.1311853462476114, 'high_quality_samples': 15177}\n",
      "en: {'total_samples': 16396, 'available_splits': ['train', 'dev', 'test', 'validated', 'invalidated', 'other', 'reported'], 'unique_speakers': 12082, 'avg_up_votes': 2.2098072700658697, 'high_quality_samples': 16396}\n",
      "es: {'total_samples': 15893, 'available_splits': ['train', 'dev', 'test', 'validated', 'invalidated', 'other', 'reported'], 'unique_speakers': 6550, 'avg_up_votes': 2.0812936512930222, 'high_quality_samples': 15893}\n",
      "ja: {'total_samples': 8004, 'available_splits': ['train', 'dev', 'test', 'validated', 'invalidated', 'other', 'reported'], 'unique_speakers': 2015, 'avg_up_votes': 3.505247376311844, 'high_quality_samples': 8004}\n",
      "de: {'total_samples': 16196, 'available_splits': ['train', 'dev', 'test', 'validated', 'invalidated', 'other', 'reported'], 'unique_speakers': 5030, 'avg_up_votes': 2.1308347740182763, 'high_quality_samples': 16196}\n",
      "pl: {'total_samples': 9819, 'available_splits': ['train', 'dev', 'test', 'validated', 'invalidated', 'other', 'reported'], 'unique_speakers': 2442, 'avg_up_votes': 2.569915470007129, 'high_quality_samples': 9819}\n",
      "pt: {'total_samples': 9641, 'available_splits': ['train', 'dev', 'test', 'validated', 'invalidated', 'other', 'reported'], 'unique_speakers': 2162, 'avg_up_votes': 2.120423192614874, 'high_quality_samples': 9641}\n",
      "fr: {'total_samples': 16186, 'available_splits': ['train', 'dev', 'test', 'validated', 'invalidated', 'other', 'reported'], 'unique_speakers': 5147, 'avg_up_votes': 2.0838378845916226, 'high_quality_samples': 16186}\n",
      "zh-CN: {'total_samples': 10635, 'available_splits': ['train', 'dev', 'test', 'validated', 'invalidated', 'other', 'reported'], 'unique_speakers': 2811, 'avg_up_votes': 2.062905500705219, 'high_quality_samples': 10635}\n"
     ]
    }
   ],
   "source": [
    "# print size of all the languages in cv22\n",
    "cv = CommonVoiceDataset(os.getenv(\"CV22_PATH\"))\n",
    "all_stats = {}\n",
    "for lang in cv.languages:\n",
    "    stats = cv.get_language_stats(lang, split=\"test\")\n",
    "    all_stats[lang] = stats\n",
    "    print(f\"{lang}: {stats}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7d0f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the class\n",
    "cv_dataset = CommonVoiceDataset(os.getenv(\"CV22_PATH\"))\n",
    "print(cv_dataset.languages)\n",
    "\n",
    "# get the first 5 samples of english test set\n",
    "samples_df = cv_dataset.get_samples(\"en\", split=\"test\", n_samples=5)\n",
    "\n",
    "# get the first sample of english test set with audio path and metadata\n",
    "sample_with_audio = cv_dataset.get_sample_with_audio(\"en\", samples_df.iloc[0])\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(sample_with_audio[\"audio_path\"])\n",
    "pprint(sample_with_audio[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2ef790",
   "metadata": {},
   "source": [
    "## Sample Transcriptions endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "724cb70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install openai groq speechmatics-python elevenlabs jiwer -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55aae8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "SAMPLE_AUDIO_FILE = \"/root/data/common_voice_22/cv-corpus-22.0-2025-06-20/en/clips/common_voice_en_77702.mp3\"\n",
    "SAMPLE_TEXT = \"He was thinking about omens, and someone had appeared.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576b46dc",
   "metadata": {},
   "source": [
    "### OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f81b894c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He was thinking about omens, and someone had appeared.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "audio_file = open(SAMPLE_AUDIO_FILE, \"rb\")\n",
    "\n",
    "transcription = client.audio.transcriptions.create(model=\"gpt-4o-transcribe\", file=audio_file)\n",
    "\n",
    "print(transcription.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a63c3f",
   "metadata": {},
   "source": [
    "### Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "821bf051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " He was thinking about omens and someone had appeared.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "\n",
    "client = Groq()\n",
    "filename = SAMPLE_AUDIO_FILE\n",
    "\n",
    "with open(filename, \"rb\") as file:\n",
    "    transcription = client.audio.transcriptions.create(\n",
    "        file=(filename, file.read()),\n",
    "        model=\"whisper-large-v3-turbo\",\n",
    "        response_format=\"verbose_json\",\n",
    "    )\n",
    "    print(transcription.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b856344d",
   "metadata": {},
   "source": [
    "### ElevenLabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991a01a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elevenlabs import ElevenLabs\n",
    "\n",
    "# Initialize client\n",
    "client = ElevenLabs(api_key=os.getenv(\"ELEVENLABS_API_KEY\"))\n",
    "\n",
    "# Open your audio file and send to API\n",
    "with open(SAMPLE_AUDIO_FILE, \"rb\") as f:\n",
    "    transcript = client.speech_to_text.convert(\n",
    "        file=f,\n",
    "        model_id=\"scribe_v1\",  # or \"scribe_v1_experimental\"\n",
    "        language_code=\"en\",  # optional\n",
    "    )\n",
    "\n",
    "# 'This request exceeds your API key quota of 10. You have 0 credits remaining, while 5 credits are required for this request.'\n",
    "print(transcript.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea294ac6",
   "metadata": {},
   "source": [
    "### Spechmatics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cdc9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/speechmatics/speechmatics-python-sdk\n",
    "\n",
    "from speechmatics.models import ConnectionSettings, BatchTranscriptionConfig\n",
    "from speechmatics.batch_client import BatchClient\n",
    "from httpx import HTTPStatusError\n",
    "\n",
    "API_KEY = os.getenv(\"SPEECHMATICS_API_KEY\")\n",
    "PATH_TO_FILE = SAMPLE_AUDIO_FILE\n",
    "LANGUAGE = \"en\"\n",
    "\n",
    "settings = ConnectionSettings(\n",
    "    url=\"https://asr.api.speechmatics.com/v2\",  # Batch API endpoint\n",
    "    auth_token=API_KEY,\n",
    ")\n",
    "\n",
    "with BatchClient(settings) as client:\n",
    "    try:\n",
    "        # Submit job\n",
    "        job_id = client.submit_job(PATH_TO_FILE, BatchTranscriptionConfig(language=LANGUAGE))\n",
    "        print(f\"Job {job_id} submitted. Waiting for completion...\")\n",
    "\n",
    "        # Wait for results (txt, json-v2, srt, etc.)\n",
    "        transcript = client.wait_for_completion(job_id, transcription_format=\"txt\")\n",
    "        print(\"Transcript:\\n\", transcript)\n",
    "\n",
    "    except HTTPStatusError as e:\n",
    "        if e.response.status_code == 401:\n",
    "            print(\"Invalid API key – check your API_KEY.\")\n",
    "        elif e.response.status_code == 400:\n",
    "            print(\"Bad request:\", e.response.json().get(\"detail\"))\n",
    "        else:\n",
    "            raise e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e002477",
   "metadata": {},
   "source": [
    "### Gladia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e57df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import os\n",
    "\n",
    "API_KEY = os.getenv(\"GLADIA_API_KEY\")\n",
    "AUDIO_FILE = SAMPLE_AUDIO_FILE\n",
    "\n",
    "headers = {\"x-gladia-key\": API_KEY}\n",
    "\n",
    "# 1. Upload audio file\n",
    "with open(AUDIO_FILE, \"rb\") as f:\n",
    "    resp = requests.post(\n",
    "        \"https://api.gladia.io/v2/upload\",\n",
    "        headers=headers,\n",
    "        files={\"audio\": (os.path.basename(AUDIO_FILE), f, \"audio/wav\")},\n",
    "    )\n",
    "resp.raise_for_status()\n",
    "file_url = resp.json()[\"audio_url\"]\n",
    "print(\"Uploaded:\", file_url)\n",
    "\n",
    "# 2. Request transcription\n",
    "payload = {\"audio_url\": file_url}\n",
    "resp = requests.post(\n",
    "    \"https://api.gladia.io/v2/pre-recorded\", headers={**headers, \"Content-Type\": \"application/json\"}, json=payload\n",
    ")\n",
    "resp.raise_for_status()\n",
    "job = resp.json()\n",
    "job_id, result_url = job[\"id\"], job[\"result_url\"]\n",
    "print(\"Job ID:\", job_id)\n",
    "\n",
    "# 3. Poll until done\n",
    "while True:\n",
    "    resp = requests.get(result_url, headers=headers)\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "    if data[\"status\"] == \"done\":\n",
    "        transcript = data[\"result\"][\"transcription\"][\"full_transcript\"]\n",
    "        print(\"\\nTranscription:\\n\", transcript)\n",
    "        break\n",
    "    elif data[\"status\"] == \"error\":\n",
    "        print(\"Error:\", data)\n",
    "        break\n",
    "    else:\n",
    "        time.sleep(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc23ab3",
   "metadata": {},
   "source": [
    "## ✅ Transcription functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da04db87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from audio_eval import (\n",
    "    DEFAULT_SERVICE_MODELS,\n",
    "    build_service_function_map,\n",
    "    transcribe_elevenlabs,\n",
    "    transcribe_gladia,\n",
    "    transcribe_groq,\n",
    "    transcribe_menlo,\n",
    "    transcribe_openai,\n",
    "    transcribe_speechmatics,\n",
    "    transcribe_vllm,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4756ff2",
   "metadata": {},
   "source": [
    "## ✅ Text normalization functions\n",
    "- loot from Huggingface - OpenASRLeaderboard repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "678e17c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from audio_eval import (\n",
    "    DEFAULT_NORMALIZER,\n",
    "    english_normalizer,\n",
    "    get_text_normalizer,\n",
    "    multilingual_normalizer,\n",
    ")\n",
    "\n",
    "text_normalizer = DEFAULT_NORMALIZER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80d4cff",
   "metadata": {},
   "source": [
    "## Service Transcribe Functions\n",
    "\n",
    "Define transcribe functions for each ASR service with the same interface: `transcribe(audio_path)` -> str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60548ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SERVICES: ['menlo_large-v3', 'vllm_large-v3', 'openai_gpt-4o-transcribe', 'openai_gpt-4o-mini-transcribe', 'openai_whisper-1', 'groq_whisper-large-v3', 'groq_whisper-large-v3-turbo', 'elevenlabs_scribe_v1', 'gladia_pre-recorded']\n",
      "Len SERVICES: 9\n"
     ]
    }
   ],
   "source": [
    "# Service and model configurations\n",
    "SERVICE_MODELS = list(DEFAULT_SERVICE_MODELS)\n",
    "\n",
    "# Create service functions with models\n",
    "SERVICE_FUNCS = build_service_function_map(SERVICE_MODELS)\n",
    "SERVICES = list(SERVICE_FUNCS.keys())\n",
    "print(f\"SERVICES: {SERVICES}\")\n",
    "print(f\"Len SERVICES: {len(SERVICES)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d4ea0f",
   "metadata": {},
   "source": [
    "## WER Evaluation Pipeline\n",
    "\n",
    "Functions for running WER evaluation across multiple services with retry, error handling, and checkpointing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9c5b4164",
   "metadata": {},
   "outputs": [],
   "source": [
    "from audio_eval import run_wer_evaluation, run_wer_evaluation_parallel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2e0050",
   "metadata": {},
   "source": [
    "## Run WER Evaluation\n",
    "\n",
    "Example usage of the WER evaluation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "37a09a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing language: en\n",
      "Processing language: en\n",
      "Processing language: en\n",
      "Processing language: en\n",
      "Processing language: en\n",
      "Evaluating 8 services on en\n",
      "Evaluating 8 services on en\n",
      "Evaluating 8 services on en\n",
      "Evaluating 8 services on en\n",
      "Evaluating 8 services on en\n",
      "Evaluating 8 services on en\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of services: 8\n",
      "Services: ['menlo_large-v3', 'openai_gpt-4o-transcribe', 'openai_gpt-4o-mini-transcribe', 'openai_whisper-1', 'groq_whisper-large-v3', 'groq_whisper-large-v3-turbo', 'elevenlabs_scribe_v1', 'gladia_pre-recorded']\n",
      "Found 10 languages: ['ru', 'it', 'en', 'es', 'ja']...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "menlo_large-v3 on en: WER=0.0000, Avg Time=0.50s, Samples=1\n",
      "menlo_large-v3 on en: WER=0.0000, Avg Time=0.50s, Samples=1\n",
      "menlo_large-v3 on en: WER=0.0000, Avg Time=0.50s, Samples=1\n",
      "openai_gpt-4o-transcribe on en: WER=0.0000, Avg Time=1.29s, Samples=1\n",
      "openai_gpt-4o-transcribe on en: WER=0.0000, Avg Time=1.29s, Samples=1\n",
      "openai_gpt-4o-transcribe on en: WER=0.0000, Avg Time=1.29s, Samples=1\n",
      "openai_gpt-4o-mini-transcribe on en: WER=0.0000, Avg Time=1.44s, Samples=1\n",
      "openai_gpt-4o-mini-transcribe on en: WER=0.0000, Avg Time=1.44s, Samples=1\n",
      "openai_gpt-4o-mini-transcribe on en: WER=0.0000, Avg Time=1.44s, Samples=1\n",
      "openai_whisper-1 on en: WER=0.0000, Avg Time=1.14s, Samples=1\n",
      "menlo_large-v3 on en: WER=0.0000, Avg Time=0.50s, Samples=1\n",
      "menlo_large-v3 on en: WER=0.0000, Avg Time=0.50s, Samples=1\n",
      "openai_gpt-4o-transcribe on en: WER=0.0000, Avg Time=1.29s, Samples=1\n",
      "openai_gpt-4o-transcribe on en: WER=0.0000, Avg Time=1.29s, Samples=1\n",
      "openai_gpt-4o-transcribe on en: WER=0.0000, Avg Time=1.29s, Samples=1\n",
      "openai_gpt-4o-mini-transcribe on en: WER=0.0000, Avg Time=1.44s, Samples=1\n",
      "openai_gpt-4o-mini-transcribe on en: WER=0.0000, Avg Time=1.44s, Samples=1\n",
      "openai_gpt-4o-mini-transcribe on en: WER=0.0000, Avg Time=1.44s, Samples=1\n",
      "openai_whisper-1 on en: WER=0.0000, Avg Time=1.14s, Samples=1\n",
      "openai_whisper-1 on en: WER=0.0000, Avg Time=1.14s, Samples=1\n",
      "openai_whisper-1 on en: WER=0.0000, Avg Time=1.14s, Samples=1\n",
      "groq_whisper-large-v3 on en: WER=0.0000, Avg Time=0.60s, Samples=1\n",
      "groq_whisper-large-v3 on en: WER=0.0000, Avg Time=0.60s, Samples=1\n",
      "groq_whisper-large-v3 on en: WER=0.0000, Avg Time=0.60s, Samples=1\n",
      "groq_whisper-large-v3-turbo on en: WER=0.0000, Avg Time=0.62s, Samples=1\n",
      "groq_whisper-large-v3-turbo on en: WER=0.0000, Avg Time=0.62s, Samples=1\n",
      "groq_whisper-large-v3-turbo on en: WER=0.0000, Avg Time=0.62s, Samples=1\n",
      "elevenlabs_scribe_v1 on en: WER=0.0000, Avg Time=4.92s, Samples=1\n",
      "elevenlabs_scribe_v1 on en: WER=0.0000, Avg Time=4.92s, Samples=1\n",
      "elevenlabs_scribe_v1 on en: WER=0.0000, Avg Time=4.92s, Samples=1\n",
      "gladia_pre-recorded on en: WER=0.0000, Avg Time=18.64s, Samples=1\n",
      "gladia_pre-recorded on en: WER=0.0000, Avg Time=18.64s, Samples=1\n",
      "gladia_pre-recorded on en: WER=0.0000, Avg Time=18.64s, Samples=1\n",
      "openai_whisper-1 on en: WER=0.0000, Avg Time=1.14s, Samples=1\n",
      "openai_whisper-1 on en: WER=0.0000, Avg Time=1.14s, Samples=1\n",
      "groq_whisper-large-v3 on en: WER=0.0000, Avg Time=0.60s, Samples=1\n",
      "groq_whisper-large-v3 on en: WER=0.0000, Avg Time=0.60s, Samples=1\n",
      "groq_whisper-large-v3 on en: WER=0.0000, Avg Time=0.60s, Samples=1\n",
      "groq_whisper-large-v3-turbo on en: WER=0.0000, Avg Time=0.62s, Samples=1\n",
      "groq_whisper-large-v3-turbo on en: WER=0.0000, Avg Time=0.62s, Samples=1\n",
      "groq_whisper-large-v3-turbo on en: WER=0.0000, Avg Time=0.62s, Samples=1\n",
      "elevenlabs_scribe_v1 on en: WER=0.0000, Avg Time=4.92s, Samples=1\n",
      "elevenlabs_scribe_v1 on en: WER=0.0000, Avg Time=4.92s, Samples=1\n",
      "elevenlabs_scribe_v1 on en: WER=0.0000, Avg Time=4.92s, Samples=1\n",
      "gladia_pre-recorded on en: WER=0.0000, Avg Time=18.64s, Samples=1\n",
      "gladia_pre-recorded on en: WER=0.0000, Avg Time=18.64s, Samples=1\n",
      "gladia_pre-recorded on en: WER=0.0000, Avg Time=18.64s, Samples=1\n",
      "Evaluation completed\n",
      "Evaluation completed\n",
      "Evaluation completed\n",
      "Evaluation completed\n",
      "Evaluation completed\n",
      "Evaluation completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final results:\n",
      "en:\n",
      "  menlo_large-v3: WER=0.0000, Avg Time=0.50s, Samples=1\n",
      "  openai_gpt-4o-transcribe: WER=0.0000, Avg Time=1.29s, Samples=1\n",
      "  openai_gpt-4o-mini-transcribe: WER=0.0000, Avg Time=1.44s, Samples=1\n",
      "  openai_whisper-1: WER=0.0000, Avg Time=1.14s, Samples=1\n",
      "  groq_whisper-large-v3: WER=0.0000, Avg Time=0.60s, Samples=1\n",
      "  groq_whisper-large-v3-turbo: WER=0.0000, Avg Time=0.62s, Samples=1\n",
      "  elevenlabs_scribe_v1: WER=0.0000, Avg Time=4.92s, Samples=1\n",
      "  gladia_pre-recorded: WER=0.0000, Avg Time=18.64s, Samples=1\n"
     ]
    }
   ],
   "source": [
    "# Run evaluation for all services on 1 English sample (PARALLEL VERSION)\n",
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "print(f\"Number of services: {len(SERVICES)}\")\n",
    "print(f\"Services: {SERVICES}\")\n",
    "results = run_wer_evaluation_parallel(\n",
    "    dataset_path=os.getenv(\"CV22_PATH\"),\n",
    "    services=SERVICES,  # All services\n",
    "    service_funcs=SERVICE_FUNCS,\n",
    "    languages=[\"en\"],  # English only\n",
    "    results_file=f\"results/wer_results_{timestamp}.json\",\n",
    "    checkpoint_file=f\"results/wer_checkpoint_{timestamp}.json\",\n",
    "    log_file=f\"logs/wer_eval_{timestamp}.log\",\n",
    "    max_retries=2,  # Fewer retries for testing\n",
    "    n_samples=1,  # 1 sample\n",
    "    max_workers=8,  # Run up to 8 services in parallel per sample\n",
    "    normalizer_resolver=get_text_normalizer,\n",
    ")\n",
    "\n",
    "print(\"\\nFinal results:\")\n",
    "for lang, data in results.items():\n",
    "    print(f\"{lang}:\")\n",
    "    for service, metrics in data.items():\n",
    "        print(\n",
    "            f\"  {service}: WER={metrics['wer']:.4f}, Avg Time={metrics['timing']:.2f}s, Samples={metrics['n_samples']}\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda9efec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation for ALL languages, 50 samples each, all services (PARALLEL VERSION)\n",
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Get all available languages\n",
    "cv_dataset = CommonVoiceDataset(os.getenv(\"CV22_PATH\"))\n",
    "all_languages = cv_dataset.languages\n",
    "print(f\"Found {len(all_languages)} languages: {all_languages}\")\n",
    "\n",
    "print(f\"Number of services: {len(SERVICES)}\")\n",
    "print(f\"Services: {SERVICES}\")\n",
    "\n",
    "results = run_wer_evaluation_parallel(\n",
    "    dataset_path=os.getenv(\"CV22_PATH\"),\n",
    "    services=SERVICES,  # All services\n",
    "    service_funcs=SERVICE_FUNCS,\n",
    "    languages=all_languages,  # All languages\n",
    "    results_file=f\"results/wer_results_all_{timestamp}.json\",\n",
    "    checkpoint_file=f\"results/wer_checkpoint_all_{timestamp}.json\",\n",
    "    log_file=f\"logs/wer_eval_all_{timestamp}.log\",\n",
    "    max_retries=2,  # Fewer retries for testing\n",
    "    n_samples=50,  # 50 samples per language\n",
    "    max_workers=8,  # Run up to 8 services in parallel per sample\n",
    "    normalizer_resolver=get_text_normalizer,\n",
    ")\n",
    "\n",
    "print(\"\\nFinal results:\")\n",
    "for lang, data in results.items():\n",
    "    print(f\"\\n{lang.upper()}:\")\n",
    "    for service, metrics in data.items():\n",
    "        print(\n",
    "            f\"  {service}: WER={metrics['wer']:.4f}, Avg Time={metrics['timing']:.2f}s, Samples={metrics['n_samples']}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2f5999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing language-specific normalizers:\n",
      "EN: He was thinking about omens, and someone had appeared.\n",
      "       -> he was thinking about omens and someone had appeared\n",
      "\n",
      "ES: Él estaba pensando en augurios, y alguien había aparecido.\n",
      "       -> el estaba pensando en augurios y alguien habia aparecido\n",
      "\n",
      "FR: Il pensait aux présages, et quelqu'un était apparu.\n",
      "       -> il pensait aux presages et quelqu un etait apparu\n",
      "\n",
      "DE: Er dachte an Omen, und jemand war erschienen.\n",
      "       -> er dachte an omen und jemand war erschienen\n",
      "\n",
      "JA: 彼は前兆について考えていて、誰かが現れた。\n",
      "       -> 彼は前兆について考えていて 誰かか現れた\n",
      "\n",
      "ZH: 他在思考预兆，有人出现了。\n",
      "       -> 他在思考预兆 有人出现了\n",
      "\n",
      "RU: Он думал о предзнаменованиях, и кто-то появился.\n",
      "       -> он думал о предзнаменованиях и кто то появился\n",
      "\n",
      "AR: كان يفكر في النذير، وظهر شخص ما.\n",
      "       -> كان يفكر في النذير وظهر شخص ما\n",
      "\n",
      "HI: वह पूर्वाभास के बारे में सोच रहा था, और कोई प्रकट हुआ था।\n",
      "       -> वह परव भ स क ब र म स च रह थ और क ई परकट हआ थ\n",
      "\n",
      "PT: Ele estava pensando em presságios, e alguém havia aparecido.\n",
      "       -> ele estava pensando em pressagios e alguem havia aparecido\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test language-specific normalizers\n",
    "test_texts = {\n",
    "    \"en\": \"He was thinking about omens, and someone had appeared.\",\n",
    "    \"es\": \"Él estaba pensando en augurios, y alguien había aparecido.\",\n",
    "    \"fr\": \"Il pensait aux présages, et quelqu'un était apparu.\",\n",
    "    \"de\": \"Er dachte an Omen, und jemand war erschienen.\",\n",
    "    \"ja\": \"彼は前兆について考えていて、誰かが現れた。\",\n",
    "    \"zh\": \"他在思考预兆，有人出现了。\",\n",
    "    \"ru\": \"Он думал о предзнаменованиях, и кто-то появился.\",\n",
    "    \"ar\": \"كان يفكر في النذير، وظهر شخص ما.\",\n",
    "    \"hi\": \"वह पूर्वाभास के बारे में सोच रहा था, और कोई प्रकट हुआ था।\",\n",
    "    \"pt\": \"Ele estava pensando em presságios, e alguém havia aparecido.\",\n",
    "}\n",
    "\n",
    "print(\"Testing language-specific normalizers:\")\n",
    "for lang, text in test_texts.items():\n",
    "    normalizer = get_text_normalizer(lang)\n",
    "    normalized = normalizer(text)\n",
    "    print(f\"{lang.upper()}: {text}\")\n",
    "    print(f\"       -> {normalized}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jan-incub-multimodal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
