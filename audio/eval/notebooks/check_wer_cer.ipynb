{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "da403aef",
      "metadata": {},
      "source": [
        "# CER/WER 101 (custom, HF evaluate, jiwer)\n",
        "- Apply audio_eval/text_normalizer_utils.get_text_normalizer per language\n",
        "- Tokenization now matches evaluation pipeline (whitespace only)\n",
        "- currently following kotoba-whisper's repo logic to normalize Japanese and Chinese text\n",
        "  - https://github.com/kotoba-tech/kotoba-whisper/blob/649fe3d1427d9ad6027940e29e7651fd516ea590/run_short_form_eval.py#L200\n",
        "\n",
        "## Text normalization challenges\n",
        "- Japanese words can appear in hiragana, katakana, or kanji but mean the same thing.\n",
        "- Example: Tokyo → とうきょう (hiragana), トウキョウ (katakana), 東京 (kanji).\n",
        "- All forms sound identical, yet ASR evaluation would count them as different.\n",
        "- Normalization needs to unify such variants while keeping true distinctions intact.\n",
        "\n",
        "Here are languages that, like Japanese, have multiple scripts or character sets for the same sounds/words, causing normalization headaches in ASR:\n",
        "\n",
        "- Chinese → Simplified (认识) vs. Traditional (認識), same meaning/sound.  \n",
        "- Korean → Hangul (한글) vs. Hanja (漢字), older texts often mix them.  \n",
        "- Hindi and related Indic languages → Hindi in Devanagari (नमस्ते) vs. Romanized (namaste).  \n",
        "- Arabic & Persian → Variants of Arabic script across regions; e.g., Urdu shares sounds but uses different character forms.  \n",
        "- Serbian → Can be written in Cyrillic (Србија) or Latin (Srbija), same word.  \n",
        "- Mongolian → Traditional script vs. Cyrillic script in Mongolia.  \n",
        "- Kazakh / Uzbek / other Central Asian languages → Cyrillic vs. Latin vs. Arabic script versions, depending on region.  \n",
        "\n",
        "These cases mirror Japanese in that different scripts = same sound/meaning, so without normalization, ASR scoring looks worse than it is."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "86178b28",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Normalization preview\n",
            "-------------------------------------\n",
            "English    REF raw: Hello, nice to meet you.\n",
            "English    REF official: hello nice to meet you\n",
            "English    REF basic: hello nice to meet you \n",
            "English    REF multilingual: hello nice to meet you\n",
            "English    HYP raw: Hello, nice to meet ya.\n",
            "English    HYP official: hello nice to meet ya\n",
            "English    HYP basic: hello nice to meet ya \n",
            "English    HYP multilingual: hello nice to meet ya\n",
            "\n",
            "Japanese   REF raw: こんにちは、はじめまして。\n",
            "Japanese   REF official: こんにちは はしめまして\n",
            "Japanese   REF basic: こ ん に ち は は じ め ま し て \n",
            "Japanese   REF multilingual: こんにちは はしめまして\n",
            "Japanese   HYP raw: こんにちは、はじめましで。\n",
            "Japanese   HYP official: こんにちは はしめまして\n",
            "Japanese   HYP basic: こ ん に ち は は じ め ま し で \n",
            "Japanese   HYP multilingual: こんにちは はしめまして\n",
            "\n",
            "Chinese    REF raw: 你好，很高兴认识你。\n",
            "Chinese    REF official: 你好 很高兴认识你\n",
            "Chinese    REF basic: 你 好 很 高 兴 认 识 你 \n",
            "Chinese    REF multilingual: 你好 很高兴认识你\n",
            "Chinese    HYP raw: 你好，很高兴認識你。\n",
            "Chinese    HYP official: 你好 很高兴認識你\n",
            "Chinese    HYP basic: 你 好 很 高 兴 認 識 你 \n",
            "Chinese    HYP multilingual: 你好 很高兴認識你\n",
            "\n",
            "Normalized (official pipeline)\n",
            "Lang       Method      CER      WER\n",
            "-------------------------------------\n",
            "English    custom   0.091  0.200\n",
            "          evaluate 0.091  0.200\n",
            "          jiwer    0.091  0.200\n",
            "-------------------------------------\n",
            "Japanese   custom   0.000  0.000\n",
            "          evaluate 0.000  0.000\n",
            "          jiwer    0.000  0.000\n",
            "-------------------------------------\n",
            "Chinese    custom   0.222  0.500\n",
            "          evaluate 0.222  0.500\n",
            "          jiwer    0.222  0.500\n",
            "-------------------------------------\n",
            "\n",
            "BasicTextNormalizer (JA/ZH split letters)\n",
            "Lang       Method      CER      WER\n",
            "-------------------------------------\n",
            "English    custom   0.087  0.200\n",
            "          evaluate 0.091  0.200\n",
            "          jiwer    0.091  0.200\n",
            "-------------------------------------\n",
            "Japanese   custom   0.045  0.091\n",
            "          evaluate 0.048  0.091\n",
            "          jiwer    0.048  0.091\n",
            "-------------------------------------\n",
            "Chinese    custom   0.125  0.250\n",
            "          evaluate 0.133  0.250\n",
            "          jiwer    0.133  0.250\n",
            "-------------------------------------\n",
            "\n",
            "BasicMultilingualTextNormalizer (default)\n",
            "Lang       Method      CER      WER\n",
            "-------------------------------------\n",
            "English    custom   0.091  0.200\n",
            "          evaluate 0.091  0.200\n",
            "          jiwer    0.091  0.200\n",
            "-------------------------------------\n",
            "Japanese   custom   0.000  0.000\n",
            "          evaluate 0.000  0.000\n",
            "          jiwer    0.000  0.000\n",
            "-------------------------------------\n",
            "Chinese    custom   0.222  0.500\n",
            "          evaluate 0.222  0.500\n",
            "          jiwer    0.222  0.500\n",
            "-------------------------------------\n",
            "\n",
            "Raw text (char-level tokens for JA/ZH)\n",
            "Lang       Method      CER      WER\n",
            "-------------------------------------\n",
            "English    custom   0.083  0.200\n",
            "          evaluate 0.083  0.200\n",
            "          jiwer    0.083  0.200\n",
            "-------------------------------------\n",
            "Japanese   custom   0.077  0.077\n",
            "          evaluate 0.077  0.077\n",
            "          jiwer    0.077  0.077\n",
            "-------------------------------------\n",
            "Chinese    custom   0.200  0.200\n",
            "          evaluate 0.200  0.200\n",
            "          jiwer    0.200  0.200\n",
            "-------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import sys, importlib, subprocess\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "def ensure(pkg):\n",
        "    try:\n",
        "        importlib.import_module(pkg)\n",
        "    except ImportError:\n",
        "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', pkg])\n",
        "\n",
        "# ensure dependencies\n",
        "ensure('evaluate')\n",
        "ensure('jiwer')\n",
        "\n",
        "# Ensure project root is on sys.path so audio_eval imports work\n",
        "repo_root = Path.cwd().resolve()\n",
        "while repo_root != repo_root.parent and not (repo_root / 'audio_eval').exists():\n",
        "    repo_root = repo_root.parent\n",
        "if (repo_root / 'audio_eval').exists() and str(repo_root) not in sys.path:\n",
        "    sys.path.append(str(repo_root))\n",
        "\n",
        "import evaluate\n",
        "import jiwer\n",
        "\n",
        "from audio_eval.text_normalizer_utils import get_text_normalizer\n",
        "from audio_eval.normalizer.normalizer import BasicTextNormalizer, BasicMultilingualTextNormalizer\n",
        "\n",
        "# ------------------------------\n",
        "# Custom metric utilities\n",
        "# ------------------------------\n",
        "def edit_distance(seq1, seq2):\n",
        "    m, n = len(seq1), len(seq2)\n",
        "    dp = [[0]*(n+1) for _ in range(m+1)]\n",
        "    for i in range(m+1):\n",
        "        dp[i][0] = i\n",
        "    for j in range(n+1):\n",
        "        dp[0][j] = j\n",
        "    for i in range(1, m+1):\n",
        "        for j in range(1, n+1):\n",
        "            cost = 0 if seq1[i-1] == seq2[j-1] else 1\n",
        "            dp[i][j] = min(\n",
        "                dp[i-1][j] + 1,\n",
        "                dp[i][j-1] + 1,\n",
        "                dp[i-1][j-1] + cost,\n",
        "            )\n",
        "    return dp[m][n]\n",
        "\n",
        "def custom_cer(ref, hyp):\n",
        "    return edit_distance(list(ref), list(hyp)) / max(1, len(ref))\n",
        "\n",
        "def custom_wer_tokens(ref_tokens, hyp_tokens):\n",
        "    return edit_distance(ref_tokens, hyp_tokens) / max(1, len(ref_tokens))\n",
        "\n",
        "# ------------------------------\n",
        "# HF evaluate + jiwer\n",
        "# ------------------------------\n",
        "WER_METRIC = evaluate.load('wer')\n",
        "try:\n",
        "    CER_METRIC = evaluate.load('cer')\n",
        "except Exception:\n",
        "    CER_METRIC = None\n",
        "\n",
        "def evaluate_cer(ref, hyp):\n",
        "    if CER_METRIC is not None:\n",
        "        return float(CER_METRIC.compute(references=[ref], predictions=[hyp]))\n",
        "    return custom_cer(ref, hyp)\n",
        "\n",
        "def evaluate_wer_tokens(ref_tokens, hyp_tokens):\n",
        "    ref_join = ' '.join(ref_tokens)\n",
        "    hyp_join = ' '.join(hyp_tokens)\n",
        "    return float(WER_METRIC.compute(references=[ref_join], predictions=[hyp_join]))\n",
        "\n",
        "def jiwer_wer_tokens(ref_tokens, hyp_tokens):\n",
        "    ref_join = ' '.join(ref_tokens)\n",
        "    hyp_join = ' '.join(hyp_tokens)\n",
        "    return float(jiwer.wer(ref_join, hyp_join))\n",
        "\n",
        "def jiwer_cer_score(ref, hyp):\n",
        "    try:\n",
        "        return float(jiwer.cer(ref, hyp))\n",
        "    except AttributeError:\n",
        "        measures = jiwer.compute_measures(ref, hyp)\n",
        "        if 'cer' in measures:\n",
        "            return float(measures['cer'])\n",
        "        return custom_cer(ref, hyp)\n",
        "\n",
        "def fmt(x):\n",
        "    return f\"{x:.3f}\"\n",
        "\n",
        "# ------------------------------\n",
        "# Token helpers\n",
        "# ------------------------------\n",
        "def tokens_whitespace(text):\n",
        "    return [tok for tok in text.split() if tok]\n",
        "\n",
        "def tokens_characters(text):\n",
        "    return [char for char in text if not char.isspace()]\n",
        "\n",
        "def display_table(title, pairs, tokenizer_selector):\n",
        "    print(title)\n",
        "    print('Lang       Method      CER      WER')\n",
        "    print('-------------------------------------')\n",
        "    for lang, (ref, hyp) in pairs.items():\n",
        "        token_fn = tokenizer_selector(lang)\n",
        "        ref_tokens = token_fn(ref)\n",
        "        hyp_tokens = token_fn(hyp)\n",
        "        c_cer = custom_cer(ref, hyp)\n",
        "        c_wer = custom_wer_tokens(ref_tokens, hyp_tokens)\n",
        "        e_cer = evaluate_cer(ref, hyp)\n",
        "        e_wer = evaluate_wer_tokens(ref_tokens, hyp_tokens)\n",
        "        j_cer = jiwer_cer_score(ref, hyp)\n",
        "        j_wer = jiwer_wer_tokens(ref_tokens, hyp_tokens)\n",
        "        print(f\"{lang:<10} custom   {fmt(c_cer)}  {fmt(c_wer)}\")\n",
        "        print(f'          evaluate {fmt(e_cer)}  {fmt(e_wer)}')\n",
        "        print(f'          jiwer    {fmt(j_cer)}  {fmt(j_wer)}')\n",
        "        print('-------------------------------------')\n",
        "    print()\n",
        "\n",
        "# ------------------------------\n",
        "# Test sentences (1-character change each)\n",
        "# ------------------------------\n",
        "examples = {\n",
        "    'English': (\n",
        "        'Hello, nice to meet you.',\n",
        "        'Hello, nice to meet ya.',\n",
        "    ),\n",
        "    'Japanese': (\n",
        "        'こんにちは、はじめまして。',\n",
        "        'こんにちは、はじめましで。',\n",
        "    ),\n",
        "    'Chinese': (\n",
        "        '你好，很高兴认识你。',\n",
        "        '你好，很高兴認識你。',\n",
        "    ),\n",
        "}\n",
        "\n",
        "lang_codes = {'English': 'en', 'Japanese': 'ja', 'Chinese': 'zh'}\n",
        "\n",
        "basic_text_normalizers = {\n",
        "    'English': BasicTextNormalizer(remove_diacritics=False, split_letters=False),\n",
        "    'Japanese': BasicTextNormalizer(remove_diacritics=False, split_letters=True),\n",
        "    'Chinese': BasicTextNormalizer(remove_diacritics=False, split_letters=True),\n",
        "}\n",
        "\n",
        "multilingual_text_normalizers = {\n",
        "    'English': BasicMultilingualTextNormalizer(),\n",
        "    'Japanese': BasicMultilingualTextNormalizer(),\n",
        "    'Chinese': BasicMultilingualTextNormalizer(),\n",
        "}\n",
        "\n",
        "normalized_examples = {}\n",
        "basic_text_examples = {}\n",
        "multilingual_examples = {}\n",
        "\n",
        "print('Normalization preview')\n",
        "print('-------------------------------------')\n",
        "for lang, (ref, hyp) in examples.items():\n",
        "    official_normalizer = get_text_normalizer(lang_codes[lang])\n",
        "    basic_normalizer = basic_text_normalizers[lang]\n",
        "    multilingual_normalizer = multilingual_text_normalizers[lang]\n",
        "    norm_ref_off = official_normalizer(ref)\n",
        "    norm_hyp_off = official_normalizer(hyp)\n",
        "    norm_ref_basic = basic_normalizer(ref)\n",
        "    norm_hyp_basic = basic_normalizer(hyp)\n",
        "    norm_ref_multi = multilingual_normalizer(ref)\n",
        "    norm_hyp_multi = multilingual_normalizer(hyp)\n",
        "    normalized_examples[lang] = (norm_ref_off, norm_hyp_off)\n",
        "    basic_text_examples[lang] = (norm_ref_basic, norm_hyp_basic)\n",
        "    multilingual_examples[lang] = (norm_ref_multi, norm_hyp_multi)\n",
        "    print(f'{lang:<10} REF raw: {ref}')\n",
        "    print(f'{lang:<10} REF official: {norm_ref_off}')\n",
        "    print(f'{lang:<10} REF basic: {norm_ref_basic}')\n",
        "    print(f'{lang:<10} REF multilingual: {norm_ref_multi}')\n",
        "    print(f'{lang:<10} HYP raw: {hyp}')\n",
        "    print(f'{lang:<10} HYP official: {norm_hyp_off}')\n",
        "    print(f'{lang:<10} HYP basic: {norm_hyp_basic}')\n",
        "    print(f'{lang:<10} HYP multilingual: {norm_hyp_multi}')\n",
        "    print('')\n",
        "\n",
        "display_table('Normalized (official pipeline)', normalized_examples, lambda _: tokens_whitespace)\n",
        "display_table('BasicTextNormalizer (JA/ZH split letters)', basic_text_examples, lambda _: tokens_whitespace)\n",
        "display_table('BasicMultilingualTextNormalizer (default)', multilingual_examples, lambda _: tokens_whitespace)\n",
        "display_table('Raw text (char-level tokens for JA/ZH)', examples, lambda lang: tokens_characters if lang in {'Japanese', 'Chinese'} else tokens_whitespace)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "jan-incub-multimodal",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
