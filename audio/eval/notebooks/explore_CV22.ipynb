{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe4c7258",
   "metadata": {},
   "source": [
    "# Notebook to eval Common Voice dataset\n",
    "- https://github.com/common-voice/cv-dataset\n",
    "- now legacy code as the code of this notebook is refactored into eval scripts\n",
    "- this notebook is for initial exploration of cv22 dataset and building eval pipeline\n",
    "\n",
    "\n",
    "```bash\n",
    "[lang].tar.gz/\n",
    "├── clips/\n",
    "│   ├── *.mp3 files                # audio clips\n",
    "├── dev.tsv                        # development set (subset for ML model dev/tuning)\n",
    "├── invalidated.tsv                # clips with ≥2 validations where down_votes > up_votes, \n",
    "│                                  # or ≥3 validations with down_votes = up_votes\n",
    "├── other.tsv                      # clips without enough validations to determine status\n",
    "├── test.tsv                       # test set (subset for model evaluation)\n",
    "├── train.tsv                      # training set (largest subset for ML model training)\n",
    "├── validated.tsv                  # clips with ≥2 validations where up_votes > down_votes\n",
    "├── reported.tsv                   # (since Corpus 5.0) sentences flagged/reported by contributors\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0361487",
   "metadata": {},
   "source": [
    "## Variables, file utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "062df8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok. Reading from .env file\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "REPO_ROOT = Path(\"..\").resolve()\n",
    "if str(REPO_ROOT) not in sys.path:\n",
    "    sys.path.append(str(REPO_ROOT))\n",
    "\n",
    "load_dotenv(override=True)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from functools import partial\n",
    "print(os.getenv(\"TEST\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c8168db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_dirs(path, return_full_path=False):\n",
    "    path = os.path.abspath(os.path.realpath(os.path.expanduser(path)))\n",
    "    dirs = []\n",
    "    for f in os.listdir(path):\n",
    "        full_path = os.path.join(path, f)\n",
    "        if os.path.isdir(full_path):\n",
    "            dirs.append(full_path if return_full_path else f)\n",
    "    return dirs\n",
    "\n",
    "\n",
    "def list_files(path, ext=\".mp3\", return_full_path=False):\n",
    "    path = os.path.abspath(os.path.realpath(os.path.expanduser(path)))\n",
    "    files = []\n",
    "    for f in os.listdir(path):\n",
    "        full_path = os.path.join(path, f)\n",
    "        if os.path.isfile(full_path) and f.endswith(ext):\n",
    "            files.append(full_path if return_full_path else f)\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c94ad59",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dirs(os.getenv(\"CV22_PATH\"))\n",
    "test_tsv_files = list_files(\n",
    "    os.path.join(os.getenv(\"CV22_PATH\"), \"en\"),\n",
    "    ext=\"test.tsv\",\n",
    "    return_full_path=True,\n",
    ")\n",
    "\n",
    "print(test_tsv_files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22793bc",
   "metadata": {},
   "source": [
    "## CV22 manager class\n",
    "- simple class to easily work with Common Voice dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "422c83f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import threading\n",
    "import concurrent.futures\n",
    "import threading\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class CommonVoiceDataset:\n",
    "    def __init__(self, root_path, random_seed=42):\n",
    "        \"\"\"Initialize Common Voice dataset handler.\"\"\"\n",
    "        self.root_path = os.path.abspath(os.path.realpath(os.path.expanduser(root_path)))\n",
    "        self.languages = self._list_languages()\n",
    "        self.random_seed = random_seed\n",
    "        self.__valid_splits = [\"train\", \"dev\", \"test\", \"validated\", \"invalidated\", \"other\", \"reported\"]\n",
    "\n",
    "        print(f\"Found {len(self.languages)} languages: {self.languages[:5]}...\")\n",
    "\n",
    "    # Private methods\n",
    "    def __validate_language(self, lang_code):\n",
    "        if lang_code not in self.languages:\n",
    "            raise ValueError(f\"Language '{lang_code}' not found\")\n",
    "\n",
    "    def __validate_split(self, split):\n",
    "        if split not in self.__valid_splits:\n",
    "            raise ValueError(f\"Invalid split '{split}'\")\n",
    "\n",
    "    # Protected methods\n",
    "    def _list_languages(self):\n",
    "        \"\"\"List all language directories.\"\"\"\n",
    "        return [d for d in os.listdir(self.root_path) if os.path.isdir(os.path.join(self.root_path, d))]\n",
    "\n",
    "    def _get_language_path(self, lang_code):\n",
    "        self.__validate_language(lang_code)\n",
    "        return os.path.join(self.root_path, lang_code)\n",
    "\n",
    "    def _get_split_tsv_path(self, lang_code, split):\n",
    "        self.__validate_split(split)\n",
    "        lang_path = self._get_language_path(lang_code)\n",
    "        tsv_file = os.path.join(lang_path, f\"{split}.tsv\")\n",
    "\n",
    "        if not os.path.exists(tsv_file):\n",
    "            available = self._get_available_splits(lang_code)\n",
    "            raise ValueError(f\"Split '{split}' not found for {lang_code}. Available: {available}\")\n",
    "\n",
    "        return tsv_file\n",
    "\n",
    "    def _get_available_splits(self, lang_code):\n",
    "        lang_path = self._get_language_path(lang_code)\n",
    "        return [s for s in self.__valid_splits if os.path.exists(os.path.join(lang_path, f\"{s}.tsv\"))]\n",
    "\n",
    "    def _load_dataframe(self, tsv_path):\n",
    "        df = pd.read_csv(tsv_path, sep=\"\\t\", low_memory=False)\n",
    "\n",
    "        # Clean missing data\n",
    "        if \"path\" in df.columns and \"sentence\" in df.columns:\n",
    "            df = df.dropna(subset=[\"path\", \"sentence\"])\n",
    "            df = df[df[\"sentence\"].str.strip() != \"\"]\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _apply_quality_filters(self, df, min_up_votes=0, require_gender=False, require_age=False):\n",
    "        if \"up_votes\" in df.columns and min_up_votes > 0:\n",
    "            df = df[df[\"up_votes\"] >= min_up_votes]\n",
    "\n",
    "        if require_gender and \"gender\" in df.columns:\n",
    "            df = df[df[\"gender\"].notna()]\n",
    "\n",
    "        if require_age and \"age\" in df.columns:\n",
    "            df = df[df[\"age\"].notna()]\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _sample_deterministically(self, df, n_samples):\n",
    "        np.random.seed(self.random_seed)\n",
    "\n",
    "        if len(df) > n_samples:\n",
    "            return df.sample(n=n_samples, random_state=self.random_seed)\n",
    "        return df\n",
    "\n",
    "    def _build_audio_path(self, lang_code, filename):\n",
    "        lang_path = self._get_language_path(lang_code)\n",
    "\n",
    "        # Try clips subdirectory first\n",
    "        audio_path = os.path.join(lang_path, \"clips\", filename)\n",
    "        if os.path.exists(audio_path):\n",
    "            return audio_path\n",
    "\n",
    "        # Try direct path\n",
    "        audio_path = os.path.join(lang_path, filename)\n",
    "        if os.path.exists(audio_path):\n",
    "            return audio_path\n",
    "\n",
    "        raise FileNotFoundError(f\"Audio file not found: {filename}\")\n",
    "\n",
    "    def _extract_sample_metadata(self, row):\n",
    "        return {\n",
    "            \"client_id\": row.get(\"client_id\", \"\"),\n",
    "            \"age\": row.get(\"age\", \"\"),\n",
    "            \"gender\": row.get(\"gender\", \"\"),\n",
    "            \"accents\": row.get(\"accents\", \"\"),\n",
    "            \"up_votes\": row.get(\"up_votes\", 0),\n",
    "            \"down_votes\": row.get(\"down_votes\", 0),\n",
    "        }\n",
    "\n",
    "    # Public methods\n",
    "    def load_split(self, lang_code, split=\"test\"):\n",
    "        \"\"\"Load a dataset split.\"\"\"\n",
    "        tsv_path = self._get_split_tsv_path(lang_code, split)\n",
    "        return self._load_dataframe(tsv_path)\n",
    "\n",
    "    def get_samples(\n",
    "        self, lang_code, n_samples=100, split=\"test\", min_up_votes=2, require_gender=False, require_age=False\n",
    "    ):\n",
    "        \"\"\"Get n deterministic samples from a language split.\"\"\"\n",
    "        df = self.load_split(lang_code, split)\n",
    "        df = self._apply_quality_filters(df, min_up_votes, require_gender, require_age)\n",
    "        df = self._sample_deterministically(df, n_samples)\n",
    "\n",
    "        if len(df) < n_samples:\n",
    "            print(f\"Warning: {lang_code}/{split} has only {len(df)}/{n_samples} samples\")\n",
    "\n",
    "        return df.reset_index(drop=True)\n",
    "\n",
    "    def get_audio_file_path(self, lang_code, audio_filename):\n",
    "        \"\"\"Get full path to audio file.\"\"\"\n",
    "        return self._build_audio_path(lang_code, audio_filename)\n",
    "\n",
    "    def get_sample_with_audio(self, lang_code, sample_row):\n",
    "        \"\"\"Get audio path and metadata for a sample.\"\"\"\n",
    "        metadata = self._extract_sample_metadata(sample_row)\n",
    "\n",
    "        return {\n",
    "            \"audio_path\": self._build_audio_path(lang_code, sample_row[\"path\"]),\n",
    "            \"text\": sample_row[\"sentence\"],\n",
    "            \"lang_code\": lang_code,\n",
    "            **metadata,\n",
    "        }\n",
    "\n",
    "    def iter_language_samples(self, n_samples=100, split=\"test\", languages=None, min_up_votes=2, skip_errors=True):\n",
    "        \"\"\"Iterate through samples for each language.\"\"\"\n",
    "        langs_to_process = languages if languages else sorted(self.languages)\n",
    "\n",
    "        for lang_code in langs_to_process:\n",
    "            try:\n",
    "                samples = self.get_samples(lang_code, n_samples, split, min_up_votes=min_up_votes)\n",
    "\n",
    "                if len(samples) > 0:\n",
    "                    yield lang_code, samples\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {lang_code}: {e}\")\n",
    "                if not skip_errors:\n",
    "                    raise\n",
    "\n",
    "    def get_batch_generator(self, lang_code, samples_df, batch_size=16):\n",
    "        \"\"\"Generate batches of samples.\"\"\"\n",
    "        for i in range(0, len(samples_df), batch_size):\n",
    "            batch = []\n",
    "\n",
    "            for _, row in samples_df.iloc[i : i + batch_size].iterrows():\n",
    "                try:\n",
    "                    sample = self.get_sample_with_audio(lang_code, row)\n",
    "                    batch.append(sample)\n",
    "                except Exception as e:\n",
    "                    print(f\"Skipping sample: {e}\")\n",
    "                    continue\n",
    "\n",
    "            if batch:\n",
    "                yield batch\n",
    "\n",
    "    def get_language_stats(self, lang_code, split=\"test\"):\n",
    "        \"\"\"Get statistics for a language/split.\"\"\"\n",
    "        try:\n",
    "            df = self.load_split(lang_code, split)\n",
    "\n",
    "            stats = {\n",
    "                \"total_samples\": len(df),\n",
    "                \"available_splits\": self._get_available_splits(lang_code),\n",
    "            }\n",
    "\n",
    "            if \"client_id\" in df.columns:\n",
    "                stats[\"unique_speakers\"] = df[\"client_id\"].nunique()\n",
    "\n",
    "            if \"up_votes\" in df.columns:\n",
    "                stats[\"avg_up_votes\"] = df[\"up_votes\"].mean()\n",
    "                stats[\"high_quality_samples\"] = len(df[df[\"up_votes\"] >= 2])\n",
    "\n",
    "            return stats\n",
    "\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "    def iter_language_samples_with_checkpoint(\n",
    "        self, n_samples=100, split=\"test\", languages=None, checkpoint_file=\"wer_checkpoint.json\"\n",
    "    ):\n",
    "        \"\"\"Iterate through languages, skipping already completed ones.\"\"\"\n",
    "        # Load checkpoint if exists\n",
    "        completed = set()\n",
    "        if os.path.exists(checkpoint_file):\n",
    "            with open(checkpoint_file, \"r\") as f:\n",
    "                checkpoint = json.load(f)\n",
    "                completed = set(checkpoint.get(\"completed\", []))\n",
    "                print(f\"Resuming from checkpoint: {len(completed)} languages already done\")\n",
    "\n",
    "        langs_to_process = languages if languages else sorted(self.languages)\n",
    "        langs_to_process = [l for l in langs_to_process if l not in completed]\n",
    "\n",
    "        for lang_code in langs_to_process:\n",
    "            try:\n",
    "                samples = self.get_samples(lang_code, n_samples, split)\n",
    "                if len(samples) > 0:\n",
    "                    yield lang_code, samples\n",
    "            except Exception as e:\n",
    "                print(f\"[{datetime.now()}] Error with {lang_code}: {e}\")\n",
    "                yield lang_code, None  # Return None to signal error but continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f7800987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 languages: ['ru', 'it', 'en', 'es', 'ja']...\n",
      "ru: {'total_samples': 10244, 'available_splits': ['train', 'dev', 'test', 'validated', 'invalidated', 'other', 'reported'], 'unique_speakers': 2033, 'avg_up_votes': 2.0807301835220615, 'high_quality_samples': 10244}\n",
      "it: {'total_samples': 15177, 'available_splits': ['train', 'dev', 'test', 'validated', 'invalidated', 'other', 'reported'], 'unique_speakers': 3848, 'avg_up_votes': 2.1311853462476114, 'high_quality_samples': 15177}\n",
      "en: {'total_samples': 16396, 'available_splits': ['train', 'dev', 'test', 'validated', 'invalidated', 'other', 'reported'], 'unique_speakers': 12082, 'avg_up_votes': 2.2098072700658697, 'high_quality_samples': 16396}\n",
      "es: {'total_samples': 15893, 'available_splits': ['train', 'dev', 'test', 'validated', 'invalidated', 'other', 'reported'], 'unique_speakers': 6550, 'avg_up_votes': 2.0812936512930222, 'high_quality_samples': 15893}\n",
      "en: {'total_samples': 16396, 'available_splits': ['train', 'dev', 'test', 'validated', 'invalidated', 'other', 'reported'], 'unique_speakers': 12082, 'avg_up_votes': 2.2098072700658697, 'high_quality_samples': 16396}\n",
      "es: {'total_samples': 15893, 'available_splits': ['train', 'dev', 'test', 'validated', 'invalidated', 'other', 'reported'], 'unique_speakers': 6550, 'avg_up_votes': 2.0812936512930222, 'high_quality_samples': 15893}\n",
      "ja: {'total_samples': 8004, 'available_splits': ['train', 'dev', 'test', 'validated', 'invalidated', 'other', 'reported'], 'unique_speakers': 2015, 'avg_up_votes': 3.505247376311844, 'high_quality_samples': 8004}\n",
      "de: {'total_samples': 16196, 'available_splits': ['train', 'dev', 'test', 'validated', 'invalidated', 'other', 'reported'], 'unique_speakers': 5030, 'avg_up_votes': 2.1308347740182763, 'high_quality_samples': 16196}\n",
      "ja: {'total_samples': 8004, 'available_splits': ['train', 'dev', 'test', 'validated', 'invalidated', 'other', 'reported'], 'unique_speakers': 2015, 'avg_up_votes': 3.505247376311844, 'high_quality_samples': 8004}\n",
      "de: {'total_samples': 16196, 'available_splits': ['train', 'dev', 'test', 'validated', 'invalidated', 'other', 'reported'], 'unique_speakers': 5030, 'avg_up_votes': 2.1308347740182763, 'high_quality_samples': 16196}\n",
      "pl: {'total_samples': 9819, 'available_splits': ['train', 'dev', 'test', 'validated', 'invalidated', 'other', 'reported'], 'unique_speakers': 2442, 'avg_up_votes': 2.569915470007129, 'high_quality_samples': 9819}\n",
      "pt: {'total_samples': 9641, 'available_splits': ['train', 'dev', 'test', 'validated', 'invalidated', 'other', 'reported'], 'unique_speakers': 2162, 'avg_up_votes': 2.120423192614874, 'high_quality_samples': 9641}\n",
      "fr: {'total_samples': 16186, 'available_splits': ['train', 'dev', 'test', 'validated', 'invalidated', 'other', 'reported'], 'unique_speakers': 5147, 'avg_up_votes': 2.0838378845916226, 'high_quality_samples': 16186}\n",
      "pl: {'total_samples': 9819, 'available_splits': ['train', 'dev', 'test', 'validated', 'invalidated', 'other', 'reported'], 'unique_speakers': 2442, 'avg_up_votes': 2.569915470007129, 'high_quality_samples': 9819}\n",
      "pt: {'total_samples': 9641, 'available_splits': ['train', 'dev', 'test', 'validated', 'invalidated', 'other', 'reported'], 'unique_speakers': 2162, 'avg_up_votes': 2.120423192614874, 'high_quality_samples': 9641}\n",
      "fr: {'total_samples': 16186, 'available_splits': ['train', 'dev', 'test', 'validated', 'invalidated', 'other', 'reported'], 'unique_speakers': 5147, 'avg_up_votes': 2.0838378845916226, 'high_quality_samples': 16186}\n",
      "zh-CN: {'total_samples': 10635, 'available_splits': ['train', 'dev', 'test', 'validated', 'invalidated', 'other', 'reported'], 'unique_speakers': 2811, 'avg_up_votes': 2.062905500705219, 'high_quality_samples': 10635}\n",
      "zh-CN: {'total_samples': 10635, 'available_splits': ['train', 'dev', 'test', 'validated', 'invalidated', 'other', 'reported'], 'unique_speakers': 2811, 'avg_up_votes': 2.062905500705219, 'high_quality_samples': 10635}\n"
     ]
    }
   ],
   "source": [
    "# print size of all the languages in cv22\n",
    "cv = CommonVoiceDataset(os.getenv(\"CV22_PATH\"))\n",
    "all_stats = {}\n",
    "for lang in cv.languages:\n",
    "    stats = cv.get_language_stats(lang, split=\"test\")\n",
    "    all_stats[lang] = stats\n",
    "    print(f\"{lang}: {stats}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5d7d0f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 languages: ['ru', 'it', 'en', 'es', 'ja']...\n",
      "['ru', 'it', 'en', 'es', 'ja', 'de', 'pl', 'pt', 'fr', 'zh-CN']\n",
      "'/root/data/common_voice_22/cv-corpus-22.0-2025-06-20/en/clips/common_voice_en_77702.mp3'\n",
      "'He was thinking about omens, and someone had appeared.'\n"
     ]
    }
   ],
   "source": [
    "# use the class\n",
    "cv_dataset = CommonVoiceDataset(os.getenv(\"CV22_PATH\"))\n",
    "print(cv_dataset.languages)\n",
    "\n",
    "# get the first 5 samples of english test set\n",
    "samples_df = cv_dataset.get_samples(\"en\", split=\"test\", n_samples=5)\n",
    "\n",
    "# get the first sample of english test set with audio path and metadata\n",
    "sample_with_audio = cv_dataset.get_sample_with_audio(\"en\", samples_df.iloc[0])\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(sample_with_audio[\"audio_path\"])\n",
    "pprint(sample_with_audio[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5a46a8",
   "metadata": {},
   "source": [
    "## Functions to evaluate ASR models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05be5e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to evaluate ASR models have been replaced by run_wer_evaluation\n",
    "# which supports multiple services with proper checkpointing and error handling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2ef790",
   "metadata": {},
   "source": [
    "## Sample Transcriptions endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "724cb70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install openai groq speechmatics-python elevenlabs jiwer -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "55aae8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "SAMPLE_AUDIO_FILE = \"/root/data/common_voice_22/cv-corpus-22.0-2025-06-20/en/clips/common_voice_en_77702.mp3\"\n",
    "SAMPLE_TEXT = \"He was thinking about omens, and someone had appeared.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576b46dc",
   "metadata": {},
   "source": [
    "### OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81b894c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "audio_file = open(SAMPLE_AUDIO_FILE, \"rb\")\n",
    "\n",
    "transcription = client.audio.transcriptions.create(model=\"gpt-4o-transcribe\", file=audio_file)\n",
    "\n",
    "print(transcription.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a63c3f",
   "metadata": {},
   "source": [
    "### Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821bf051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " He was thinking about omens and someone had appeared.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "\n",
    "client = Groq()\n",
    "filename = SAMPLE_AUDIO_FILE\n",
    "\n",
    "with open(filename, \"rb\") as file:\n",
    "    transcription = client.audio.transcriptions.create(\n",
    "        file=(filename, file.read()),\n",
    "        model=\"whisper-large-v3-turbo\",\n",
    "        response_format=\"verbose_json\",\n",
    "    )\n",
    "    print(transcription.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b856344d",
   "metadata": {},
   "source": [
    "### ElevenLabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991a01a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He was thinking about omens, and someone had appeared.\n"
     ]
    }
   ],
   "source": [
    "from elevenlabs import ElevenLabs\n",
    "\n",
    "# Initialize client\n",
    "client = ElevenLabs(api_key=os.getenv(\"ELEVENLABS_API_KEY\"))\n",
    "\n",
    "# Open your audio file and send to API\n",
    "with open(SAMPLE_AUDIO_FILE, \"rb\") as f:\n",
    "    transcript = client.speech_to_text.convert(\n",
    "        file=f,\n",
    "        model_id=\"scribe_v1\",  # or \"scribe_v1_experimental\"\n",
    "        language_code=\"en\",  # optional\n",
    "    )\n",
    "\n",
    "# 'This request exceeds your API key quota of 10. You have 0 credits remaining, while 5 credits are required for this request.'\n",
    "print(transcript.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea294ac6",
   "metadata": {},
   "source": [
    "### Spechmatics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cdc9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job bfa1a4g0fx submitted. Waiting for completion...\n",
      "Transcript:\n",
      " He was thinking about omens and someone had appeared.\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/speechmatics/speechmatics-python-sdk\n",
    "\n",
    "from speechmatics.models import ConnectionSettings, BatchTranscriptionConfig\n",
    "from speechmatics.batch_client import BatchClient\n",
    "from httpx import HTTPStatusError\n",
    "\n",
    "API_KEY = os.getenv(\"SPEECHMATICS_API_KEY\")\n",
    "PATH_TO_FILE = SAMPLE_AUDIO_FILE\n",
    "LANGUAGE = \"en\"\n",
    "\n",
    "settings = ConnectionSettings(\n",
    "    url=\"https://asr.api.speechmatics.com/v2\",  # Batch API endpoint\n",
    "    auth_token=API_KEY,\n",
    ")\n",
    "\n",
    "with BatchClient(settings) as client:\n",
    "    try:\n",
    "        # Submit job\n",
    "        job_id = client.submit_job(PATH_TO_FILE, BatchTranscriptionConfig(language=LANGUAGE))\n",
    "        print(f\"Job {job_id} submitted. Waiting for completion...\")\n",
    "\n",
    "        # Wait for results (txt, json-v2, srt, etc.)\n",
    "        transcript = client.wait_for_completion(job_id, transcription_format=\"txt\")\n",
    "        print(\"Transcript:\\n\", transcript)\n",
    "\n",
    "    except HTTPStatusError as e:\n",
    "        if e.response.status_code == 401:\n",
    "            print(\"Invalid API key – check your API_KEY.\")\n",
    "        elif e.response.status_code == 400:\n",
    "            print(\"Bad request:\", e.response.json().get(\"detail\"))\n",
    "        else:\n",
    "            raise e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e002477",
   "metadata": {},
   "source": [
    "### Gladia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e57df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded: https://api.gladia.io/file/2d772b19-0d18-4c02-acfc-abbdb41a1504\n",
      "Job ID: 45387c1f-f0c6-445d-89ab-135070334072\n",
      "\n",
      "Transcription:\n",
      " He was thinking about omens and someone had appeared.\n"
     ]
    }
   ],
   "source": [
    "import requests, time, os\n",
    "\n",
    "API_KEY = os.getenv(\"GLADIA_API_KEY\")\n",
    "AUDIO_FILE = SAMPLE_AUDIO_FILE\n",
    "\n",
    "headers = {\"x-gladia-key\": API_KEY}\n",
    "\n",
    "# 1. Upload audio file\n",
    "with open(AUDIO_FILE, \"rb\") as f:\n",
    "    resp = requests.post(\n",
    "        \"https://api.gladia.io/v2/upload\",\n",
    "        headers=headers,\n",
    "        files={\"audio\": (os.path.basename(AUDIO_FILE), f, \"audio/wav\")},\n",
    "    )\n",
    "resp.raise_for_status()\n",
    "file_url = resp.json()[\"audio_url\"]\n",
    "print(\"Uploaded:\", file_url)\n",
    "\n",
    "# 2. Request transcription\n",
    "payload = {\"audio_url\": file_url}\n",
    "resp = requests.post(\n",
    "    \"https://api.gladia.io/v2/pre-recorded\", headers={**headers, \"Content-Type\": \"application/json\"}, json=payload\n",
    ")\n",
    "resp.raise_for_status()\n",
    "job = resp.json()\n",
    "job_id, result_url = job[\"id\"], job[\"result_url\"]\n",
    "print(\"Job ID:\", job_id)\n",
    "\n",
    "# 3. Poll until done\n",
    "while True:\n",
    "    resp = requests.get(result_url, headers=headers)\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "    if data[\"status\"] == \"done\":\n",
    "        transcript = data[\"result\"][\"transcription\"][\"full_transcript\"]\n",
    "        print(\"\\nTranscription:\\n\", transcript)\n",
    "        break\n",
    "    elif data[\"status\"] == \"error\":\n",
    "        print(\"Error:\", data)\n",
    "        break\n",
    "    else:\n",
    "        time.sleep(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da04db87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transcribe functions for each service\n",
    "def transcribe_openai(audio_path, model=\"gpt-4o-transcribe\"):\n",
    "    import logging\n",
    "    from openai import OpenAI\n",
    "\n",
    "    # Suppress verbose logs from httpx and other libraries\n",
    "    logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n",
    "    logging.getLogger(\"openai\").setLevel(logging.WARNING)\n",
    "\n",
    "    client = OpenAI()\n",
    "    with open(audio_path, \"rb\") as audio_file:\n",
    "        transcription = client.audio.transcriptions.create(model=model, file=audio_file)\n",
    "    return transcription.text\n",
    "\n",
    "\n",
    "def transcribe_groq(audio_path, model=\"whisper-large-v3-turbo\"):\n",
    "    import logging\n",
    "\n",
    "    # Suppress verbose logs from all HTTP libraries before importing\n",
    "    logging.getLogger(\"httpx\").setLevel(logging.ERROR)\n",
    "    logging.getLogger(\"urllib3\").setLevel(logging.ERROR)\n",
    "    logging.getLogger(\"requests\").setLevel(logging.ERROR)\n",
    "    logging.getLogger(\"groq\").setLevel(logging.ERROR)\n",
    "\n",
    "    from groq import Groq\n",
    "\n",
    "    client = Groq()\n",
    "    with open(audio_path, \"rb\") as file:\n",
    "        transcription = client.audio.transcriptions.create(\n",
    "            file=(os.path.basename(audio_path), file.read()),\n",
    "            model=model,\n",
    "            response_format=\"verbose_json\",\n",
    "        )\n",
    "    return transcription.text\n",
    "\n",
    "\n",
    "def transcribe_elevenlabs(audio_path, model_id=\"scribe_v1\"):\n",
    "    import logging\n",
    "    from elevenlabs import ElevenLabs\n",
    "\n",
    "    # Suppress verbose logs from httpx and other libraries\n",
    "    logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n",
    "    logging.getLogger(\"elevenlabs\").setLevel(logging.WARNING)\n",
    "\n",
    "    client = ElevenLabs(api_key=os.getenv(\"ELEVENLABS_API_KEY\"))\n",
    "    with open(audio_path, \"rb\") as f:\n",
    "        transcript = client.speech_to_text.convert(\n",
    "            file=f,\n",
    "            model_id=model_id,\n",
    "            language_code=\"en\",\n",
    "        )\n",
    "    return transcript.text\n",
    "\n",
    "\n",
    "def transcribe_speechmatics(audio_path, language=\"en\"):\n",
    "    import logging\n",
    "    from speechmatics.models import ConnectionSettings, BatchTranscriptionConfig\n",
    "    from speechmatics.batch_client import BatchClient\n",
    "    from httpx import HTTPStatusError\n",
    "\n",
    "    # Suppress verbose logs from speechmatics and httpx\n",
    "    logging.getLogger(\"speechmatics\").setLevel(logging.WARNING)\n",
    "    logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n",
    "\n",
    "    API_KEY = os.getenv(\"SPEECHMATICS_API_KEY\")\n",
    "    settings = ConnectionSettings(\n",
    "        url=\"https://asr.api.speechmatics.com/v2\",\n",
    "        auth_token=API_KEY,\n",
    "    )\n",
    "\n",
    "    with BatchClient(settings) as client:\n",
    "        try:\n",
    "            job_id = client.submit_job(audio_path, BatchTranscriptionConfig(language=language))\n",
    "            transcript = client.wait_for_completion(job_id, transcription_format=\"txt\")\n",
    "            return transcript\n",
    "        except HTTPStatusError as e:\n",
    "            if e.response.status_code == 401:\n",
    "                raise Exception(\"Invalid API key – check your SPEECHMATICS_API_KEY.\")\n",
    "            elif e.response.status_code == 400:\n",
    "                raise Exception(f\"Bad request: {e.response.json().get('detail')}\")\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "\n",
    "def transcribe_gladia(audio_path):\n",
    "    import logging, requests, time\n",
    "\n",
    "    # Suppress verbose logs from requests and urllib3\n",
    "    logging.getLogger(\"requests\").setLevel(logging.WARNING)\n",
    "    logging.getLogger(\"urllib3\").setLevel(logging.WARNING)\n",
    "\n",
    "    API_KEY = os.getenv(\"GLADIA_API_KEY\")\n",
    "    headers = {\"x-gladia-key\": API_KEY}\n",
    "\n",
    "    # 1. Upload audio file\n",
    "    with open(audio_path, \"rb\") as f:\n",
    "        resp = requests.post(\n",
    "            \"https://api.gladia.io/v2/upload\",\n",
    "            headers=headers,\n",
    "            files={\"audio\": (os.path.basename(audio_path), f, \"audio/wav\")},\n",
    "        )\n",
    "    resp.raise_for_status()\n",
    "    file_url = resp.json()[\"audio_url\"]\n",
    "\n",
    "    # 2. Request transcription\n",
    "    payload = {\"audio_url\": file_url}\n",
    "    resp = requests.post(\n",
    "        \"https://api.gladia.io/v2/pre-recorded\", headers={**headers, \"Content-Type\": \"application/json\"}, json=payload\n",
    "    )\n",
    "    resp.raise_for_status()\n",
    "    job = resp.json()\n",
    "    job_id, result_url = job[\"id\"], job[\"result_url\"]\n",
    "\n",
    "    # 3. Poll until done\n",
    "    while True:\n",
    "        resp = requests.get(result_url, headers=headers)\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "        if data[\"status\"] == \"done\":\n",
    "            transcript = data[\"result\"][\"transcription\"][\"full_transcript\"]\n",
    "            return transcript\n",
    "        elif data[\"status\"] == \"error\":\n",
    "            raise Exception(f\"Transcription error: {data}\")\n",
    "        else:\n",
    "            time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678e17c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        lang_code: Language code (e.g., 'en', 'es', 'fr', etc.)\n",
    "\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if lang_code.lower() in [\"en\", \"english\"]:\n",
    "    else:\n",
    "\n",
    "\n",
    "# For backward compatibility, keep the old name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80d4cff",
   "metadata": {},
   "source": [
    "## Service Transcribe Functions\n",
    "\n",
    "Define transcribe functions for each ASR service with the same interface: `transcribe(audio_path)` -> str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "60548ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SERVICES: ['openai_gpt-4o-transcribe', 'openai_gpt-4o-mini-transcribe', 'openai_whisper-1', 'groq_whisper-large-v3', 'groq_whisper-large-v3-turbo', 'elevenlabs_scribe_v1', 'speechmatics_en', 'gladia_pre-recorded']\n",
      "Len SERVICES: 8\n"
     ]
    }
   ],
   "source": [
    "# Service and model configurations\n",
    "SERVICE_MODELS = [\n",
    "    (\"openai\", \"gpt-4o-transcribe\"),\n",
    "    (\"openai\", \"gpt-4o-mini-transcribe\"),\n",
    "    (\"openai\", \"whisper-1\"),\n",
    "    (\"groq\", \"whisper-large-v3\"),\n",
    "    (\"groq\", \"whisper-large-v3-turbo\"),\n",
    "    (\"elevenlabs\", \"scribe_v1\"),\n",
    "    (\"speechmatics\", \"en\"),\n",
    "    (\"gladia\", \"pre-recorded\"),\n",
    "]\n",
    "\n",
    "# Create service functions with models\n",
    "SERVICE_FUNCS = {}\n",
    "for svc, model in SERVICE_MODELS:\n",
    "    if svc == \"openai\":\n",
    "        SERVICE_FUNCS[f\"{svc}_{model}\"] = partial(transcribe_openai, model=model)\n",
    "    elif svc == \"groq\":\n",
    "        SERVICE_FUNCS[f\"{svc}_{model}\"] = partial(transcribe_groq, model=model)\n",
    "    elif svc == \"elevenlabs\":\n",
    "        SERVICE_FUNCS[f\"{svc}_{model}\"] = partial(transcribe_elevenlabs, model_id=model)\n",
    "    elif svc == \"speechmatics\":\n",
    "        SERVICE_FUNCS[f\"{svc}_{model}\"] = partial(transcribe_speechmatics, language=model)\n",
    "    elif svc == \"gladia\":\n",
    "        SERVICE_FUNCS[f\"{svc}_{model}\"] = transcribe_gladia\n",
    "\n",
    "# Services list (keys of SERVICE_FUNCS)\n",
    "SERVICES = list(SERVICE_FUNCS.keys())\n",
    "print(f\"SERVICES: {SERVICES}\")\n",
    "print(f\"Len SERVICES: {len(SERVICES)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d4ea0f",
   "metadata": {},
   "source": [
    "## WER Evaluation Pipeline\n",
    "\n",
    "Functions for running WER evaluation across multiple services with retry, error handling, and checkpointing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5b4164",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "from typing import Dict, List, Any\n",
    "import jiwer\n",
    "import concurrent.futures\n",
    "\n",
    "\n",
    "def run_wer_evaluation(\n",
    "    dataset_path: str,\n",
    "    services: List[str],\n",
    "    service_funcs: Dict[str, callable],\n",
    "    languages: List[str],\n",
    "    results_file: str,\n",
    "    checkpoint_file: str,\n",
    "    log_file: str,\n",
    "    max_retries: int = 3,\n",
    "    n_samples: int = 100,\n",
    ") -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Run WER evaluation across multiple ASR services with checkpointing and error handling.\n",
    "\n",
    "    Args:\n",
    "        dataset_path: Path to Common Voice dataset\n",
    "        services: List of service names\n",
    "        service_funcs: Dict mapping service names to transcribe functions\n",
    "        languages: List of language codes to evaluate\n",
    "        results_file: Path to save final results\n",
    "        checkpoint_file: Path for checkpointing progress\n",
    "        log_file: Path for logging\n",
    "        max_retries: Maximum retries per transcription\n",
    "        n_samples: Number of samples per language\n",
    "\n",
    "    Returns:\n",
    "        Dict with results per language\n",
    "    \"\"\"\n",
    "\n",
    "    # Setup logging\n",
    "    logging.basicConfig(filename=log_file, level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "    console = logging.StreamHandler()\n",
    "    console.setLevel(logging.INFO)\n",
    "    logging.getLogger(\"\").addHandler(console)\n",
    "\n",
    "    # Initialize dataset\n",
    "    cv_dataset = CommonVoiceDataset(dataset_path)\n",
    "\n",
    "    # Load checkpoint if exists\n",
    "    completed_services = set()\n",
    "    if os.path.exists(checkpoint_file):\n",
    "        with open(checkpoint_file, \"r\") as f:\n",
    "            checkpoint = json.load(f)\n",
    "            completed_services = set(checkpoint.get(\"completed\", []))\n",
    "            logging.info(f\"Resuming from checkpoint: {len(completed_services)} services completed\")\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for lang_code in languages:\n",
    "        if lang_code not in cv_dataset.languages:\n",
    "            logging.warning(f\"Language {lang_code} not found in dataset\")\n",
    "            continue\n",
    "\n",
    "        logging.info(f\"Processing language: {lang_code}\")\n",
    "\n",
    "        # Get samples\n",
    "        samples_df = cv_dataset.get_samples(lang_code, n_samples=n_samples, split=\"test\")\n",
    "        if len(samples_df) == 0:\n",
    "            logging.warning(f\"No samples found for {lang_code}\")\n",
    "            continue\n",
    "\n",
    "        lang_results = {}\n",
    "\n",
    "        for service_name in services:\n",
    "            if f\"{lang_code}_{service_name}\" in completed_services:\n",
    "                logging.info(f\"Skipping completed: {lang_code}_{service_name}\")\n",
    "                continue\n",
    "\n",
    "            logging.info(f\"Evaluating {service_name} on {lang_code}\")\n",
    "\n",
    "            transcribe_func = service_funcs[service_name]\n",
    "            predictions = []\n",
    "            references = []\n",
    "            timings = []\n",
    "\n",
    "            # Process samples sequentially, but run services in parallel for each sample\n",
    "            for idx, (_, row) in enumerate(samples_df.iterrows()):\n",
    "                sample = cv_dataset.get_sample_with_audio(lang_code, row)\n",
    "                audio_path = sample[\"audio_path\"]\n",
    "                reference_text = sample[\"text\"]\n",
    "\n",
    "                # Normalize reference\n",
    "                normalized_ref = text_normalizer(reference_text)\n",
    "                references.append(normalized_ref)\n",
    "\n",
    "                # Transcribe with retries (single service call)\n",
    "                transcription = None\n",
    "                start_time = time.time()\n",
    "\n",
    "                for attempt in range(max_retries):\n",
    "                    try:\n",
    "                        transcription = transcribe_func(audio_path)\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        logging.warning(\n",
    "                            f\"Attempt {attempt + 1} failed for {service_name} on {lang_code} sample {idx}: {e}\"\n",
    "                        )\n",
    "                        if attempt == max_retries - 1:\n",
    "                            logging.error(f\"Failed to transcribe {lang_code} sample {idx} after {max_retries} attempts\")\n",
    "                            transcription = \"\"  # Empty transcription on failure\n",
    "\n",
    "                end_time = time.time()\n",
    "                timings.append(end_time - start_time)\n",
    "\n",
    "                # Normalize prediction\n",
    "                if transcription:\n",
    "                    normalized_pred = text_normalizer(transcription)\n",
    "                else:\n",
    "                    normalized_pred = \"\"\n",
    "                predictions.append(normalized_pred)\n",
    "\n",
    "                if (idx + 1) % 10 == 0:\n",
    "                    logging.info(f\"Processed {idx + 1}/{len(samples_df)} samples for {service_name}\")\n",
    "\n",
    "            # Calculate WER\n",
    "            if predictions and references:\n",
    "                wer = jiwer.wer(references, predictions)\n",
    "                avg_time = sum(timings) / len(timings)\n",
    "            else:\n",
    "                wer = 1.0\n",
    "                avg_time = 0.0\n",
    "\n",
    "            lang_results[service_name] = {\"wer\": wer, \"timing\": avg_time, \"n_samples\": len(predictions)}\n",
    "\n",
    "            logging.info(\n",
    "                f\"{service_name} on {lang_code}: WER={wer:.4f}, Avg Time={avg_time:.2f}s, Samples={len(predictions)}\"\n",
    "            )\n",
    "\n",
    "            # Save checkpoint\n",
    "            completed_services.add(f\"{lang_code}_{service_name}\")\n",
    "            checkpoint = {\"completed\": list(completed_services)}\n",
    "            with open(checkpoint_file, \"w\") as f:\n",
    "                json.dump(checkpoint, f)\n",
    "\n",
    "        results[lang_code] = lang_results\n",
    "\n",
    "        # Save intermediate results\n",
    "        with open(results_file, \"w\") as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "\n",
    "    logging.info(\"Evaluation completed\")\n",
    "    return results\n",
    "\n",
    "\n",
    "def run_wer_evaluation_parallel(\n",
    "    dataset_path: str,\n",
    "    services: List[str],\n",
    "    service_funcs: Dict[str, callable],\n",
    "    languages: List[str],\n",
    "    results_file: str,\n",
    "    checkpoint_file: str,\n",
    "    log_file: str,\n",
    "    max_retries: int = 3,\n",
    "    n_samples: int = 100,\n",
    "    max_workers: int = 8,\n",
    ") -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Run WER evaluation across multiple ASR services with parallel processing and checkpointing.\n",
    "\n",
    "    Args:\n",
    "        dataset_path: Path to Common Voice dataset\n",
    "        services: List of service names\n",
    "        service_funcs: Dict mapping service names to transcribe functions\n",
    "        languages: List of language codes to evaluate\n",
    "        results_file: Path to save final results\n",
    "        checkpoint_file: Path for checkpointing progress\n",
    "        log_file: Path for logging\n",
    "        max_retries: Maximum retries per transcription\n",
    "        n_samples: Number of samples per language\n",
    "        max_workers: Maximum number of parallel workers (services per sample)\n",
    "\n",
    "    Returns:\n",
    "        Dict with results per language\n",
    "    \"\"\"\n",
    "\n",
    "    # Suppress verbose HTTP logging globally before any service imports\n",
    "    import logging\n",
    "\n",
    "    logging.getLogger(\"httpx\").setLevel(logging.ERROR)\n",
    "    logging.getLogger(\"urllib3\").setLevel(logging.ERROR)\n",
    "    logging.getLogger(\"requests\").setLevel(logging.ERROR)\n",
    "    logging.getLogger(\"openai\").setLevel(logging.ERROR)\n",
    "    logging.getLogger(\"groq\").setLevel(logging.ERROR)\n",
    "    logging.getLogger(\"elevenlabs\").setLevel(logging.ERROR)\n",
    "    logging.getLogger(\"speechmatics\").setLevel(logging.ERROR)\n",
    "\n",
    "    # Setup logging\n",
    "    logging.basicConfig(filename=log_file, level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "    console = logging.StreamHandler()\n",
    "    console.setLevel(logging.INFO)\n",
    "    logging.getLogger(\"\").addHandler(console)\n",
    "\n",
    "    # Initialize dataset\n",
    "    cv_dataset = CommonVoiceDataset(dataset_path)\n",
    "\n",
    "    # Load checkpoint if exists\n",
    "    completed_services = set()\n",
    "    if os.path.exists(checkpoint_file):\n",
    "        with open(checkpoint_file, \"r\") as f:\n",
    "            checkpoint = json.load(f)\n",
    "            completed_services = set(checkpoint.get(\"completed\", []))\n",
    "            logging.info(f\"Resuming from checkpoint: {len(completed_services)} services completed\")\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    def transcribe_with_retry(service_name, audio_path, sample_idx, lang_code):\n",
    "        \"\"\"Transcribe a single audio file with retries for one service.\"\"\"\n",
    "        transcribe_func = service_funcs[service_name]\n",
    "        start_time = time.time()\n",
    "\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                transcription = transcribe_func(audio_path)\n",
    "                end_time = time.time()\n",
    "                return {\n",
    "                    \"service\": service_name,\n",
    "                    \"transcription\": transcription,\n",
    "                    \"timing\": end_time - start_time,\n",
    "                    \"success\": True,\n",
    "                }\n",
    "            except Exception as e:\n",
    "                logging.warning(\n",
    "                    f\"Attempt {attempt + 1} failed for {service_name} on {lang_code} sample {sample_idx}: {e}\"\n",
    "                )\n",
    "                if attempt == max_retries - 1:\n",
    "                    logging.error(f\"Failed to transcribe {lang_code} sample {sample_idx} after {max_retries} attempts\")\n",
    "                    end_time = time.time()\n",
    "                    return {\n",
    "                        \"service\": service_name,\n",
    "                        \"transcription\": \"\",\n",
    "                        \"timing\": end_time - start_time,\n",
    "                        \"success\": False,\n",
    "                    }\n",
    "\n",
    "    for lang_code in languages:\n",
    "        if lang_code not in cv_dataset.languages:\n",
    "            logging.warning(f\"Language {lang_code} not found in dataset\")\n",
    "            continue\n",
    "\n",
    "        logging.info(f\"Processing language: {lang_code}\")\n",
    "\n",
    "        # Get samples\n",
    "        samples_df = cv_dataset.get_samples(lang_code, n_samples=n_samples, split=\"test\")\n",
    "        if len(samples_df) == 0:\n",
    "            logging.warning(f\"No samples found for {lang_code}\")\n",
    "            continue\n",
    "\n",
    "        lang_results = {}\n",
    "\n",
    "        # Check which services need to be processed\n",
    "        services_to_process = [s for s in services if f\"{lang_code}_{s}\" not in completed_services]\n",
    "\n",
    "        if not services_to_process:\n",
    "            logging.info(f\"All services already completed for {lang_code}, loading from checkpoint\")\n",
    "            # Load existing results if available\n",
    "            if os.path.exists(results_file):\n",
    "                with open(results_file, \"r\") as f:\n",
    "                    existing_results = json.load(f)\n",
    "                    if lang_code in existing_results:\n",
    "                        lang_results = existing_results[lang_code]\n",
    "            results[lang_code] = lang_results\n",
    "            continue\n",
    "\n",
    "        logging.info(f\"Evaluating {len(services_to_process)} services on {lang_code}\")\n",
    "\n",
    "        # Process each sample, running all services in parallel for that sample\n",
    "        all_predictions = {service: [] for service in services_to_process}\n",
    "        all_references = []\n",
    "        all_timings = {service: [] for service in services_to_process}\n",
    "\n",
    "        for idx, (_, row) in enumerate(samples_df.iterrows()):\n",
    "            sample = cv_dataset.get_sample_with_audio(lang_code, row)\n",
    "            audio_path = sample[\"audio_path\"]\n",
    "            reference_text = sample[\"text\"]\n",
    "\n",
    "            # Normalize reference\n",
    "            normalized_ref = text_normalizer(reference_text)\n",
    "            all_references.append(normalized_ref)\n",
    "\n",
    "            # Run all services in parallel for this sample\n",
    "            with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "                # Submit all service transcription tasks for this sample\n",
    "                future_to_service = {\n",
    "                    executor.submit(transcribe_with_retry, service, audio_path, idx, lang_code): service\n",
    "                    for service in services_to_process\n",
    "                }\n",
    "\n",
    "                # Collect results as they complete\n",
    "                for future in concurrent.futures.as_completed(future_to_service):\n",
    "                    result = future.result()\n",
    "                    service = result[\"service\"]\n",
    "                    transcription = result[\"transcription\"]\n",
    "                    timing = result[\"timing\"]\n",
    "\n",
    "                    # Normalize prediction\n",
    "                    if transcription:\n",
    "                        normalized_pred = text_normalizer(transcription)\n",
    "                    else:\n",
    "                        normalized_pred = \"\"\n",
    "\n",
    "                    all_predictions[service].append(normalized_pred)\n",
    "                    all_timings[service].append(timing)\n",
    "\n",
    "            if (idx + 1) % 5 == 0:\n",
    "                logging.info(f\"Processed {idx + 1}/{len(samples_df)} samples\")\n",
    "\n",
    "        # Calculate WER for each service\n",
    "        for service_name in services_to_process:\n",
    "            predictions = all_predictions[service_name]\n",
    "            timings = all_timings[service_name]\n",
    "\n",
    "            if predictions and all_references:\n",
    "                wer = jiwer.wer(all_references, predictions)\n",
    "                avg_time = sum(timings) / len(timings)\n",
    "            else:\n",
    "                wer = 1.0\n",
    "                avg_time = 0.0\n",
    "\n",
    "            lang_results[service_name] = {\"wer\": wer, \"timing\": avg_time, \"n_samples\": len(predictions)}\n",
    "\n",
    "            logging.info(\n",
    "                f\"{service_name} on {lang_code}: WER={wer:.4f}, Avg Time={avg_time:.2f}s, Samples={len(predictions)}\"\n",
    "            )\n",
    "\n",
    "            # Save checkpoint\n",
    "            completed_services.add(f\"{lang_code}_{service_name}\")\n",
    "            checkpoint = {\"completed\": list(completed_services)}\n",
    "            with open(checkpoint_file, \"w\") as f:\n",
    "                json.dump(checkpoint, f)\n",
    "\n",
    "        results[lang_code] = lang_results\n",
    "\n",
    "        # Save intermediate results\n",
    "        with open(results_file, \"w\") as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "\n",
    "    logging.info(\"Evaluation completed\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2e0050",
   "metadata": {},
   "source": [
    "## Run WER Evaluation\n",
    "\n",
    "Example usage of the WER evaluation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "37a09a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing language: en\n",
      "Processing language: en\n",
      "Processing language: en\n",
      "Processing language: en\n",
      "Processing language: en\n",
      "Evaluating 8 services on en\n",
      "Evaluating 8 services on en\n",
      "Evaluating 8 services on en\n",
      "Evaluating 8 services on en\n",
      "Evaluating 8 services on en\n",
      "Evaluating 8 services on en\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of services: 8\n",
      "Services: ['openai_gpt-4o-transcribe', 'openai_gpt-4o-mini-transcribe', 'openai_whisper-1', 'groq_whisper-large-v3', 'groq_whisper-large-v3-turbo', 'elevenlabs_scribe_v1', 'speechmatics_en', 'gladia_pre-recorded']\n",
      "Found 10 languages: ['ru', 'it', 'en', 'es', 'ja']...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST https://api.groq.com/openai/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.elevenlabs.io/v1/speech-to-text \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.elevenlabs.io/v1/speech-to-text \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.elevenlabs.io/v1/speech-to-text \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.elevenlabs.io/v1/speech-to-text \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.elevenlabs.io/v1/speech-to-text \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.elevenlabs.io/v1/speech-to-text \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://asr.api.speechmatics.com/v2/jobs?sm-sdk=python-5.0.0 \"HTTP/2 201 Created\"\n",
      "HTTP Request: POST https://asr.api.speechmatics.com/v2/jobs?sm-sdk=python-5.0.0 \"HTTP/2 201 Created\"\n",
      "HTTP Request: POST https://asr.api.speechmatics.com/v2/jobs?sm-sdk=python-5.0.0 \"HTTP/2 201 Created\"\n",
      "HTTP Request: POST https://asr.api.speechmatics.com/v2/jobs?sm-sdk=python-5.0.0 \"HTTP/2 201 Created\"\n",
      "HTTP Request: POST https://asr.api.speechmatics.com/v2/jobs?sm-sdk=python-5.0.0 \"HTTP/2 201 Created\"\n",
      "HTTP Request: POST https://asr.api.speechmatics.com/v2/jobs?sm-sdk=python-5.0.0 \"HTTP/2 201 Created\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/e044gzz91q?sm-sdk=python-5.0.0 \"HTTP/2 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/e044gzz91q?sm-sdk=python-5.0.0 \"HTTP/2 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/e044gzz91q?sm-sdk=python-5.0.0 \"HTTP/2 200 OK\"\n",
      "Waiting 1 sec to begin polling for completion.\n",
      "Waiting 1 sec to begin polling for completion.\n",
      "Waiting 1 sec to begin polling for completion.\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/e044gzz91q?sm-sdk=python-5.0.0 \"HTTP/2 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/e044gzz91q?sm-sdk=python-5.0.0 \"HTTP/2 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/e044gzz91q?sm-sdk=python-5.0.0 \"HTTP/2 200 OK\"\n",
      "Waiting 1 sec to begin polling for completion.\n",
      "Waiting 1 sec to begin polling for completion.\n",
      "Waiting 1 sec to begin polling for completion.\n",
      "Starting poll.\n",
      "Starting poll.\n",
      "Starting poll.\n",
      "Starting poll.\n",
      "Starting poll.\n",
      "Starting poll.\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/e044gzz91q?sm-sdk=python-5.0.0 \"HTTP/2 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/e044gzz91q?sm-sdk=python-5.0.0 \"HTTP/2 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/e044gzz91q?sm-sdk=python-5.0.0 \"HTTP/2 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/e044gzz91q?sm-sdk=python-5.0.0 \"HTTP/2 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/e044gzz91q?sm-sdk=python-5.0.0 \"HTTP/2 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/e044gzz91q?sm-sdk=python-5.0.0 \"HTTP/2 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/e044gzz91q/transcript?format=txt \"HTTP/2 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/e044gzz91q/transcript?format=txt \"HTTP/2 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/e044gzz91q/transcript?format=txt \"HTTP/2 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/e044gzz91q/transcript?format=txt \"HTTP/2 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/e044gzz91q/transcript?format=txt \"HTTP/2 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/e044gzz91q/transcript?format=txt \"HTTP/2 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.elevenlabs.io/v1/speech-to-text \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.elevenlabs.io/v1/speech-to-text \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.elevenlabs.io/v1/speech-to-text \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.elevenlabs.io/v1/speech-to-text \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.elevenlabs.io/v1/speech-to-text \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.elevenlabs.io/v1/speech-to-text \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://asr.api.speechmatics.com/v2/jobs?sm-sdk=python-5.0.0 \"HTTP/2 201 Created\"\n",
      "HTTP Request: POST https://asr.api.speechmatics.com/v2/jobs?sm-sdk=python-5.0.0 \"HTTP/2 201 Created\"\n",
      "HTTP Request: POST https://asr.api.speechmatics.com/v2/jobs?sm-sdk=python-5.0.0 \"HTTP/2 201 Created\"\n",
      "HTTP Request: POST https://asr.api.speechmatics.com/v2/jobs?sm-sdk=python-5.0.0 \"HTTP/2 201 Created\"\n",
      "HTTP Request: POST https://asr.api.speechmatics.com/v2/jobs?sm-sdk=python-5.0.0 \"HTTP/2 201 Created\"\n",
      "HTTP Request: POST https://asr.api.speechmatics.com/v2/jobs?sm-sdk=python-5.0.0 \"HTTP/2 201 Created\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/erjdgcjp0b?sm-sdk=python-5.0.0 \"HTTP/2 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/erjdgcjp0b?sm-sdk=python-5.0.0 \"HTTP/2 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/erjdgcjp0b?sm-sdk=python-5.0.0 \"HTTP/2 200 OK\"\n",
      "Waiting 1 sec to begin polling for completion.\n",
      "Waiting 1 sec to begin polling for completion.\n",
      "Waiting 1 sec to begin polling for completion.\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/erjdgcjp0b?sm-sdk=python-5.0.0 \"HTTP/2 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/erjdgcjp0b?sm-sdk=python-5.0.0 \"HTTP/2 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/erjdgcjp0b?sm-sdk=python-5.0.0 \"HTTP/2 200 OK\"\n",
      "Waiting 1 sec to begin polling for completion.\n",
      "Waiting 1 sec to begin polling for completion.\n",
      "Waiting 1 sec to begin polling for completion.\n",
      "Starting poll.\n",
      "Starting poll.\n",
      "Starting poll.\n",
      "Starting poll.\n",
      "Starting poll.\n",
      "Starting poll.\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/erjdgcjp0b?sm-sdk=python-5.0.0 \"HTTP/2 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/erjdgcjp0b?sm-sdk=python-5.0.0 \"HTTP/2 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/erjdgcjp0b?sm-sdk=python-5.0.0 \"HTTP/2 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/erjdgcjp0b?sm-sdk=python-5.0.0 \"HTTP/2 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/erjdgcjp0b?sm-sdk=python-5.0.0 \"HTTP/2 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/erjdgcjp0b?sm-sdk=python-5.0.0 \"HTTP/2 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/erjdgcjp0b/transcript?format=txt \"HTTP/2 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/erjdgcjp0b/transcript?format=txt \"HTTP/2 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/erjdgcjp0b/transcript?format=txt \"HTTP/2 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/erjdgcjp0b/transcript?format=txt \"HTTP/2 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/erjdgcjp0b/transcript?format=txt \"HTTP/2 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/erjdgcjp0b/transcript?format=txt \"HTTP/2 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.elevenlabs.io/v1/speech-to-text \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.elevenlabs.io/v1/speech-to-text \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.elevenlabs.io/v1/speech-to-text \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.elevenlabs.io/v1/speech-to-text \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.elevenlabs.io/v1/speech-to-text \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.elevenlabs.io/v1/speech-to-text \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://asr.api.speechmatics.com/v2/jobs?sm-sdk=python-5.0.0 \"HTTP/2 201 Created\"\n",
      "HTTP Request: POST https://asr.api.speechmatics.com/v2/jobs?sm-sdk=python-5.0.0 \"HTTP/2 201 Created\"\n",
      "HTTP Request: POST https://asr.api.speechmatics.com/v2/jobs?sm-sdk=python-5.0.0 \"HTTP/2 201 Created\"\n",
      "HTTP Request: POST https://asr.api.speechmatics.com/v2/jobs?sm-sdk=python-5.0.0 \"HTTP/2 201 Created\"\n",
      "HTTP Request: POST https://asr.api.speechmatics.com/v2/jobs?sm-sdk=python-5.0.0 \"HTTP/2 201 Created\"\n",
      "HTTP Request: POST https://asr.api.speechmatics.com/v2/jobs?sm-sdk=python-5.0.0 \"HTTP/2 201 Created\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/2k3qhnw6cb?sm-sdk=python-5.0.0 \"HTTP/2 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/2k3qhnw6cb?sm-sdk=python-5.0.0 \"HTTP/2 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/2k3qhnw6cb?sm-sdk=python-5.0.0 \"HTTP/2 200 OK\"\n",
      "Waiting 0 sec to begin polling for completion.\n",
      "Waiting 0 sec to begin polling for completion.\n",
      "Waiting 0 sec to begin polling for completion.\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/2k3qhnw6cb?sm-sdk=python-5.0.0 \"HTTP/2 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/2k3qhnw6cb?sm-sdk=python-5.0.0 \"HTTP/2 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/2k3qhnw6cb?sm-sdk=python-5.0.0 \"HTTP/2 200 OK\"\n",
      "Waiting 0 sec to begin polling for completion.\n",
      "Waiting 0 sec to begin polling for completion.\n",
      "Waiting 0 sec to begin polling for completion.\n",
      "Starting poll.\n",
      "Starting poll.\n",
      "Starting poll.\n",
      "Starting poll.\n",
      "Starting poll.\n",
      "Starting poll.\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/2k3qhnw6cb?sm-sdk=python-5.0.0 \"HTTP/2 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/2k3qhnw6cb?sm-sdk=python-5.0.0 \"HTTP/2 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/2k3qhnw6cb?sm-sdk=python-5.0.0 \"HTTP/2 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/2k3qhnw6cb?sm-sdk=python-5.0.0 \"HTTP/2 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/2k3qhnw6cb?sm-sdk=python-5.0.0 \"HTTP/2 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/2k3qhnw6cb?sm-sdk=python-5.0.0 \"HTTP/2 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/2k3qhnw6cb/transcript?format=txt \"HTTP/2 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/2k3qhnw6cb/transcript?format=txt \"HTTP/2 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/2k3qhnw6cb/transcript?format=txt \"HTTP/2 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/2k3qhnw6cb/transcript?format=txt \"HTTP/2 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/2k3qhnw6cb/transcript?format=txt \"HTTP/2 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/2k3qhnw6cb/transcript?format=txt \"HTTP/2 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.elevenlabs.io/v1/speech-to-text \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.elevenlabs.io/v1/speech-to-text \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.elevenlabs.io/v1/speech-to-text \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.elevenlabs.io/v1/speech-to-text \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.elevenlabs.io/v1/speech-to-text \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.elevenlabs.io/v1/speech-to-text \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://asr.api.speechmatics.com/v2/jobs?sm-sdk=python-5.0.0 \"HTTP/2 201 Created\"\n",
      "HTTP Request: POST https://asr.api.speechmatics.com/v2/jobs?sm-sdk=python-5.0.0 \"HTTP/2 201 Created\"\n",
      "HTTP Request: POST https://asr.api.speechmatics.com/v2/jobs?sm-sdk=python-5.0.0 \"HTTP/2 201 Created\"\n",
      "HTTP Request: POST https://asr.api.speechmatics.com/v2/jobs?sm-sdk=python-5.0.0 \"HTTP/2 201 Created\"\n",
      "HTTP Request: POST https://asr.api.speechmatics.com/v2/jobs?sm-sdk=python-5.0.0 \"HTTP/2 201 Created\"\n",
      "HTTP Request: POST https://asr.api.speechmatics.com/v2/jobs?sm-sdk=python-5.0.0 \"HTTP/2 201 Created\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/tnoo2itybg?sm-sdk=python-5.0.0 \"HTTP/2 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/tnoo2itybg?sm-sdk=python-5.0.0 \"HTTP/2 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/tnoo2itybg?sm-sdk=python-5.0.0 \"HTTP/2 200 OK\"\n",
      "Waiting 0 sec to begin polling for completion.\n",
      "Waiting 0 sec to begin polling for completion.\n",
      "Waiting 0 sec to begin polling for completion.\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/tnoo2itybg?sm-sdk=python-5.0.0 \"HTTP/2 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/tnoo2itybg?sm-sdk=python-5.0.0 \"HTTP/2 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/tnoo2itybg?sm-sdk=python-5.0.0 \"HTTP/2 200 OK\"\n",
      "Waiting 0 sec to begin polling for completion.\n",
      "Waiting 0 sec to begin polling for completion.\n",
      "Waiting 0 sec to begin polling for completion.\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "Starting poll.\n",
      "Starting poll.\n",
      "Starting poll.\n",
      "Starting poll.\n",
      "Starting poll.\n",
      "Starting poll.\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/tnoo2itybg?sm-sdk=python-5.0.0 \"HTTP/2 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/tnoo2itybg?sm-sdk=python-5.0.0 \"HTTP/2 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/tnoo2itybg?sm-sdk=python-5.0.0 \"HTTP/2 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/tnoo2itybg?sm-sdk=python-5.0.0 \"HTTP/2 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/tnoo2itybg?sm-sdk=python-5.0.0 \"HTTP/2 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/tnoo2itybg?sm-sdk=python-5.0.0 \"HTTP/2 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/tnoo2itybg/transcript?format=txt \"HTTP/2 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/tnoo2itybg/transcript?format=txt \"HTTP/2 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/tnoo2itybg/transcript?format=txt \"HTTP/2 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/tnoo2itybg/transcript?format=txt \"HTTP/2 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/tnoo2itybg/transcript?format=txt \"HTTP/2 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/tnoo2itybg/transcript?format=txt \"HTTP/2 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.elevenlabs.io/v1/speech-to-text \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.elevenlabs.io/v1/speech-to-text \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.elevenlabs.io/v1/speech-to-text \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.elevenlabs.io/v1/speech-to-text \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.elevenlabs.io/v1/speech-to-text \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.elevenlabs.io/v1/speech-to-text \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://asr.api.speechmatics.com/v2/jobs?sm-sdk=python-5.0.0 \"HTTP/2 201 Created\"\n",
      "HTTP Request: POST https://asr.api.speechmatics.com/v2/jobs?sm-sdk=python-5.0.0 \"HTTP/2 201 Created\"\n",
      "HTTP Request: POST https://asr.api.speechmatics.com/v2/jobs?sm-sdk=python-5.0.0 \"HTTP/2 201 Created\"\n",
      "HTTP Request: POST https://asr.api.speechmatics.com/v2/jobs?sm-sdk=python-5.0.0 \"HTTP/2 201 Created\"\n",
      "HTTP Request: POST https://asr.api.speechmatics.com/v2/jobs?sm-sdk=python-5.0.0 \"HTTP/2 201 Created\"\n",
      "HTTP Request: POST https://asr.api.speechmatics.com/v2/jobs?sm-sdk=python-5.0.0 \"HTTP/2 201 Created\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/xi5ifavk33?sm-sdk=python-5.0.0 \"HTTP/2 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/xi5ifavk33?sm-sdk=python-5.0.0 \"HTTP/2 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/xi5ifavk33?sm-sdk=python-5.0.0 \"HTTP/2 200 OK\"\n",
      "Waiting 0 sec to begin polling for completion.\n",
      "Waiting 0 sec to begin polling for completion.\n",
      "Waiting 0 sec to begin polling for completion.\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/xi5ifavk33?sm-sdk=python-5.0.0 \"HTTP/2 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/xi5ifavk33?sm-sdk=python-5.0.0 \"HTTP/2 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/xi5ifavk33?sm-sdk=python-5.0.0 \"HTTP/2 200 OK\"\n",
      "Waiting 0 sec to begin polling for completion.\n",
      "Waiting 0 sec to begin polling for completion.\n",
      "Waiting 0 sec to begin polling for completion.\n",
      "Starting poll.\n",
      "Starting poll.\n",
      "Starting poll.\n",
      "Starting poll.\n",
      "Starting poll.\n",
      "Starting poll.\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/xi5ifavk33?sm-sdk=python-5.0.0 \"HTTP/2 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/xi5ifavk33?sm-sdk=python-5.0.0 \"HTTP/2 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/xi5ifavk33?sm-sdk=python-5.0.0 \"HTTP/2 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/xi5ifavk33?sm-sdk=python-5.0.0 \"HTTP/2 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/xi5ifavk33?sm-sdk=python-5.0.0 \"HTTP/2 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/xi5ifavk33?sm-sdk=python-5.0.0 \"HTTP/2 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/xi5ifavk33/transcript?format=txt \"HTTP/2 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/xi5ifavk33/transcript?format=txt \"HTTP/2 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/xi5ifavk33/transcript?format=txt \"HTTP/2 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/xi5ifavk33/transcript?format=txt \"HTTP/2 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/xi5ifavk33/transcript?format=txt \"HTTP/2 200 OK\"\n",
      "HTTP Request: GET https://asr.api.speechmatics.com/v2/jobs/xi5ifavk33/transcript?format=txt \"HTTP/2 200 OK\"\n",
      "Processed 5/5 samples\n",
      "Processed 5/5 samples\n",
      "Processed 5/5 samples\n",
      "openai_gpt-4o-transcribe on en: WER=0.1351, Avg Time=1.07s, Samples=5\n",
      "openai_gpt-4o-transcribe on en: WER=0.1351, Avg Time=1.07s, Samples=5\n",
      "openai_gpt-4o-transcribe on en: WER=0.1351, Avg Time=1.07s, Samples=5\n",
      "openai_gpt-4o-mini-transcribe on en: WER=0.1622, Avg Time=1.12s, Samples=5\n",
      "openai_gpt-4o-mini-transcribe on en: WER=0.1622, Avg Time=1.12s, Samples=5\n",
      "openai_gpt-4o-mini-transcribe on en: WER=0.1622, Avg Time=1.12s, Samples=5\n",
      "openai_whisper-1 on en: WER=0.1892, Avg Time=1.41s, Samples=5\n",
      "openai_whisper-1 on en: WER=0.1892, Avg Time=1.41s, Samples=5\n",
      "openai_whisper-1 on en: WER=0.1892, Avg Time=1.41s, Samples=5\n",
      "groq_whisper-large-v3 on en: WER=0.2432, Avg Time=0.31s, Samples=5\n",
      "groq_whisper-large-v3 on en: WER=0.2432, Avg Time=0.31s, Samples=5\n",
      "Processed 5/5 samples\n",
      "Processed 5/5 samples\n",
      "Processed 5/5 samples\n",
      "openai_gpt-4o-transcribe on en: WER=0.1351, Avg Time=1.07s, Samples=5\n",
      "openai_gpt-4o-transcribe on en: WER=0.1351, Avg Time=1.07s, Samples=5\n",
      "openai_gpt-4o-transcribe on en: WER=0.1351, Avg Time=1.07s, Samples=5\n",
      "openai_gpt-4o-mini-transcribe on en: WER=0.1622, Avg Time=1.12s, Samples=5\n",
      "openai_gpt-4o-mini-transcribe on en: WER=0.1622, Avg Time=1.12s, Samples=5\n",
      "openai_gpt-4o-mini-transcribe on en: WER=0.1622, Avg Time=1.12s, Samples=5\n",
      "openai_whisper-1 on en: WER=0.1892, Avg Time=1.41s, Samples=5\n",
      "openai_whisper-1 on en: WER=0.1892, Avg Time=1.41s, Samples=5\n",
      "openai_whisper-1 on en: WER=0.1892, Avg Time=1.41s, Samples=5\n",
      "groq_whisper-large-v3 on en: WER=0.2432, Avg Time=0.31s, Samples=5\n",
      "groq_whisper-large-v3 on en: WER=0.2432, Avg Time=0.31s, Samples=5\n",
      "groq_whisper-large-v3 on en: WER=0.2432, Avg Time=0.31s, Samples=5\n",
      "groq_whisper-large-v3-turbo on en: WER=0.1892, Avg Time=0.28s, Samples=5\n",
      "groq_whisper-large-v3-turbo on en: WER=0.1892, Avg Time=0.28s, Samples=5\n",
      "groq_whisper-large-v3-turbo on en: WER=0.1892, Avg Time=0.28s, Samples=5\n",
      "elevenlabs_scribe_v1 on en: WER=0.1081, Avg Time=0.76s, Samples=5\n",
      "elevenlabs_scribe_v1 on en: WER=0.1081, Avg Time=0.76s, Samples=5\n",
      "elevenlabs_scribe_v1 on en: WER=0.1081, Avg Time=0.76s, Samples=5\n",
      "speechmatics_en on en: WER=0.1622, Avg Time=2.65s, Samples=5\n",
      "speechmatics_en on en: WER=0.1622, Avg Time=2.65s, Samples=5\n",
      "speechmatics_en on en: WER=0.1622, Avg Time=2.65s, Samples=5\n",
      "gladia_pre-recorded on en: WER=0.2432, Avg Time=6.39s, Samples=5\n",
      "gladia_pre-recorded on en: WER=0.2432, Avg Time=6.39s, Samples=5\n",
      "gladia_pre-recorded on en: WER=0.2432, Avg Time=6.39s, Samples=5\n",
      "groq_whisper-large-v3 on en: WER=0.2432, Avg Time=0.31s, Samples=5\n",
      "groq_whisper-large-v3-turbo on en: WER=0.1892, Avg Time=0.28s, Samples=5\n",
      "groq_whisper-large-v3-turbo on en: WER=0.1892, Avg Time=0.28s, Samples=5\n",
      "groq_whisper-large-v3-turbo on en: WER=0.1892, Avg Time=0.28s, Samples=5\n",
      "elevenlabs_scribe_v1 on en: WER=0.1081, Avg Time=0.76s, Samples=5\n",
      "elevenlabs_scribe_v1 on en: WER=0.1081, Avg Time=0.76s, Samples=5\n",
      "elevenlabs_scribe_v1 on en: WER=0.1081, Avg Time=0.76s, Samples=5\n",
      "speechmatics_en on en: WER=0.1622, Avg Time=2.65s, Samples=5\n",
      "speechmatics_en on en: WER=0.1622, Avg Time=2.65s, Samples=5\n",
      "speechmatics_en on en: WER=0.1622, Avg Time=2.65s, Samples=5\n",
      "gladia_pre-recorded on en: WER=0.2432, Avg Time=6.39s, Samples=5\n",
      "gladia_pre-recorded on en: WER=0.2432, Avg Time=6.39s, Samples=5\n",
      "gladia_pre-recorded on en: WER=0.2432, Avg Time=6.39s, Samples=5\n",
      "Evaluation completed\n",
      "Evaluation completed\n",
      "Evaluation completed\n",
      "Evaluation completed\n",
      "Evaluation completed\n",
      "Evaluation completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final results:\n",
      "en:\n",
      "  openai_gpt-4o-transcribe: WER=0.1351, Avg Time=1.07s, Samples=5\n",
      "  openai_gpt-4o-mini-transcribe: WER=0.1622, Avg Time=1.12s, Samples=5\n",
      "  openai_whisper-1: WER=0.1892, Avg Time=1.41s, Samples=5\n",
      "  groq_whisper-large-v3: WER=0.2432, Avg Time=0.31s, Samples=5\n",
      "  groq_whisper-large-v3-turbo: WER=0.1892, Avg Time=0.28s, Samples=5\n",
      "  elevenlabs_scribe_v1: WER=0.1081, Avg Time=0.76s, Samples=5\n",
      "  speechmatics_en: WER=0.1622, Avg Time=2.65s, Samples=5\n",
      "  gladia_pre-recorded: WER=0.2432, Avg Time=6.39s, Samples=5\n"
     ]
    }
   ],
   "source": [
    "# Run evaluation for all services on 5 English samples (PARALLEL VERSION)\n",
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "print(f\"Number of services: {len(SERVICES)}\")\n",
    "print(f\"Services: {SERVICES}\")\n",
    "results = run_wer_evaluation_parallel(\n",
    "    dataset_path=os.getenv(\"CV22_PATH\"),\n",
    "    services=SERVICES,  # All services\n",
    "    service_funcs=SERVICE_FUNCS,\n",
    "    languages=[\"en\"],  # English only\n",
    "    results_file=f\"results/wer_results_{timestamp}.json\",\n",
    "    checkpoint_file=f\"results/wer_checkpoint_{timestamp}.json\",\n",
    "    log_file=f\"logs/wer_eval_{timestamp}.log\",\n",
    "    max_retries=2,  # Fewer retries for testing\n",
    "    n_samples=5,  # 5 samples\n",
    "    max_workers=8,  # Run up to 8 services in parallel per sample\n",
    ")\n",
    "\n",
    "print(\"\\nFinal results:\")\n",
    "for lang, data in results.items():\n",
    "    print(f\"{lang}:\")\n",
    "    for service, metrics in data.items():\n",
    "        print(\n",
    "            f\"  {service}: WER={metrics['wer']:.4f}, Avg Time={metrics['timing']:.2f}s, Samples={metrics['n_samples']}\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cda9efec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing language: ru\n",
      "Processing language: ru\n",
      "Processing language: ru\n",
      "Processing language: ru\n",
      "Processing language: ru\n",
      "Processing language: ru\n",
      "Processing language: ru\n",
      "Processing language: ru\n",
      "Processing language: ru\n",
      "Processing language: ru\n",
      "Processing language: ru\n",
      "Evaluating 8 services on ru\n",
      "Evaluating 8 services on ru\n",
      "Evaluating 8 services on ru\n",
      "Evaluating 8 services on ru\n",
      "Evaluating 8 services on ru\n",
      "Evaluating 8 services on ru\n",
      "Evaluating 8 services on ru\n",
      "Evaluating 8 services on ru\n",
      "Evaluating 8 services on ru\n",
      "Evaluating 8 services on ru\n",
      "Evaluating 8 services on ru\n",
      "Evaluating 8 services on ru\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 languages: ['ru', 'it', 'en', 'es', 'ja']...\n",
      "Found 10 languages: ['ru', 'it', 'en', 'es', 'ja', 'de', 'pl', 'pt', 'fr', 'zh-CN']\n",
      "Number of services: 8\n",
      "Services: ['openai_gpt-4o-transcribe', 'openai_gpt-4o-mini-transcribe', 'openai_whisper-1', 'groq_whisper-large-v3', 'groq_whisper-large-v3-turbo', 'elevenlabs_scribe_v1', 'speechmatics_en', 'gladia_pre-recorded']\n",
      "Found 10 languages: ['ru', 'it', 'en', 'es', 'ja']...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "openai_gpt-4o-transcribe on ru: WER=0.5000, Avg Time=1.09s, Samples=2\n",
      "openai_gpt-4o-transcribe on ru: WER=0.5000, Avg Time=1.09s, Samples=2\n",
      "openai_gpt-4o-transcribe on ru: WER=0.5000, Avg Time=1.09s, Samples=2\n",
      "openai_gpt-4o-transcribe on ru: WER=0.5000, Avg Time=1.09s, Samples=2\n",
      "openai_gpt-4o-transcribe on ru: WER=0.5000, Avg Time=1.09s, Samples=2\n",
      "openai_gpt-4o-transcribe on ru: WER=0.5000, Avg Time=1.09s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on ru: WER=0.3750, Avg Time=1.53s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on ru: WER=0.3750, Avg Time=1.53s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on ru: WER=0.3750, Avg Time=1.53s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on ru: WER=0.3750, Avg Time=1.53s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on ru: WER=0.3750, Avg Time=1.53s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on ru: WER=0.3750, Avg Time=1.53s, Samples=2\n",
      "openai_whisper-1 on ru: WER=0.6250, Avg Time=1.46s, Samples=2\n",
      "openai_whisper-1 on ru: WER=0.6250, Avg Time=1.46s, Samples=2\n",
      "openai_whisper-1 on ru: WER=0.6250, Avg Time=1.46s, Samples=2\n",
      "openai_whisper-1 on ru: WER=0.6250, Avg Time=1.46s, Samples=2\n",
      "openai_whisper-1 on ru: WER=0.6250, Avg Time=1.46s, Samples=2\n",
      "openai_whisper-1 on ru: WER=0.6250, Avg Time=1.46s, Samples=2\n",
      "openai_gpt-4o-transcribe on ru: WER=0.5000, Avg Time=1.09s, Samples=2\n",
      "openai_gpt-4o-transcribe on ru: WER=0.5000, Avg Time=1.09s, Samples=2\n",
      "openai_gpt-4o-transcribe on ru: WER=0.5000, Avg Time=1.09s, Samples=2\n",
      "openai_gpt-4o-transcribe on ru: WER=0.5000, Avg Time=1.09s, Samples=2\n",
      "openai_gpt-4o-transcribe on ru: WER=0.5000, Avg Time=1.09s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on ru: WER=0.3750, Avg Time=1.53s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on ru: WER=0.3750, Avg Time=1.53s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on ru: WER=0.3750, Avg Time=1.53s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on ru: WER=0.3750, Avg Time=1.53s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on ru: WER=0.3750, Avg Time=1.53s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on ru: WER=0.3750, Avg Time=1.53s, Samples=2\n",
      "openai_whisper-1 on ru: WER=0.6250, Avg Time=1.46s, Samples=2\n",
      "openai_whisper-1 on ru: WER=0.6250, Avg Time=1.46s, Samples=2\n",
      "openai_whisper-1 on ru: WER=0.6250, Avg Time=1.46s, Samples=2\n",
      "openai_whisper-1 on ru: WER=0.6250, Avg Time=1.46s, Samples=2\n",
      "openai_whisper-1 on ru: WER=0.6250, Avg Time=1.46s, Samples=2\n",
      "openai_whisper-1 on ru: WER=0.6250, Avg Time=1.46s, Samples=2\n",
      "groq_whisper-large-v3 on ru: WER=0.6250, Avg Time=0.27s, Samples=2\n",
      "groq_whisper-large-v3 on ru: WER=0.6250, Avg Time=0.27s, Samples=2\n",
      "groq_whisper-large-v3 on ru: WER=0.6250, Avg Time=0.27s, Samples=2\n",
      "groq_whisper-large-v3 on ru: WER=0.6250, Avg Time=0.27s, Samples=2\n",
      "groq_whisper-large-v3 on ru: WER=0.6250, Avg Time=0.27s, Samples=2\n",
      "groq_whisper-large-v3 on ru: WER=0.6250, Avg Time=0.27s, Samples=2\n",
      "groq_whisper-large-v3-turbo on ru: WER=0.5000, Avg Time=0.26s, Samples=2\n",
      "groq_whisper-large-v3-turbo on ru: WER=0.5000, Avg Time=0.26s, Samples=2\n",
      "groq_whisper-large-v3-turbo on ru: WER=0.5000, Avg Time=0.26s, Samples=2\n",
      "groq_whisper-large-v3-turbo on ru: WER=0.5000, Avg Time=0.26s, Samples=2\n",
      "groq_whisper-large-v3-turbo on ru: WER=0.5000, Avg Time=0.26s, Samples=2\n",
      "groq_whisper-large-v3-turbo on ru: WER=0.5000, Avg Time=0.26s, Samples=2\n",
      "elevenlabs_scribe_v1 on ru: WER=2.0000, Avg Time=0.76s, Samples=2\n",
      "elevenlabs_scribe_v1 on ru: WER=2.0000, Avg Time=0.76s, Samples=2\n",
      "elevenlabs_scribe_v1 on ru: WER=2.0000, Avg Time=0.76s, Samples=2\n",
      "elevenlabs_scribe_v1 on ru: WER=2.0000, Avg Time=0.76s, Samples=2\n",
      "elevenlabs_scribe_v1 on ru: WER=2.0000, Avg Time=0.76s, Samples=2\n",
      "elevenlabs_scribe_v1 on ru: WER=2.0000, Avg Time=0.76s, Samples=2\n",
      "speechmatics_en on ru: WER=1.1250, Avg Time=2.26s, Samples=2\n",
      "groq_whisper-large-v3 on ru: WER=0.6250, Avg Time=0.27s, Samples=2\n",
      "groq_whisper-large-v3 on ru: WER=0.6250, Avg Time=0.27s, Samples=2\n",
      "groq_whisper-large-v3 on ru: WER=0.6250, Avg Time=0.27s, Samples=2\n",
      "groq_whisper-large-v3 on ru: WER=0.6250, Avg Time=0.27s, Samples=2\n",
      "groq_whisper-large-v3 on ru: WER=0.6250, Avg Time=0.27s, Samples=2\n",
      "groq_whisper-large-v3 on ru: WER=0.6250, Avg Time=0.27s, Samples=2\n",
      "groq_whisper-large-v3-turbo on ru: WER=0.5000, Avg Time=0.26s, Samples=2\n",
      "groq_whisper-large-v3-turbo on ru: WER=0.5000, Avg Time=0.26s, Samples=2\n",
      "groq_whisper-large-v3-turbo on ru: WER=0.5000, Avg Time=0.26s, Samples=2\n",
      "groq_whisper-large-v3-turbo on ru: WER=0.5000, Avg Time=0.26s, Samples=2\n",
      "groq_whisper-large-v3-turbo on ru: WER=0.5000, Avg Time=0.26s, Samples=2\n",
      "groq_whisper-large-v3-turbo on ru: WER=0.5000, Avg Time=0.26s, Samples=2\n",
      "elevenlabs_scribe_v1 on ru: WER=2.0000, Avg Time=0.76s, Samples=2\n",
      "elevenlabs_scribe_v1 on ru: WER=2.0000, Avg Time=0.76s, Samples=2\n",
      "elevenlabs_scribe_v1 on ru: WER=2.0000, Avg Time=0.76s, Samples=2\n",
      "elevenlabs_scribe_v1 on ru: WER=2.0000, Avg Time=0.76s, Samples=2\n",
      "elevenlabs_scribe_v1 on ru: WER=2.0000, Avg Time=0.76s, Samples=2\n",
      "elevenlabs_scribe_v1 on ru: WER=2.0000, Avg Time=0.76s, Samples=2\n",
      "speechmatics_en on ru: WER=1.1250, Avg Time=2.26s, Samples=2\n",
      "speechmatics_en on ru: WER=1.1250, Avg Time=2.26s, Samples=2\n",
      "speechmatics_en on ru: WER=1.1250, Avg Time=2.26s, Samples=2\n",
      "speechmatics_en on ru: WER=1.1250, Avg Time=2.26s, Samples=2\n",
      "speechmatics_en on ru: WER=1.1250, Avg Time=2.26s, Samples=2\n",
      "speechmatics_en on ru: WER=1.1250, Avg Time=2.26s, Samples=2\n",
      "gladia_pre-recorded on ru: WER=0.1250, Avg Time=6.35s, Samples=2\n",
      "gladia_pre-recorded on ru: WER=0.1250, Avg Time=6.35s, Samples=2\n",
      "gladia_pre-recorded on ru: WER=0.1250, Avg Time=6.35s, Samples=2\n",
      "gladia_pre-recorded on ru: WER=0.1250, Avg Time=6.35s, Samples=2\n",
      "gladia_pre-recorded on ru: WER=0.1250, Avg Time=6.35s, Samples=2\n",
      "gladia_pre-recorded on ru: WER=0.1250, Avg Time=6.35s, Samples=2\n",
      "Processing language: it\n",
      "Processing language: it\n",
      "Processing language: it\n",
      "Processing language: it\n",
      "Processing language: it\n",
      "Processing language: it\n",
      "speechmatics_en on ru: WER=1.1250, Avg Time=2.26s, Samples=2\n",
      "speechmatics_en on ru: WER=1.1250, Avg Time=2.26s, Samples=2\n",
      "speechmatics_en on ru: WER=1.1250, Avg Time=2.26s, Samples=2\n",
      "speechmatics_en on ru: WER=1.1250, Avg Time=2.26s, Samples=2\n",
      "speechmatics_en on ru: WER=1.1250, Avg Time=2.26s, Samples=2\n",
      "gladia_pre-recorded on ru: WER=0.1250, Avg Time=6.35s, Samples=2\n",
      "gladia_pre-recorded on ru: WER=0.1250, Avg Time=6.35s, Samples=2\n",
      "gladia_pre-recorded on ru: WER=0.1250, Avg Time=6.35s, Samples=2\n",
      "gladia_pre-recorded on ru: WER=0.1250, Avg Time=6.35s, Samples=2\n",
      "gladia_pre-recorded on ru: WER=0.1250, Avg Time=6.35s, Samples=2\n",
      "gladia_pre-recorded on ru: WER=0.1250, Avg Time=6.35s, Samples=2\n",
      "Processing language: it\n",
      "Processing language: it\n",
      "Processing language: it\n",
      "Processing language: it\n",
      "Processing language: it\n",
      "Processing language: it\n",
      "Evaluating 8 services on it\n",
      "Evaluating 8 services on it\n",
      "Evaluating 8 services on it\n",
      "Evaluating 8 services on it\n",
      "Evaluating 8 services on it\n",
      "Evaluating 8 services on it\n",
      "Evaluating 8 services on it\n",
      "Evaluating 8 services on it\n",
      "Evaluating 8 services on it\n",
      "Evaluating 8 services on it\n",
      "Evaluating 8 services on it\n",
      "Evaluating 8 services on it\n",
      "openai_gpt-4o-transcribe on it: WER=0.1053, Avg Time=1.17s, Samples=2\n",
      "openai_gpt-4o-transcribe on it: WER=0.1053, Avg Time=1.17s, Samples=2\n",
      "openai_gpt-4o-transcribe on it: WER=0.1053, Avg Time=1.17s, Samples=2\n",
      "openai_gpt-4o-transcribe on it: WER=0.1053, Avg Time=1.17s, Samples=2\n",
      "openai_gpt-4o-transcribe on it: WER=0.1053, Avg Time=1.17s, Samples=2\n",
      "openai_gpt-4o-transcribe on it: WER=0.1053, Avg Time=1.17s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on it: WER=0.1053, Avg Time=0.88s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on it: WER=0.1053, Avg Time=0.88s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on it: WER=0.1053, Avg Time=0.88s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on it: WER=0.1053, Avg Time=0.88s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on it: WER=0.1053, Avg Time=0.88s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on it: WER=0.1053, Avg Time=0.88s, Samples=2\n",
      "openai_whisper-1 on it: WER=0.0000, Avg Time=1.82s, Samples=2\n",
      "openai_whisper-1 on it: WER=0.0000, Avg Time=1.82s, Samples=2\n",
      "openai_whisper-1 on it: WER=0.0000, Avg Time=1.82s, Samples=2\n",
      "openai_whisper-1 on it: WER=0.0000, Avg Time=1.82s, Samples=2\n",
      "openai_whisper-1 on it: WER=0.0000, Avg Time=1.82s, Samples=2\n",
      "openai_whisper-1 on it: WER=0.0000, Avg Time=1.82s, Samples=2\n",
      "openai_gpt-4o-transcribe on it: WER=0.1053, Avg Time=1.17s, Samples=2\n",
      "openai_gpt-4o-transcribe on it: WER=0.1053, Avg Time=1.17s, Samples=2\n",
      "openai_gpt-4o-transcribe on it: WER=0.1053, Avg Time=1.17s, Samples=2\n",
      "openai_gpt-4o-transcribe on it: WER=0.1053, Avg Time=1.17s, Samples=2\n",
      "openai_gpt-4o-transcribe on it: WER=0.1053, Avg Time=1.17s, Samples=2\n",
      "openai_gpt-4o-transcribe on it: WER=0.1053, Avg Time=1.17s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on it: WER=0.1053, Avg Time=0.88s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on it: WER=0.1053, Avg Time=0.88s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on it: WER=0.1053, Avg Time=0.88s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on it: WER=0.1053, Avg Time=0.88s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on it: WER=0.1053, Avg Time=0.88s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on it: WER=0.1053, Avg Time=0.88s, Samples=2\n",
      "openai_whisper-1 on it: WER=0.0000, Avg Time=1.82s, Samples=2\n",
      "openai_whisper-1 on it: WER=0.0000, Avg Time=1.82s, Samples=2\n",
      "openai_whisper-1 on it: WER=0.0000, Avg Time=1.82s, Samples=2\n",
      "openai_whisper-1 on it: WER=0.0000, Avg Time=1.82s, Samples=2\n",
      "openai_whisper-1 on it: WER=0.0000, Avg Time=1.82s, Samples=2\n",
      "openai_whisper-1 on it: WER=0.0000, Avg Time=1.82s, Samples=2\n",
      "groq_whisper-large-v3 on it: WER=0.0000, Avg Time=0.39s, Samples=2\n",
      "groq_whisper-large-v3 on it: WER=0.0000, Avg Time=0.39s, Samples=2\n",
      "groq_whisper-large-v3 on it: WER=0.0000, Avg Time=0.39s, Samples=2\n",
      "groq_whisper-large-v3 on it: WER=0.0000, Avg Time=0.39s, Samples=2\n",
      "groq_whisper-large-v3 on it: WER=0.0000, Avg Time=0.39s, Samples=2\n",
      "groq_whisper-large-v3 on it: WER=0.0000, Avg Time=0.39s, Samples=2\n",
      "groq_whisper-large-v3-turbo on it: WER=0.0000, Avg Time=0.27s, Samples=2\n",
      "groq_whisper-large-v3-turbo on it: WER=0.0000, Avg Time=0.27s, Samples=2\n",
      "groq_whisper-large-v3-turbo on it: WER=0.0000, Avg Time=0.27s, Samples=2\n",
      "groq_whisper-large-v3-turbo on it: WER=0.0000, Avg Time=0.27s, Samples=2\n",
      "groq_whisper-large-v3-turbo on it: WER=0.0000, Avg Time=0.27s, Samples=2\n",
      "groq_whisper-large-v3-turbo on it: WER=0.0000, Avg Time=0.27s, Samples=2\n",
      "elevenlabs_scribe_v1 on it: WER=0.0000, Avg Time=0.81s, Samples=2\n",
      "elevenlabs_scribe_v1 on it: WER=0.0000, Avg Time=0.81s, Samples=2\n",
      "elevenlabs_scribe_v1 on it: WER=0.0000, Avg Time=0.81s, Samples=2\n",
      "elevenlabs_scribe_v1 on it: WER=0.0000, Avg Time=0.81s, Samples=2\n",
      "elevenlabs_scribe_v1 on it: WER=0.0000, Avg Time=0.81s, Samples=2\n",
      "elevenlabs_scribe_v1 on it: WER=0.0000, Avg Time=0.81s, Samples=2\n",
      "speechmatics_en on it: WER=0.9474, Avg Time=2.67s, Samples=2\n",
      "speechmatics_en on it: WER=0.9474, Avg Time=2.67s, Samples=2\n",
      "speechmatics_en on it: WER=0.9474, Avg Time=2.67s, Samples=2\n",
      "groq_whisper-large-v3 on it: WER=0.0000, Avg Time=0.39s, Samples=2\n",
      "groq_whisper-large-v3 on it: WER=0.0000, Avg Time=0.39s, Samples=2\n",
      "groq_whisper-large-v3 on it: WER=0.0000, Avg Time=0.39s, Samples=2\n",
      "groq_whisper-large-v3 on it: WER=0.0000, Avg Time=0.39s, Samples=2\n",
      "groq_whisper-large-v3 on it: WER=0.0000, Avg Time=0.39s, Samples=2\n",
      "groq_whisper-large-v3 on it: WER=0.0000, Avg Time=0.39s, Samples=2\n",
      "groq_whisper-large-v3-turbo on it: WER=0.0000, Avg Time=0.27s, Samples=2\n",
      "groq_whisper-large-v3-turbo on it: WER=0.0000, Avg Time=0.27s, Samples=2\n",
      "groq_whisper-large-v3-turbo on it: WER=0.0000, Avg Time=0.27s, Samples=2\n",
      "groq_whisper-large-v3-turbo on it: WER=0.0000, Avg Time=0.27s, Samples=2\n",
      "groq_whisper-large-v3-turbo on it: WER=0.0000, Avg Time=0.27s, Samples=2\n",
      "groq_whisper-large-v3-turbo on it: WER=0.0000, Avg Time=0.27s, Samples=2\n",
      "elevenlabs_scribe_v1 on it: WER=0.0000, Avg Time=0.81s, Samples=2\n",
      "elevenlabs_scribe_v1 on it: WER=0.0000, Avg Time=0.81s, Samples=2\n",
      "elevenlabs_scribe_v1 on it: WER=0.0000, Avg Time=0.81s, Samples=2\n",
      "elevenlabs_scribe_v1 on it: WER=0.0000, Avg Time=0.81s, Samples=2\n",
      "elevenlabs_scribe_v1 on it: WER=0.0000, Avg Time=0.81s, Samples=2\n",
      "elevenlabs_scribe_v1 on it: WER=0.0000, Avg Time=0.81s, Samples=2\n",
      "speechmatics_en on it: WER=0.9474, Avg Time=2.67s, Samples=2\n",
      "speechmatics_en on it: WER=0.9474, Avg Time=2.67s, Samples=2\n",
      "speechmatics_en on it: WER=0.9474, Avg Time=2.67s, Samples=2\n",
      "speechmatics_en on it: WER=0.9474, Avg Time=2.67s, Samples=2\n",
      "speechmatics_en on it: WER=0.9474, Avg Time=2.67s, Samples=2\n",
      "speechmatics_en on it: WER=0.9474, Avg Time=2.67s, Samples=2\n",
      "gladia_pre-recorded on it: WER=0.0000, Avg Time=6.43s, Samples=2\n",
      "gladia_pre-recorded on it: WER=0.0000, Avg Time=6.43s, Samples=2\n",
      "gladia_pre-recorded on it: WER=0.0000, Avg Time=6.43s, Samples=2\n",
      "gladia_pre-recorded on it: WER=0.0000, Avg Time=6.43s, Samples=2\n",
      "gladia_pre-recorded on it: WER=0.0000, Avg Time=6.43s, Samples=2\n",
      "gladia_pre-recorded on it: WER=0.0000, Avg Time=6.43s, Samples=2\n",
      "Processing language: en\n",
      "Processing language: en\n",
      "Processing language: en\n",
      "Processing language: en\n",
      "Processing language: en\n",
      "Processing language: en\n",
      "speechmatics_en on it: WER=0.9474, Avg Time=2.67s, Samples=2\n",
      "speechmatics_en on it: WER=0.9474, Avg Time=2.67s, Samples=2\n",
      "speechmatics_en on it: WER=0.9474, Avg Time=2.67s, Samples=2\n",
      "gladia_pre-recorded on it: WER=0.0000, Avg Time=6.43s, Samples=2\n",
      "gladia_pre-recorded on it: WER=0.0000, Avg Time=6.43s, Samples=2\n",
      "gladia_pre-recorded on it: WER=0.0000, Avg Time=6.43s, Samples=2\n",
      "gladia_pre-recorded on it: WER=0.0000, Avg Time=6.43s, Samples=2\n",
      "gladia_pre-recorded on it: WER=0.0000, Avg Time=6.43s, Samples=2\n",
      "gladia_pre-recorded on it: WER=0.0000, Avg Time=6.43s, Samples=2\n",
      "Processing language: en\n",
      "Processing language: en\n",
      "Processing language: en\n",
      "Processing language: en\n",
      "Processing language: en\n",
      "Processing language: en\n",
      "Evaluating 8 services on en\n",
      "Evaluating 8 services on en\n",
      "Evaluating 8 services on en\n",
      "Evaluating 8 services on en\n",
      "Evaluating 8 services on en\n",
      "Evaluating 8 services on en\n",
      "Evaluating 8 services on en\n",
      "Evaluating 8 services on en\n",
      "Evaluating 8 services on en\n",
      "Evaluating 8 services on en\n",
      "Evaluating 8 services on en\n",
      "Evaluating 8 services on en\n",
      "openai_gpt-4o-transcribe on en: WER=0.0588, Avg Time=1.14s, Samples=2\n",
      "openai_gpt-4o-transcribe on en: WER=0.0588, Avg Time=1.14s, Samples=2\n",
      "openai_gpt-4o-transcribe on en: WER=0.0588, Avg Time=1.14s, Samples=2\n",
      "openai_gpt-4o-transcribe on en: WER=0.0588, Avg Time=1.14s, Samples=2\n",
      "openai_gpt-4o-transcribe on en: WER=0.0588, Avg Time=1.14s, Samples=2\n",
      "openai_gpt-4o-transcribe on en: WER=0.0588, Avg Time=1.14s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on en: WER=0.0588, Avg Time=0.91s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on en: WER=0.0588, Avg Time=0.91s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on en: WER=0.0588, Avg Time=0.91s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on en: WER=0.0588, Avg Time=0.91s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on en: WER=0.0588, Avg Time=0.91s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on en: WER=0.0588, Avg Time=0.91s, Samples=2\n",
      "openai_whisper-1 on en: WER=0.0588, Avg Time=1.23s, Samples=2\n",
      "openai_gpt-4o-transcribe on en: WER=0.0588, Avg Time=1.14s, Samples=2\n",
      "openai_gpt-4o-transcribe on en: WER=0.0588, Avg Time=1.14s, Samples=2\n",
      "openai_gpt-4o-transcribe on en: WER=0.0588, Avg Time=1.14s, Samples=2\n",
      "openai_gpt-4o-transcribe on en: WER=0.0588, Avg Time=1.14s, Samples=2\n",
      "openai_gpt-4o-transcribe on en: WER=0.0588, Avg Time=1.14s, Samples=2\n",
      "openai_gpt-4o-transcribe on en: WER=0.0588, Avg Time=1.14s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on en: WER=0.0588, Avg Time=0.91s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on en: WER=0.0588, Avg Time=0.91s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on en: WER=0.0588, Avg Time=0.91s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on en: WER=0.0588, Avg Time=0.91s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on en: WER=0.0588, Avg Time=0.91s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on en: WER=0.0588, Avg Time=0.91s, Samples=2\n",
      "openai_whisper-1 on en: WER=0.0588, Avg Time=1.23s, Samples=2\n",
      "openai_whisper-1 on en: WER=0.0588, Avg Time=1.23s, Samples=2\n",
      "openai_whisper-1 on en: WER=0.0588, Avg Time=1.23s, Samples=2\n",
      "openai_whisper-1 on en: WER=0.0588, Avg Time=1.23s, Samples=2\n",
      "openai_whisper-1 on en: WER=0.0588, Avg Time=1.23s, Samples=2\n",
      "openai_whisper-1 on en: WER=0.0588, Avg Time=1.23s, Samples=2\n",
      "groq_whisper-large-v3 on en: WER=0.0588, Avg Time=0.29s, Samples=2\n",
      "groq_whisper-large-v3 on en: WER=0.0588, Avg Time=0.29s, Samples=2\n",
      "groq_whisper-large-v3 on en: WER=0.0588, Avg Time=0.29s, Samples=2\n",
      "groq_whisper-large-v3 on en: WER=0.0588, Avg Time=0.29s, Samples=2\n",
      "groq_whisper-large-v3 on en: WER=0.0588, Avg Time=0.29s, Samples=2\n",
      "groq_whisper-large-v3 on en: WER=0.0588, Avg Time=0.29s, Samples=2\n",
      "groq_whisper-large-v3-turbo on en: WER=0.0588, Avg Time=0.31s, Samples=2\n",
      "groq_whisper-large-v3-turbo on en: WER=0.0588, Avg Time=0.31s, Samples=2\n",
      "groq_whisper-large-v3-turbo on en: WER=0.0588, Avg Time=0.31s, Samples=2\n",
      "groq_whisper-large-v3-turbo on en: WER=0.0588, Avg Time=0.31s, Samples=2\n",
      "groq_whisper-large-v3-turbo on en: WER=0.0588, Avg Time=0.31s, Samples=2\n",
      "groq_whisper-large-v3-turbo on en: WER=0.0588, Avg Time=0.31s, Samples=2\n",
      "elevenlabs_scribe_v1 on en: WER=0.0588, Avg Time=0.73s, Samples=2\n",
      "elevenlabs_scribe_v1 on en: WER=0.0588, Avg Time=0.73s, Samples=2\n",
      "openai_whisper-1 on en: WER=0.0588, Avg Time=1.23s, Samples=2\n",
      "openai_whisper-1 on en: WER=0.0588, Avg Time=1.23s, Samples=2\n",
      "openai_whisper-1 on en: WER=0.0588, Avg Time=1.23s, Samples=2\n",
      "openai_whisper-1 on en: WER=0.0588, Avg Time=1.23s, Samples=2\n",
      "openai_whisper-1 on en: WER=0.0588, Avg Time=1.23s, Samples=2\n",
      "groq_whisper-large-v3 on en: WER=0.0588, Avg Time=0.29s, Samples=2\n",
      "groq_whisper-large-v3 on en: WER=0.0588, Avg Time=0.29s, Samples=2\n",
      "groq_whisper-large-v3 on en: WER=0.0588, Avg Time=0.29s, Samples=2\n",
      "groq_whisper-large-v3 on en: WER=0.0588, Avg Time=0.29s, Samples=2\n",
      "groq_whisper-large-v3 on en: WER=0.0588, Avg Time=0.29s, Samples=2\n",
      "groq_whisper-large-v3 on en: WER=0.0588, Avg Time=0.29s, Samples=2\n",
      "groq_whisper-large-v3-turbo on en: WER=0.0588, Avg Time=0.31s, Samples=2\n",
      "groq_whisper-large-v3-turbo on en: WER=0.0588, Avg Time=0.31s, Samples=2\n",
      "groq_whisper-large-v3-turbo on en: WER=0.0588, Avg Time=0.31s, Samples=2\n",
      "groq_whisper-large-v3-turbo on en: WER=0.0588, Avg Time=0.31s, Samples=2\n",
      "groq_whisper-large-v3-turbo on en: WER=0.0588, Avg Time=0.31s, Samples=2\n",
      "groq_whisper-large-v3-turbo on en: WER=0.0588, Avg Time=0.31s, Samples=2\n",
      "elevenlabs_scribe_v1 on en: WER=0.0588, Avg Time=0.73s, Samples=2\n",
      "elevenlabs_scribe_v1 on en: WER=0.0588, Avg Time=0.73s, Samples=2\n",
      "elevenlabs_scribe_v1 on en: WER=0.0588, Avg Time=0.73s, Samples=2\n",
      "elevenlabs_scribe_v1 on en: WER=0.0588, Avg Time=0.73s, Samples=2\n",
      "elevenlabs_scribe_v1 on en: WER=0.0588, Avg Time=0.73s, Samples=2\n",
      "elevenlabs_scribe_v1 on en: WER=0.0588, Avg Time=0.73s, Samples=2\n",
      "speechmatics_en on en: WER=0.0588, Avg Time=2.99s, Samples=2\n",
      "speechmatics_en on en: WER=0.0588, Avg Time=2.99s, Samples=2\n",
      "speechmatics_en on en: WER=0.0588, Avg Time=2.99s, Samples=2\n",
      "speechmatics_en on en: WER=0.0588, Avg Time=2.99s, Samples=2\n",
      "speechmatics_en on en: WER=0.0588, Avg Time=2.99s, Samples=2\n",
      "speechmatics_en on en: WER=0.0588, Avg Time=2.99s, Samples=2\n",
      "gladia_pre-recorded on en: WER=0.0588, Avg Time=6.38s, Samples=2\n",
      "gladia_pre-recorded on en: WER=0.0588, Avg Time=6.38s, Samples=2\n",
      "gladia_pre-recorded on en: WER=0.0588, Avg Time=6.38s, Samples=2\n",
      "gladia_pre-recorded on en: WER=0.0588, Avg Time=6.38s, Samples=2\n",
      "gladia_pre-recorded on en: WER=0.0588, Avg Time=6.38s, Samples=2\n",
      "gladia_pre-recorded on en: WER=0.0588, Avg Time=6.38s, Samples=2\n",
      "elevenlabs_scribe_v1 on en: WER=0.0588, Avg Time=0.73s, Samples=2\n",
      "elevenlabs_scribe_v1 on en: WER=0.0588, Avg Time=0.73s, Samples=2\n",
      "elevenlabs_scribe_v1 on en: WER=0.0588, Avg Time=0.73s, Samples=2\n",
      "elevenlabs_scribe_v1 on en: WER=0.0588, Avg Time=0.73s, Samples=2\n",
      "speechmatics_en on en: WER=0.0588, Avg Time=2.99s, Samples=2\n",
      "speechmatics_en on en: WER=0.0588, Avg Time=2.99s, Samples=2\n",
      "speechmatics_en on en: WER=0.0588, Avg Time=2.99s, Samples=2\n",
      "speechmatics_en on en: WER=0.0588, Avg Time=2.99s, Samples=2\n",
      "speechmatics_en on en: WER=0.0588, Avg Time=2.99s, Samples=2\n",
      "speechmatics_en on en: WER=0.0588, Avg Time=2.99s, Samples=2\n",
      "gladia_pre-recorded on en: WER=0.0588, Avg Time=6.38s, Samples=2\n",
      "gladia_pre-recorded on en: WER=0.0588, Avg Time=6.38s, Samples=2\n",
      "gladia_pre-recorded on en: WER=0.0588, Avg Time=6.38s, Samples=2\n",
      "gladia_pre-recorded on en: WER=0.0588, Avg Time=6.38s, Samples=2\n",
      "gladia_pre-recorded on en: WER=0.0588, Avg Time=6.38s, Samples=2\n",
      "gladia_pre-recorded on en: WER=0.0588, Avg Time=6.38s, Samples=2\n",
      "Processing language: es\n",
      "Processing language: es\n",
      "Processing language: es\n",
      "Processing language: es\n",
      "Processing language: es\n",
      "Processing language: es\n",
      "Processing language: es\n",
      "Processing language: es\n",
      "Processing language: es\n",
      "Processing language: es\n",
      "Processing language: es\n",
      "Processing language: es\n",
      "Evaluating 8 services on es\n",
      "Evaluating 8 services on es\n",
      "Evaluating 8 services on es\n",
      "Evaluating 8 services on es\n",
      "Evaluating 8 services on es\n",
      "Evaluating 8 services on es\n",
      "Evaluating 8 services on es\n",
      "Evaluating 8 services on es\n",
      "Evaluating 8 services on es\n",
      "Evaluating 8 services on es\n",
      "Evaluating 8 services on es\n",
      "Evaluating 8 services on es\n",
      "openai_gpt-4o-transcribe on es: WER=0.0000, Avg Time=0.88s, Samples=2\n",
      "openai_gpt-4o-transcribe on es: WER=0.0000, Avg Time=0.88s, Samples=2\n",
      "openai_gpt-4o-transcribe on es: WER=0.0000, Avg Time=0.88s, Samples=2\n",
      "openai_gpt-4o-transcribe on es: WER=0.0000, Avg Time=0.88s, Samples=2\n",
      "openai_gpt-4o-transcribe on es: WER=0.0000, Avg Time=0.88s, Samples=2\n",
      "openai_gpt-4o-transcribe on es: WER=0.0000, Avg Time=0.88s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on es: WER=0.0000, Avg Time=0.81s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on es: WER=0.0000, Avg Time=0.81s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on es: WER=0.0000, Avg Time=0.81s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on es: WER=0.0000, Avg Time=0.81s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on es: WER=0.0000, Avg Time=0.81s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on es: WER=0.0000, Avg Time=0.81s, Samples=2\n",
      "openai_whisper-1 on es: WER=0.0000, Avg Time=1.48s, Samples=2\n",
      "openai_whisper-1 on es: WER=0.0000, Avg Time=1.48s, Samples=2\n",
      "openai_whisper-1 on es: WER=0.0000, Avg Time=1.48s, Samples=2\n",
      "openai_gpt-4o-transcribe on es: WER=0.0000, Avg Time=0.88s, Samples=2\n",
      "openai_gpt-4o-transcribe on es: WER=0.0000, Avg Time=0.88s, Samples=2\n",
      "openai_gpt-4o-transcribe on es: WER=0.0000, Avg Time=0.88s, Samples=2\n",
      "openai_gpt-4o-transcribe on es: WER=0.0000, Avg Time=0.88s, Samples=2\n",
      "openai_gpt-4o-transcribe on es: WER=0.0000, Avg Time=0.88s, Samples=2\n",
      "openai_gpt-4o-transcribe on es: WER=0.0000, Avg Time=0.88s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on es: WER=0.0000, Avg Time=0.81s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on es: WER=0.0000, Avg Time=0.81s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on es: WER=0.0000, Avg Time=0.81s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on es: WER=0.0000, Avg Time=0.81s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on es: WER=0.0000, Avg Time=0.81s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on es: WER=0.0000, Avg Time=0.81s, Samples=2\n",
      "openai_whisper-1 on es: WER=0.0000, Avg Time=1.48s, Samples=2\n",
      "openai_whisper-1 on es: WER=0.0000, Avg Time=1.48s, Samples=2\n",
      "openai_whisper-1 on es: WER=0.0000, Avg Time=1.48s, Samples=2\n",
      "openai_whisper-1 on es: WER=0.0000, Avg Time=1.48s, Samples=2\n",
      "openai_whisper-1 on es: WER=0.0000, Avg Time=1.48s, Samples=2\n",
      "openai_whisper-1 on es: WER=0.0000, Avg Time=1.48s, Samples=2\n",
      "groq_whisper-large-v3 on es: WER=0.0000, Avg Time=0.28s, Samples=2\n",
      "groq_whisper-large-v3 on es: WER=0.0000, Avg Time=0.28s, Samples=2\n",
      "groq_whisper-large-v3 on es: WER=0.0000, Avg Time=0.28s, Samples=2\n",
      "groq_whisper-large-v3 on es: WER=0.0000, Avg Time=0.28s, Samples=2\n",
      "groq_whisper-large-v3 on es: WER=0.0000, Avg Time=0.28s, Samples=2\n",
      "groq_whisper-large-v3 on es: WER=0.0000, Avg Time=0.28s, Samples=2\n",
      "groq_whisper-large-v3-turbo on es: WER=0.0000, Avg Time=0.28s, Samples=2\n",
      "groq_whisper-large-v3-turbo on es: WER=0.0000, Avg Time=0.28s, Samples=2\n",
      "groq_whisper-large-v3-turbo on es: WER=0.0000, Avg Time=0.28s, Samples=2\n",
      "groq_whisper-large-v3-turbo on es: WER=0.0000, Avg Time=0.28s, Samples=2\n",
      "groq_whisper-large-v3-turbo on es: WER=0.0000, Avg Time=0.28s, Samples=2\n",
      "groq_whisper-large-v3-turbo on es: WER=0.0000, Avg Time=0.28s, Samples=2\n",
      "elevenlabs_scribe_v1 on es: WER=0.0000, Avg Time=0.78s, Samples=2\n",
      "elevenlabs_scribe_v1 on es: WER=0.0000, Avg Time=0.78s, Samples=2\n",
      "elevenlabs_scribe_v1 on es: WER=0.0000, Avg Time=0.78s, Samples=2\n",
      "openai_whisper-1 on es: WER=0.0000, Avg Time=1.48s, Samples=2\n",
      "openai_whisper-1 on es: WER=0.0000, Avg Time=1.48s, Samples=2\n",
      "openai_whisper-1 on es: WER=0.0000, Avg Time=1.48s, Samples=2\n",
      "groq_whisper-large-v3 on es: WER=0.0000, Avg Time=0.28s, Samples=2\n",
      "groq_whisper-large-v3 on es: WER=0.0000, Avg Time=0.28s, Samples=2\n",
      "groq_whisper-large-v3 on es: WER=0.0000, Avg Time=0.28s, Samples=2\n",
      "groq_whisper-large-v3 on es: WER=0.0000, Avg Time=0.28s, Samples=2\n",
      "groq_whisper-large-v3 on es: WER=0.0000, Avg Time=0.28s, Samples=2\n",
      "groq_whisper-large-v3 on es: WER=0.0000, Avg Time=0.28s, Samples=2\n",
      "groq_whisper-large-v3-turbo on es: WER=0.0000, Avg Time=0.28s, Samples=2\n",
      "groq_whisper-large-v3-turbo on es: WER=0.0000, Avg Time=0.28s, Samples=2\n",
      "groq_whisper-large-v3-turbo on es: WER=0.0000, Avg Time=0.28s, Samples=2\n",
      "groq_whisper-large-v3-turbo on es: WER=0.0000, Avg Time=0.28s, Samples=2\n",
      "groq_whisper-large-v3-turbo on es: WER=0.0000, Avg Time=0.28s, Samples=2\n",
      "groq_whisper-large-v3-turbo on es: WER=0.0000, Avg Time=0.28s, Samples=2\n",
      "elevenlabs_scribe_v1 on es: WER=0.0000, Avg Time=0.78s, Samples=2\n",
      "elevenlabs_scribe_v1 on es: WER=0.0000, Avg Time=0.78s, Samples=2\n",
      "elevenlabs_scribe_v1 on es: WER=0.0000, Avg Time=0.78s, Samples=2\n",
      "elevenlabs_scribe_v1 on es: WER=0.0000, Avg Time=0.78s, Samples=2\n",
      "elevenlabs_scribe_v1 on es: WER=0.0000, Avg Time=0.78s, Samples=2\n",
      "elevenlabs_scribe_v1 on es: WER=0.0000, Avg Time=0.78s, Samples=2\n",
      "speechmatics_en on es: WER=0.6471, Avg Time=2.58s, Samples=2\n",
      "speechmatics_en on es: WER=0.6471, Avg Time=2.58s, Samples=2\n",
      "speechmatics_en on es: WER=0.6471, Avg Time=2.58s, Samples=2\n",
      "speechmatics_en on es: WER=0.6471, Avg Time=2.58s, Samples=2\n",
      "speechmatics_en on es: WER=0.6471, Avg Time=2.58s, Samples=2\n",
      "speechmatics_en on es: WER=0.6471, Avg Time=2.58s, Samples=2\n",
      "gladia_pre-recorded on es: WER=0.0000, Avg Time=6.50s, Samples=2\n",
      "gladia_pre-recorded on es: WER=0.0000, Avg Time=6.50s, Samples=2\n",
      "gladia_pre-recorded on es: WER=0.0000, Avg Time=6.50s, Samples=2\n",
      "gladia_pre-recorded on es: WER=0.0000, Avg Time=6.50s, Samples=2\n",
      "gladia_pre-recorded on es: WER=0.0000, Avg Time=6.50s, Samples=2\n",
      "gladia_pre-recorded on es: WER=0.0000, Avg Time=6.50s, Samples=2\n",
      "elevenlabs_scribe_v1 on es: WER=0.0000, Avg Time=0.78s, Samples=2\n",
      "elevenlabs_scribe_v1 on es: WER=0.0000, Avg Time=0.78s, Samples=2\n",
      "elevenlabs_scribe_v1 on es: WER=0.0000, Avg Time=0.78s, Samples=2\n",
      "speechmatics_en on es: WER=0.6471, Avg Time=2.58s, Samples=2\n",
      "speechmatics_en on es: WER=0.6471, Avg Time=2.58s, Samples=2\n",
      "speechmatics_en on es: WER=0.6471, Avg Time=2.58s, Samples=2\n",
      "speechmatics_en on es: WER=0.6471, Avg Time=2.58s, Samples=2\n",
      "speechmatics_en on es: WER=0.6471, Avg Time=2.58s, Samples=2\n",
      "speechmatics_en on es: WER=0.6471, Avg Time=2.58s, Samples=2\n",
      "gladia_pre-recorded on es: WER=0.0000, Avg Time=6.50s, Samples=2\n",
      "gladia_pre-recorded on es: WER=0.0000, Avg Time=6.50s, Samples=2\n",
      "gladia_pre-recorded on es: WER=0.0000, Avg Time=6.50s, Samples=2\n",
      "gladia_pre-recorded on es: WER=0.0000, Avg Time=6.50s, Samples=2\n",
      "gladia_pre-recorded on es: WER=0.0000, Avg Time=6.50s, Samples=2\n",
      "gladia_pre-recorded on es: WER=0.0000, Avg Time=6.50s, Samples=2\n",
      "Processing language: ja\n",
      "Processing language: ja\n",
      "Processing language: ja\n",
      "Processing language: ja\n",
      "Processing language: ja\n",
      "Processing language: ja\n",
      "Processing language: ja\n",
      "Processing language: ja\n",
      "Processing language: ja\n",
      "Processing language: ja\n",
      "Processing language: ja\n",
      "Processing language: ja\n",
      "Evaluating 8 services on ja\n",
      "Evaluating 8 services on ja\n",
      "Evaluating 8 services on ja\n",
      "Evaluating 8 services on ja\n",
      "Evaluating 8 services on ja\n",
      "Evaluating 8 services on ja\n",
      "Evaluating 8 services on ja\n",
      "Evaluating 8 services on ja\n",
      "Evaluating 8 services on ja\n",
      "Evaluating 8 services on ja\n",
      "Evaluating 8 services on ja\n",
      "Evaluating 8 services on ja\n",
      "openai_gpt-4o-transcribe on ja: WER=0.5000, Avg Time=1.08s, Samples=2\n",
      "openai_gpt-4o-transcribe on ja: WER=0.5000, Avg Time=1.08s, Samples=2\n",
      "openai_gpt-4o-transcribe on ja: WER=0.5000, Avg Time=1.08s, Samples=2\n",
      "openai_gpt-4o-transcribe on ja: WER=0.5000, Avg Time=1.08s, Samples=2\n",
      "openai_gpt-4o-transcribe on ja: WER=0.5000, Avg Time=1.08s, Samples=2\n",
      "openai_gpt-4o-transcribe on ja: WER=0.5000, Avg Time=1.08s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on ja: WER=0.5000, Avg Time=1.25s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on ja: WER=0.5000, Avg Time=1.25s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on ja: WER=0.5000, Avg Time=1.25s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on ja: WER=0.5000, Avg Time=1.25s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on ja: WER=0.5000, Avg Time=1.25s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on ja: WER=0.5000, Avg Time=1.25s, Samples=2\n",
      "openai_whisper-1 on ja: WER=0.5000, Avg Time=0.89s, Samples=2\n",
      "openai_whisper-1 on ja: WER=0.5000, Avg Time=0.89s, Samples=2\n",
      "openai_whisper-1 on ja: WER=0.5000, Avg Time=0.89s, Samples=2\n",
      "openai_whisper-1 on ja: WER=0.5000, Avg Time=0.89s, Samples=2\n",
      "openai_whisper-1 on ja: WER=0.5000, Avg Time=0.89s, Samples=2\n",
      "openai_gpt-4o-transcribe on ja: WER=0.5000, Avg Time=1.08s, Samples=2\n",
      "openai_gpt-4o-transcribe on ja: WER=0.5000, Avg Time=1.08s, Samples=2\n",
      "openai_gpt-4o-transcribe on ja: WER=0.5000, Avg Time=1.08s, Samples=2\n",
      "openai_gpt-4o-transcribe on ja: WER=0.5000, Avg Time=1.08s, Samples=2\n",
      "openai_gpt-4o-transcribe on ja: WER=0.5000, Avg Time=1.08s, Samples=2\n",
      "openai_gpt-4o-transcribe on ja: WER=0.5000, Avg Time=1.08s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on ja: WER=0.5000, Avg Time=1.25s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on ja: WER=0.5000, Avg Time=1.25s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on ja: WER=0.5000, Avg Time=1.25s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on ja: WER=0.5000, Avg Time=1.25s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on ja: WER=0.5000, Avg Time=1.25s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on ja: WER=0.5000, Avg Time=1.25s, Samples=2\n",
      "openai_whisper-1 on ja: WER=0.5000, Avg Time=0.89s, Samples=2\n",
      "openai_whisper-1 on ja: WER=0.5000, Avg Time=0.89s, Samples=2\n",
      "openai_whisper-1 on ja: WER=0.5000, Avg Time=0.89s, Samples=2\n",
      "openai_whisper-1 on ja: WER=0.5000, Avg Time=0.89s, Samples=2\n",
      "openai_whisper-1 on ja: WER=0.5000, Avg Time=0.89s, Samples=2\n",
      "openai_whisper-1 on ja: WER=0.5000, Avg Time=0.89s, Samples=2\n",
      "groq_whisper-large-v3 on ja: WER=0.5000, Avg Time=0.29s, Samples=2\n",
      "groq_whisper-large-v3 on ja: WER=0.5000, Avg Time=0.29s, Samples=2\n",
      "groq_whisper-large-v3 on ja: WER=0.5000, Avg Time=0.29s, Samples=2\n",
      "groq_whisper-large-v3 on ja: WER=0.5000, Avg Time=0.29s, Samples=2\n",
      "groq_whisper-large-v3 on ja: WER=0.5000, Avg Time=0.29s, Samples=2\n",
      "groq_whisper-large-v3 on ja: WER=0.5000, Avg Time=0.29s, Samples=2\n",
      "groq_whisper-large-v3-turbo on ja: WER=2.0000, Avg Time=0.26s, Samples=2\n",
      "groq_whisper-large-v3-turbo on ja: WER=2.0000, Avg Time=0.26s, Samples=2\n",
      "groq_whisper-large-v3-turbo on ja: WER=2.0000, Avg Time=0.26s, Samples=2\n",
      "groq_whisper-large-v3-turbo on ja: WER=2.0000, Avg Time=0.26s, Samples=2\n",
      "groq_whisper-large-v3-turbo on ja: WER=2.0000, Avg Time=0.26s, Samples=2\n",
      "groq_whisper-large-v3-turbo on ja: WER=2.0000, Avg Time=0.26s, Samples=2\n",
      "elevenlabs_scribe_v1 on ja: WER=5.0000, Avg Time=0.75s, Samples=2\n",
      "elevenlabs_scribe_v1 on ja: WER=5.0000, Avg Time=0.75s, Samples=2\n",
      "elevenlabs_scribe_v1 on ja: WER=5.0000, Avg Time=0.75s, Samples=2\n",
      "openai_whisper-1 on ja: WER=0.5000, Avg Time=0.89s, Samples=2\n",
      "groq_whisper-large-v3 on ja: WER=0.5000, Avg Time=0.29s, Samples=2\n",
      "groq_whisper-large-v3 on ja: WER=0.5000, Avg Time=0.29s, Samples=2\n",
      "groq_whisper-large-v3 on ja: WER=0.5000, Avg Time=0.29s, Samples=2\n",
      "groq_whisper-large-v3 on ja: WER=0.5000, Avg Time=0.29s, Samples=2\n",
      "groq_whisper-large-v3 on ja: WER=0.5000, Avg Time=0.29s, Samples=2\n",
      "groq_whisper-large-v3 on ja: WER=0.5000, Avg Time=0.29s, Samples=2\n",
      "groq_whisper-large-v3-turbo on ja: WER=2.0000, Avg Time=0.26s, Samples=2\n",
      "groq_whisper-large-v3-turbo on ja: WER=2.0000, Avg Time=0.26s, Samples=2\n",
      "groq_whisper-large-v3-turbo on ja: WER=2.0000, Avg Time=0.26s, Samples=2\n",
      "groq_whisper-large-v3-turbo on ja: WER=2.0000, Avg Time=0.26s, Samples=2\n",
      "groq_whisper-large-v3-turbo on ja: WER=2.0000, Avg Time=0.26s, Samples=2\n",
      "groq_whisper-large-v3-turbo on ja: WER=2.0000, Avg Time=0.26s, Samples=2\n",
      "elevenlabs_scribe_v1 on ja: WER=5.0000, Avg Time=0.75s, Samples=2\n",
      "elevenlabs_scribe_v1 on ja: WER=5.0000, Avg Time=0.75s, Samples=2\n",
      "elevenlabs_scribe_v1 on ja: WER=5.0000, Avg Time=0.75s, Samples=2\n",
      "elevenlabs_scribe_v1 on ja: WER=5.0000, Avg Time=0.75s, Samples=2\n",
      "elevenlabs_scribe_v1 on ja: WER=5.0000, Avg Time=0.75s, Samples=2\n",
      "elevenlabs_scribe_v1 on ja: WER=5.0000, Avg Time=0.75s, Samples=2\n",
      "speechmatics_en on ja: WER=2.5000, Avg Time=2.31s, Samples=2\n",
      "speechmatics_en on ja: WER=2.5000, Avg Time=2.31s, Samples=2\n",
      "speechmatics_en on ja: WER=2.5000, Avg Time=2.31s, Samples=2\n",
      "speechmatics_en on ja: WER=2.5000, Avg Time=2.31s, Samples=2\n",
      "speechmatics_en on ja: WER=2.5000, Avg Time=2.31s, Samples=2\n",
      "speechmatics_en on ja: WER=2.5000, Avg Time=2.31s, Samples=2\n",
      "gladia_pre-recorded on ja: WER=2.0000, Avg Time=6.33s, Samples=2\n",
      "gladia_pre-recorded on ja: WER=2.0000, Avg Time=6.33s, Samples=2\n",
      "gladia_pre-recorded on ja: WER=2.0000, Avg Time=6.33s, Samples=2\n",
      "gladia_pre-recorded on ja: WER=2.0000, Avg Time=6.33s, Samples=2\n",
      "gladia_pre-recorded on ja: WER=2.0000, Avg Time=6.33s, Samples=2\n",
      "gladia_pre-recorded on ja: WER=2.0000, Avg Time=6.33s, Samples=2\n",
      "elevenlabs_scribe_v1 on ja: WER=5.0000, Avg Time=0.75s, Samples=2\n",
      "elevenlabs_scribe_v1 on ja: WER=5.0000, Avg Time=0.75s, Samples=2\n",
      "elevenlabs_scribe_v1 on ja: WER=5.0000, Avg Time=0.75s, Samples=2\n",
      "speechmatics_en on ja: WER=2.5000, Avg Time=2.31s, Samples=2\n",
      "speechmatics_en on ja: WER=2.5000, Avg Time=2.31s, Samples=2\n",
      "speechmatics_en on ja: WER=2.5000, Avg Time=2.31s, Samples=2\n",
      "speechmatics_en on ja: WER=2.5000, Avg Time=2.31s, Samples=2\n",
      "speechmatics_en on ja: WER=2.5000, Avg Time=2.31s, Samples=2\n",
      "speechmatics_en on ja: WER=2.5000, Avg Time=2.31s, Samples=2\n",
      "gladia_pre-recorded on ja: WER=2.0000, Avg Time=6.33s, Samples=2\n",
      "gladia_pre-recorded on ja: WER=2.0000, Avg Time=6.33s, Samples=2\n",
      "gladia_pre-recorded on ja: WER=2.0000, Avg Time=6.33s, Samples=2\n",
      "gladia_pre-recorded on ja: WER=2.0000, Avg Time=6.33s, Samples=2\n",
      "gladia_pre-recorded on ja: WER=2.0000, Avg Time=6.33s, Samples=2\n",
      "gladia_pre-recorded on ja: WER=2.0000, Avg Time=6.33s, Samples=2\n",
      "Processing language: de\n",
      "Processing language: de\n",
      "Processing language: de\n",
      "Processing language: de\n",
      "Processing language: de\n",
      "Processing language: de\n",
      "Processing language: de\n",
      "Processing language: de\n",
      "Processing language: de\n",
      "Processing language: de\n",
      "Processing language: de\n",
      "Processing language: de\n",
      "Evaluating 8 services on de\n",
      "Evaluating 8 services on de\n",
      "Evaluating 8 services on de\n",
      "Evaluating 8 services on de\n",
      "Evaluating 8 services on de\n",
      "Evaluating 8 services on de\n",
      "Evaluating 8 services on de\n",
      "Evaluating 8 services on de\n",
      "Evaluating 8 services on de\n",
      "Evaluating 8 services on de\n",
      "Evaluating 8 services on de\n",
      "Evaluating 8 services on de\n",
      "openai_gpt-4o-transcribe on de: WER=0.0000, Avg Time=1.07s, Samples=2\n",
      "openai_gpt-4o-transcribe on de: WER=0.0000, Avg Time=1.07s, Samples=2\n",
      "openai_gpt-4o-transcribe on de: WER=0.0000, Avg Time=1.07s, Samples=2\n",
      "openai_gpt-4o-transcribe on de: WER=0.0000, Avg Time=1.07s, Samples=2\n",
      "openai_gpt-4o-transcribe on de: WER=0.0000, Avg Time=1.07s, Samples=2\n",
      "openai_gpt-4o-transcribe on de: WER=0.0000, Avg Time=1.07s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on de: WER=0.0000, Avg Time=1.21s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on de: WER=0.0000, Avg Time=1.21s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on de: WER=0.0000, Avg Time=1.21s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on de: WER=0.0000, Avg Time=1.21s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on de: WER=0.0000, Avg Time=1.21s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on de: WER=0.0000, Avg Time=1.21s, Samples=2\n",
      "openai_whisper-1 on de: WER=0.0800, Avg Time=1.10s, Samples=2\n",
      "openai_whisper-1 on de: WER=0.0800, Avg Time=1.10s, Samples=2\n",
      "openai_whisper-1 on de: WER=0.0800, Avg Time=1.10s, Samples=2\n",
      "openai_whisper-1 on de: WER=0.0800, Avg Time=1.10s, Samples=2\n",
      "openai_gpt-4o-transcribe on de: WER=0.0000, Avg Time=1.07s, Samples=2\n",
      "openai_gpt-4o-transcribe on de: WER=0.0000, Avg Time=1.07s, Samples=2\n",
      "openai_gpt-4o-transcribe on de: WER=0.0000, Avg Time=1.07s, Samples=2\n",
      "openai_gpt-4o-transcribe on de: WER=0.0000, Avg Time=1.07s, Samples=2\n",
      "openai_gpt-4o-transcribe on de: WER=0.0000, Avg Time=1.07s, Samples=2\n",
      "openai_gpt-4o-transcribe on de: WER=0.0000, Avg Time=1.07s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on de: WER=0.0000, Avg Time=1.21s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on de: WER=0.0000, Avg Time=1.21s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on de: WER=0.0000, Avg Time=1.21s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on de: WER=0.0000, Avg Time=1.21s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on de: WER=0.0000, Avg Time=1.21s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on de: WER=0.0000, Avg Time=1.21s, Samples=2\n",
      "openai_whisper-1 on de: WER=0.0800, Avg Time=1.10s, Samples=2\n",
      "openai_whisper-1 on de: WER=0.0800, Avg Time=1.10s, Samples=2\n",
      "openai_whisper-1 on de: WER=0.0800, Avg Time=1.10s, Samples=2\n",
      "openai_whisper-1 on de: WER=0.0800, Avg Time=1.10s, Samples=2\n",
      "openai_whisper-1 on de: WER=0.0800, Avg Time=1.10s, Samples=2\n",
      "openai_whisper-1 on de: WER=0.0800, Avg Time=1.10s, Samples=2\n",
      "groq_whisper-large-v3 on de: WER=0.0400, Avg Time=0.33s, Samples=2\n",
      "groq_whisper-large-v3 on de: WER=0.0400, Avg Time=0.33s, Samples=2\n",
      "groq_whisper-large-v3 on de: WER=0.0400, Avg Time=0.33s, Samples=2\n",
      "groq_whisper-large-v3 on de: WER=0.0400, Avg Time=0.33s, Samples=2\n",
      "groq_whisper-large-v3 on de: WER=0.0400, Avg Time=0.33s, Samples=2\n",
      "groq_whisper-large-v3 on de: WER=0.0400, Avg Time=0.33s, Samples=2\n",
      "groq_whisper-large-v3-turbo on de: WER=0.0800, Avg Time=0.29s, Samples=2\n",
      "groq_whisper-large-v3-turbo on de: WER=0.0800, Avg Time=0.29s, Samples=2\n",
      "groq_whisper-large-v3-turbo on de: WER=0.0800, Avg Time=0.29s, Samples=2\n",
      "groq_whisper-large-v3-turbo on de: WER=0.0800, Avg Time=0.29s, Samples=2\n",
      "groq_whisper-large-v3-turbo on de: WER=0.0800, Avg Time=0.29s, Samples=2\n",
      "groq_whisper-large-v3-turbo on de: WER=0.0800, Avg Time=0.29s, Samples=2\n",
      "elevenlabs_scribe_v1 on de: WER=0.0400, Avg Time=0.89s, Samples=2\n",
      "elevenlabs_scribe_v1 on de: WER=0.0400, Avg Time=0.89s, Samples=2\n",
      "openai_whisper-1 on de: WER=0.0800, Avg Time=1.10s, Samples=2\n",
      "openai_whisper-1 on de: WER=0.0800, Avg Time=1.10s, Samples=2\n",
      "groq_whisper-large-v3 on de: WER=0.0400, Avg Time=0.33s, Samples=2\n",
      "groq_whisper-large-v3 on de: WER=0.0400, Avg Time=0.33s, Samples=2\n",
      "groq_whisper-large-v3 on de: WER=0.0400, Avg Time=0.33s, Samples=2\n",
      "groq_whisper-large-v3 on de: WER=0.0400, Avg Time=0.33s, Samples=2\n",
      "groq_whisper-large-v3 on de: WER=0.0400, Avg Time=0.33s, Samples=2\n",
      "groq_whisper-large-v3 on de: WER=0.0400, Avg Time=0.33s, Samples=2\n",
      "groq_whisper-large-v3-turbo on de: WER=0.0800, Avg Time=0.29s, Samples=2\n",
      "groq_whisper-large-v3-turbo on de: WER=0.0800, Avg Time=0.29s, Samples=2\n",
      "groq_whisper-large-v3-turbo on de: WER=0.0800, Avg Time=0.29s, Samples=2\n",
      "groq_whisper-large-v3-turbo on de: WER=0.0800, Avg Time=0.29s, Samples=2\n",
      "groq_whisper-large-v3-turbo on de: WER=0.0800, Avg Time=0.29s, Samples=2\n",
      "groq_whisper-large-v3-turbo on de: WER=0.0800, Avg Time=0.29s, Samples=2\n",
      "elevenlabs_scribe_v1 on de: WER=0.0400, Avg Time=0.89s, Samples=2\n",
      "elevenlabs_scribe_v1 on de: WER=0.0400, Avg Time=0.89s, Samples=2\n",
      "elevenlabs_scribe_v1 on de: WER=0.0400, Avg Time=0.89s, Samples=2\n",
      "elevenlabs_scribe_v1 on de: WER=0.0400, Avg Time=0.89s, Samples=2\n",
      "elevenlabs_scribe_v1 on de: WER=0.0400, Avg Time=0.89s, Samples=2\n",
      "elevenlabs_scribe_v1 on de: WER=0.0400, Avg Time=0.89s, Samples=2\n",
      "speechmatics_en on de: WER=0.8000, Avg Time=2.62s, Samples=2\n",
      "speechmatics_en on de: WER=0.8000, Avg Time=2.62s, Samples=2\n",
      "speechmatics_en on de: WER=0.8000, Avg Time=2.62s, Samples=2\n",
      "speechmatics_en on de: WER=0.8000, Avg Time=2.62s, Samples=2\n",
      "speechmatics_en on de: WER=0.8000, Avg Time=2.62s, Samples=2\n",
      "speechmatics_en on de: WER=0.8000, Avg Time=2.62s, Samples=2\n",
      "gladia_pre-recorded on de: WER=0.0000, Avg Time=6.23s, Samples=2\n",
      "gladia_pre-recorded on de: WER=0.0000, Avg Time=6.23s, Samples=2\n",
      "gladia_pre-recorded on de: WER=0.0000, Avg Time=6.23s, Samples=2\n",
      "gladia_pre-recorded on de: WER=0.0000, Avg Time=6.23s, Samples=2\n",
      "gladia_pre-recorded on de: WER=0.0000, Avg Time=6.23s, Samples=2\n",
      "gladia_pre-recorded on de: WER=0.0000, Avg Time=6.23s, Samples=2\n",
      "elevenlabs_scribe_v1 on de: WER=0.0400, Avg Time=0.89s, Samples=2\n",
      "elevenlabs_scribe_v1 on de: WER=0.0400, Avg Time=0.89s, Samples=2\n",
      "elevenlabs_scribe_v1 on de: WER=0.0400, Avg Time=0.89s, Samples=2\n",
      "elevenlabs_scribe_v1 on de: WER=0.0400, Avg Time=0.89s, Samples=2\n",
      "speechmatics_en on de: WER=0.8000, Avg Time=2.62s, Samples=2\n",
      "speechmatics_en on de: WER=0.8000, Avg Time=2.62s, Samples=2\n",
      "speechmatics_en on de: WER=0.8000, Avg Time=2.62s, Samples=2\n",
      "speechmatics_en on de: WER=0.8000, Avg Time=2.62s, Samples=2\n",
      "speechmatics_en on de: WER=0.8000, Avg Time=2.62s, Samples=2\n",
      "speechmatics_en on de: WER=0.8000, Avg Time=2.62s, Samples=2\n",
      "gladia_pre-recorded on de: WER=0.0000, Avg Time=6.23s, Samples=2\n",
      "gladia_pre-recorded on de: WER=0.0000, Avg Time=6.23s, Samples=2\n",
      "gladia_pre-recorded on de: WER=0.0000, Avg Time=6.23s, Samples=2\n",
      "gladia_pre-recorded on de: WER=0.0000, Avg Time=6.23s, Samples=2\n",
      "gladia_pre-recorded on de: WER=0.0000, Avg Time=6.23s, Samples=2\n",
      "gladia_pre-recorded on de: WER=0.0000, Avg Time=6.23s, Samples=2\n",
      "Processing language: pl\n",
      "Processing language: pl\n",
      "Processing language: pl\n",
      "Processing language: pl\n",
      "Processing language: pl\n",
      "Processing language: pl\n",
      "Processing language: pl\n",
      "Processing language: pl\n",
      "Processing language: pl\n",
      "Processing language: pl\n",
      "Processing language: pl\n",
      "Processing language: pl\n",
      "Evaluating 8 services on pl\n",
      "Evaluating 8 services on pl\n",
      "Evaluating 8 services on pl\n",
      "Evaluating 8 services on pl\n",
      "Evaluating 8 services on pl\n",
      "Evaluating 8 services on pl\n",
      "Evaluating 8 services on pl\n",
      "Evaluating 8 services on pl\n",
      "Evaluating 8 services on pl\n",
      "Evaluating 8 services on pl\n",
      "Evaluating 8 services on pl\n",
      "Evaluating 8 services on pl\n",
      "openai_gpt-4o-transcribe on pl: WER=0.0000, Avg Time=1.02s, Samples=2\n",
      "openai_gpt-4o-transcribe on pl: WER=0.0000, Avg Time=1.02s, Samples=2\n",
      "openai_gpt-4o-transcribe on pl: WER=0.0000, Avg Time=1.02s, Samples=2\n",
      "openai_gpt-4o-transcribe on pl: WER=0.0000, Avg Time=1.02s, Samples=2\n",
      "openai_gpt-4o-transcribe on pl: WER=0.0000, Avg Time=1.02s, Samples=2\n",
      "openai_gpt-4o-transcribe on pl: WER=0.0000, Avg Time=1.02s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on pl: WER=0.0000, Avg Time=1.47s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on pl: WER=0.0000, Avg Time=1.47s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on pl: WER=0.0000, Avg Time=1.47s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on pl: WER=0.0000, Avg Time=1.47s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on pl: WER=0.0000, Avg Time=1.47s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on pl: WER=0.0000, Avg Time=1.47s, Samples=2\n",
      "openai_whisper-1 on pl: WER=0.0000, Avg Time=1.95s, Samples=2\n",
      "openai_gpt-4o-transcribe on pl: WER=0.0000, Avg Time=1.02s, Samples=2\n",
      "openai_gpt-4o-transcribe on pl: WER=0.0000, Avg Time=1.02s, Samples=2\n",
      "openai_gpt-4o-transcribe on pl: WER=0.0000, Avg Time=1.02s, Samples=2\n",
      "openai_gpt-4o-transcribe on pl: WER=0.0000, Avg Time=1.02s, Samples=2\n",
      "openai_gpt-4o-transcribe on pl: WER=0.0000, Avg Time=1.02s, Samples=2\n",
      "openai_gpt-4o-transcribe on pl: WER=0.0000, Avg Time=1.02s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on pl: WER=0.0000, Avg Time=1.47s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on pl: WER=0.0000, Avg Time=1.47s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on pl: WER=0.0000, Avg Time=1.47s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on pl: WER=0.0000, Avg Time=1.47s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on pl: WER=0.0000, Avg Time=1.47s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on pl: WER=0.0000, Avg Time=1.47s, Samples=2\n",
      "openai_whisper-1 on pl: WER=0.0000, Avg Time=1.95s, Samples=2\n",
      "openai_whisper-1 on pl: WER=0.0000, Avg Time=1.95s, Samples=2\n",
      "openai_whisper-1 on pl: WER=0.0000, Avg Time=1.95s, Samples=2\n",
      "openai_whisper-1 on pl: WER=0.0000, Avg Time=1.95s, Samples=2\n",
      "openai_whisper-1 on pl: WER=0.0000, Avg Time=1.95s, Samples=2\n",
      "openai_whisper-1 on pl: WER=0.0000, Avg Time=1.95s, Samples=2\n",
      "groq_whisper-large-v3 on pl: WER=0.0000, Avg Time=0.29s, Samples=2\n",
      "groq_whisper-large-v3 on pl: WER=0.0000, Avg Time=0.29s, Samples=2\n",
      "groq_whisper-large-v3 on pl: WER=0.0000, Avg Time=0.29s, Samples=2\n",
      "groq_whisper-large-v3 on pl: WER=0.0000, Avg Time=0.29s, Samples=2\n",
      "groq_whisper-large-v3 on pl: WER=0.0000, Avg Time=0.29s, Samples=2\n",
      "groq_whisper-large-v3 on pl: WER=0.0000, Avg Time=0.29s, Samples=2\n",
      "groq_whisper-large-v3-turbo on pl: WER=0.0000, Avg Time=0.30s, Samples=2\n",
      "groq_whisper-large-v3-turbo on pl: WER=0.0000, Avg Time=0.30s, Samples=2\n",
      "groq_whisper-large-v3-turbo on pl: WER=0.0000, Avg Time=0.30s, Samples=2\n",
      "groq_whisper-large-v3-turbo on pl: WER=0.0000, Avg Time=0.30s, Samples=2\n",
      "groq_whisper-large-v3-turbo on pl: WER=0.0000, Avg Time=0.30s, Samples=2\n",
      "groq_whisper-large-v3-turbo on pl: WER=0.0000, Avg Time=0.30s, Samples=2\n",
      "openai_whisper-1 on pl: WER=0.0000, Avg Time=1.95s, Samples=2\n",
      "openai_whisper-1 on pl: WER=0.0000, Avg Time=1.95s, Samples=2\n",
      "openai_whisper-1 on pl: WER=0.0000, Avg Time=1.95s, Samples=2\n",
      "openai_whisper-1 on pl: WER=0.0000, Avg Time=1.95s, Samples=2\n",
      "openai_whisper-1 on pl: WER=0.0000, Avg Time=1.95s, Samples=2\n",
      "groq_whisper-large-v3 on pl: WER=0.0000, Avg Time=0.29s, Samples=2\n",
      "groq_whisper-large-v3 on pl: WER=0.0000, Avg Time=0.29s, Samples=2\n",
      "groq_whisper-large-v3 on pl: WER=0.0000, Avg Time=0.29s, Samples=2\n",
      "groq_whisper-large-v3 on pl: WER=0.0000, Avg Time=0.29s, Samples=2\n",
      "groq_whisper-large-v3 on pl: WER=0.0000, Avg Time=0.29s, Samples=2\n",
      "groq_whisper-large-v3 on pl: WER=0.0000, Avg Time=0.29s, Samples=2\n",
      "groq_whisper-large-v3-turbo on pl: WER=0.0000, Avg Time=0.30s, Samples=2\n",
      "groq_whisper-large-v3-turbo on pl: WER=0.0000, Avg Time=0.30s, Samples=2\n",
      "groq_whisper-large-v3-turbo on pl: WER=0.0000, Avg Time=0.30s, Samples=2\n",
      "groq_whisper-large-v3-turbo on pl: WER=0.0000, Avg Time=0.30s, Samples=2\n",
      "groq_whisper-large-v3-turbo on pl: WER=0.0000, Avg Time=0.30s, Samples=2\n",
      "groq_whisper-large-v3-turbo on pl: WER=0.0000, Avg Time=0.30s, Samples=2\n",
      "elevenlabs_scribe_v1 on pl: WER=0.0000, Avg Time=0.88s, Samples=2\n",
      "elevenlabs_scribe_v1 on pl: WER=0.0000, Avg Time=0.88s, Samples=2\n",
      "elevenlabs_scribe_v1 on pl: WER=0.0000, Avg Time=0.88s, Samples=2\n",
      "elevenlabs_scribe_v1 on pl: WER=0.0000, Avg Time=0.88s, Samples=2\n",
      "elevenlabs_scribe_v1 on pl: WER=0.0000, Avg Time=0.88s, Samples=2\n",
      "elevenlabs_scribe_v1 on pl: WER=0.0000, Avg Time=0.88s, Samples=2\n",
      "speechmatics_en on pl: WER=1.0000, Avg Time=3.03s, Samples=2\n",
      "speechmatics_en on pl: WER=1.0000, Avg Time=3.03s, Samples=2\n",
      "speechmatics_en on pl: WER=1.0000, Avg Time=3.03s, Samples=2\n",
      "speechmatics_en on pl: WER=1.0000, Avg Time=3.03s, Samples=2\n",
      "speechmatics_en on pl: WER=1.0000, Avg Time=3.03s, Samples=2\n",
      "speechmatics_en on pl: WER=1.0000, Avg Time=3.03s, Samples=2\n",
      "gladia_pre-recorded on pl: WER=0.0000, Avg Time=6.28s, Samples=2\n",
      "gladia_pre-recorded on pl: WER=0.0000, Avg Time=6.28s, Samples=2\n",
      "gladia_pre-recorded on pl: WER=0.0000, Avg Time=6.28s, Samples=2\n",
      "gladia_pre-recorded on pl: WER=0.0000, Avg Time=6.28s, Samples=2\n",
      "gladia_pre-recorded on pl: WER=0.0000, Avg Time=6.28s, Samples=2\n",
      "gladia_pre-recorded on pl: WER=0.0000, Avg Time=6.28s, Samples=2\n",
      "elevenlabs_scribe_v1 on pl: WER=0.0000, Avg Time=0.88s, Samples=2\n",
      "elevenlabs_scribe_v1 on pl: WER=0.0000, Avg Time=0.88s, Samples=2\n",
      "elevenlabs_scribe_v1 on pl: WER=0.0000, Avg Time=0.88s, Samples=2\n",
      "elevenlabs_scribe_v1 on pl: WER=0.0000, Avg Time=0.88s, Samples=2\n",
      "elevenlabs_scribe_v1 on pl: WER=0.0000, Avg Time=0.88s, Samples=2\n",
      "elevenlabs_scribe_v1 on pl: WER=0.0000, Avg Time=0.88s, Samples=2\n",
      "speechmatics_en on pl: WER=1.0000, Avg Time=3.03s, Samples=2\n",
      "speechmatics_en on pl: WER=1.0000, Avg Time=3.03s, Samples=2\n",
      "speechmatics_en on pl: WER=1.0000, Avg Time=3.03s, Samples=2\n",
      "speechmatics_en on pl: WER=1.0000, Avg Time=3.03s, Samples=2\n",
      "speechmatics_en on pl: WER=1.0000, Avg Time=3.03s, Samples=2\n",
      "speechmatics_en on pl: WER=1.0000, Avg Time=3.03s, Samples=2\n",
      "gladia_pre-recorded on pl: WER=0.0000, Avg Time=6.28s, Samples=2\n",
      "gladia_pre-recorded on pl: WER=0.0000, Avg Time=6.28s, Samples=2\n",
      "gladia_pre-recorded on pl: WER=0.0000, Avg Time=6.28s, Samples=2\n",
      "gladia_pre-recorded on pl: WER=0.0000, Avg Time=6.28s, Samples=2\n",
      "gladia_pre-recorded on pl: WER=0.0000, Avg Time=6.28s, Samples=2\n",
      "gladia_pre-recorded on pl: WER=0.0000, Avg Time=6.28s, Samples=2\n",
      "Processing language: pt\n",
      "Processing language: pt\n",
      "Processing language: pt\n",
      "Processing language: pt\n",
      "Processing language: pt\n",
      "Processing language: pt\n",
      "Processing language: pt\n",
      "Processing language: pt\n",
      "Processing language: pt\n",
      "Processing language: pt\n",
      "Processing language: pt\n",
      "Processing language: pt\n",
      "Evaluating 8 services on pt\n",
      "Evaluating 8 services on pt\n",
      "Evaluating 8 services on pt\n",
      "Evaluating 8 services on pt\n",
      "Evaluating 8 services on pt\n",
      "Evaluating 8 services on pt\n",
      "Evaluating 8 services on pt\n",
      "Evaluating 8 services on pt\n",
      "Evaluating 8 services on pt\n",
      "Evaluating 8 services on pt\n",
      "Evaluating 8 services on pt\n",
      "Evaluating 8 services on pt\n",
      "openai_gpt-4o-transcribe on pt: WER=0.1667, Avg Time=1.34s, Samples=2\n",
      "openai_gpt-4o-transcribe on pt: WER=0.1667, Avg Time=1.34s, Samples=2\n",
      "openai_gpt-4o-transcribe on pt: WER=0.1667, Avg Time=1.34s, Samples=2\n",
      "openai_gpt-4o-transcribe on pt: WER=0.1667, Avg Time=1.34s, Samples=2\n",
      "openai_gpt-4o-transcribe on pt: WER=0.1667, Avg Time=1.34s, Samples=2\n",
      "openai_gpt-4o-transcribe on pt: WER=0.1667, Avg Time=1.34s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on pt: WER=0.1667, Avg Time=1.04s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on pt: WER=0.1667, Avg Time=1.04s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on pt: WER=0.1667, Avg Time=1.04s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on pt: WER=0.1667, Avg Time=1.04s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on pt: WER=0.1667, Avg Time=1.04s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on pt: WER=0.1667, Avg Time=1.04s, Samples=2\n",
      "openai_whisper-1 on pt: WER=0.1667, Avg Time=0.97s, Samples=2\n",
      "openai_whisper-1 on pt: WER=0.1667, Avg Time=0.97s, Samples=2\n",
      "openai_whisper-1 on pt: WER=0.1667, Avg Time=0.97s, Samples=2\n",
      "openai_gpt-4o-transcribe on pt: WER=0.1667, Avg Time=1.34s, Samples=2\n",
      "openai_gpt-4o-transcribe on pt: WER=0.1667, Avg Time=1.34s, Samples=2\n",
      "openai_gpt-4o-transcribe on pt: WER=0.1667, Avg Time=1.34s, Samples=2\n",
      "openai_gpt-4o-transcribe on pt: WER=0.1667, Avg Time=1.34s, Samples=2\n",
      "openai_gpt-4o-transcribe on pt: WER=0.1667, Avg Time=1.34s, Samples=2\n",
      "openai_gpt-4o-transcribe on pt: WER=0.1667, Avg Time=1.34s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on pt: WER=0.1667, Avg Time=1.04s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on pt: WER=0.1667, Avg Time=1.04s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on pt: WER=0.1667, Avg Time=1.04s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on pt: WER=0.1667, Avg Time=1.04s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on pt: WER=0.1667, Avg Time=1.04s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on pt: WER=0.1667, Avg Time=1.04s, Samples=2\n",
      "openai_whisper-1 on pt: WER=0.1667, Avg Time=0.97s, Samples=2\n",
      "openai_whisper-1 on pt: WER=0.1667, Avg Time=0.97s, Samples=2\n",
      "openai_whisper-1 on pt: WER=0.1667, Avg Time=0.97s, Samples=2\n",
      "openai_whisper-1 on pt: WER=0.1667, Avg Time=0.97s, Samples=2\n",
      "openai_whisper-1 on pt: WER=0.1667, Avg Time=0.97s, Samples=2\n",
      "openai_whisper-1 on pt: WER=0.1667, Avg Time=0.97s, Samples=2\n",
      "groq_whisper-large-v3 on pt: WER=0.1667, Avg Time=0.29s, Samples=2\n",
      "groq_whisper-large-v3 on pt: WER=0.1667, Avg Time=0.29s, Samples=2\n",
      "groq_whisper-large-v3 on pt: WER=0.1667, Avg Time=0.29s, Samples=2\n",
      "groq_whisper-large-v3 on pt: WER=0.1667, Avg Time=0.29s, Samples=2\n",
      "groq_whisper-large-v3 on pt: WER=0.1667, Avg Time=0.29s, Samples=2\n",
      "groq_whisper-large-v3 on pt: WER=0.1667, Avg Time=0.29s, Samples=2\n",
      "groq_whisper-large-v3-turbo on pt: WER=0.1667, Avg Time=0.25s, Samples=2\n",
      "groq_whisper-large-v3-turbo on pt: WER=0.1667, Avg Time=0.25s, Samples=2\n",
      "groq_whisper-large-v3-turbo on pt: WER=0.1667, Avg Time=0.25s, Samples=2\n",
      "groq_whisper-large-v3-turbo on pt: WER=0.1667, Avg Time=0.25s, Samples=2\n",
      "groq_whisper-large-v3-turbo on pt: WER=0.1667, Avg Time=0.25s, Samples=2\n",
      "groq_whisper-large-v3-turbo on pt: WER=0.1667, Avg Time=0.25s, Samples=2\n",
      "elevenlabs_scribe_v1 on pt: WER=0.0833, Avg Time=0.75s, Samples=2\n",
      "openai_whisper-1 on pt: WER=0.1667, Avg Time=0.97s, Samples=2\n",
      "openai_whisper-1 on pt: WER=0.1667, Avg Time=0.97s, Samples=2\n",
      "openai_whisper-1 on pt: WER=0.1667, Avg Time=0.97s, Samples=2\n",
      "groq_whisper-large-v3 on pt: WER=0.1667, Avg Time=0.29s, Samples=2\n",
      "groq_whisper-large-v3 on pt: WER=0.1667, Avg Time=0.29s, Samples=2\n",
      "groq_whisper-large-v3 on pt: WER=0.1667, Avg Time=0.29s, Samples=2\n",
      "groq_whisper-large-v3 on pt: WER=0.1667, Avg Time=0.29s, Samples=2\n",
      "groq_whisper-large-v3 on pt: WER=0.1667, Avg Time=0.29s, Samples=2\n",
      "groq_whisper-large-v3 on pt: WER=0.1667, Avg Time=0.29s, Samples=2\n",
      "groq_whisper-large-v3-turbo on pt: WER=0.1667, Avg Time=0.25s, Samples=2\n",
      "groq_whisper-large-v3-turbo on pt: WER=0.1667, Avg Time=0.25s, Samples=2\n",
      "groq_whisper-large-v3-turbo on pt: WER=0.1667, Avg Time=0.25s, Samples=2\n",
      "groq_whisper-large-v3-turbo on pt: WER=0.1667, Avg Time=0.25s, Samples=2\n",
      "groq_whisper-large-v3-turbo on pt: WER=0.1667, Avg Time=0.25s, Samples=2\n",
      "groq_whisper-large-v3-turbo on pt: WER=0.1667, Avg Time=0.25s, Samples=2\n",
      "elevenlabs_scribe_v1 on pt: WER=0.0833, Avg Time=0.75s, Samples=2\n",
      "elevenlabs_scribe_v1 on pt: WER=0.0833, Avg Time=0.75s, Samples=2\n",
      "elevenlabs_scribe_v1 on pt: WER=0.0833, Avg Time=0.75s, Samples=2\n",
      "elevenlabs_scribe_v1 on pt: WER=0.0833, Avg Time=0.75s, Samples=2\n",
      "elevenlabs_scribe_v1 on pt: WER=0.0833, Avg Time=0.75s, Samples=2\n",
      "elevenlabs_scribe_v1 on pt: WER=0.0833, Avg Time=0.75s, Samples=2\n",
      "speechmatics_en on pt: WER=1.0000, Avg Time=2.35s, Samples=2\n",
      "speechmatics_en on pt: WER=1.0000, Avg Time=2.35s, Samples=2\n",
      "speechmatics_en on pt: WER=1.0000, Avg Time=2.35s, Samples=2\n",
      "speechmatics_en on pt: WER=1.0000, Avg Time=2.35s, Samples=2\n",
      "speechmatics_en on pt: WER=1.0000, Avg Time=2.35s, Samples=2\n",
      "speechmatics_en on pt: WER=1.0000, Avg Time=2.35s, Samples=2\n",
      "gladia_pre-recorded on pt: WER=0.1667, Avg Time=6.11s, Samples=2\n",
      "gladia_pre-recorded on pt: WER=0.1667, Avg Time=6.11s, Samples=2\n",
      "gladia_pre-recorded on pt: WER=0.1667, Avg Time=6.11s, Samples=2\n",
      "gladia_pre-recorded on pt: WER=0.1667, Avg Time=6.11s, Samples=2\n",
      "gladia_pre-recorded on pt: WER=0.1667, Avg Time=6.11s, Samples=2\n",
      "gladia_pre-recorded on pt: WER=0.1667, Avg Time=6.11s, Samples=2\n",
      "elevenlabs_scribe_v1 on pt: WER=0.0833, Avg Time=0.75s, Samples=2\n",
      "elevenlabs_scribe_v1 on pt: WER=0.0833, Avg Time=0.75s, Samples=2\n",
      "elevenlabs_scribe_v1 on pt: WER=0.0833, Avg Time=0.75s, Samples=2\n",
      "elevenlabs_scribe_v1 on pt: WER=0.0833, Avg Time=0.75s, Samples=2\n",
      "elevenlabs_scribe_v1 on pt: WER=0.0833, Avg Time=0.75s, Samples=2\n",
      "speechmatics_en on pt: WER=1.0000, Avg Time=2.35s, Samples=2\n",
      "speechmatics_en on pt: WER=1.0000, Avg Time=2.35s, Samples=2\n",
      "speechmatics_en on pt: WER=1.0000, Avg Time=2.35s, Samples=2\n",
      "speechmatics_en on pt: WER=1.0000, Avg Time=2.35s, Samples=2\n",
      "speechmatics_en on pt: WER=1.0000, Avg Time=2.35s, Samples=2\n",
      "speechmatics_en on pt: WER=1.0000, Avg Time=2.35s, Samples=2\n",
      "gladia_pre-recorded on pt: WER=0.1667, Avg Time=6.11s, Samples=2\n",
      "gladia_pre-recorded on pt: WER=0.1667, Avg Time=6.11s, Samples=2\n",
      "gladia_pre-recorded on pt: WER=0.1667, Avg Time=6.11s, Samples=2\n",
      "gladia_pre-recorded on pt: WER=0.1667, Avg Time=6.11s, Samples=2\n",
      "gladia_pre-recorded on pt: WER=0.1667, Avg Time=6.11s, Samples=2\n",
      "gladia_pre-recorded on pt: WER=0.1667, Avg Time=6.11s, Samples=2\n",
      "Processing language: fr\n",
      "Processing language: fr\n",
      "Processing language: fr\n",
      "Processing language: fr\n",
      "Processing language: fr\n",
      "Processing language: fr\n",
      "Processing language: fr\n",
      "Processing language: fr\n",
      "Processing language: fr\n",
      "Processing language: fr\n",
      "Processing language: fr\n",
      "Processing language: fr\n",
      "Evaluating 8 services on fr\n",
      "Evaluating 8 services on fr\n",
      "Evaluating 8 services on fr\n",
      "Evaluating 8 services on fr\n",
      "Evaluating 8 services on fr\n",
      "Evaluating 8 services on fr\n",
      "Evaluating 8 services on fr\n",
      "Evaluating 8 services on fr\n",
      "Evaluating 8 services on fr\n",
      "Evaluating 8 services on fr\n",
      "Evaluating 8 services on fr\n",
      "Evaluating 8 services on fr\n",
      "openai_gpt-4o-transcribe on fr: WER=0.0000, Avg Time=1.83s, Samples=2\n",
      "openai_gpt-4o-transcribe on fr: WER=0.0000, Avg Time=1.83s, Samples=2\n",
      "openai_gpt-4o-transcribe on fr: WER=0.0000, Avg Time=1.83s, Samples=2\n",
      "openai_gpt-4o-transcribe on fr: WER=0.0000, Avg Time=1.83s, Samples=2\n",
      "openai_gpt-4o-transcribe on fr: WER=0.0000, Avg Time=1.83s, Samples=2\n",
      "openai_gpt-4o-transcribe on fr: WER=0.0000, Avg Time=1.83s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on fr: WER=0.1176, Avg Time=1.11s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on fr: WER=0.1176, Avg Time=1.11s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on fr: WER=0.1176, Avg Time=1.11s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on fr: WER=0.1176, Avg Time=1.11s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on fr: WER=0.1176, Avg Time=1.11s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on fr: WER=0.1176, Avg Time=1.11s, Samples=2\n",
      "openai_whisper-1 on fr: WER=0.0588, Avg Time=1.31s, Samples=2\n",
      "openai_whisper-1 on fr: WER=0.0588, Avg Time=1.31s, Samples=2\n",
      "openai_whisper-1 on fr: WER=0.0588, Avg Time=1.31s, Samples=2\n",
      "openai_whisper-1 on fr: WER=0.0588, Avg Time=1.31s, Samples=2\n",
      "openai_gpt-4o-transcribe on fr: WER=0.0000, Avg Time=1.83s, Samples=2\n",
      "openai_gpt-4o-transcribe on fr: WER=0.0000, Avg Time=1.83s, Samples=2\n",
      "openai_gpt-4o-transcribe on fr: WER=0.0000, Avg Time=1.83s, Samples=2\n",
      "openai_gpt-4o-transcribe on fr: WER=0.0000, Avg Time=1.83s, Samples=2\n",
      "openai_gpt-4o-transcribe on fr: WER=0.0000, Avg Time=1.83s, Samples=2\n",
      "openai_gpt-4o-transcribe on fr: WER=0.0000, Avg Time=1.83s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on fr: WER=0.1176, Avg Time=1.11s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on fr: WER=0.1176, Avg Time=1.11s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on fr: WER=0.1176, Avg Time=1.11s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on fr: WER=0.1176, Avg Time=1.11s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on fr: WER=0.1176, Avg Time=1.11s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on fr: WER=0.1176, Avg Time=1.11s, Samples=2\n",
      "openai_whisper-1 on fr: WER=0.0588, Avg Time=1.31s, Samples=2\n",
      "openai_whisper-1 on fr: WER=0.0588, Avg Time=1.31s, Samples=2\n",
      "openai_whisper-1 on fr: WER=0.0588, Avg Time=1.31s, Samples=2\n",
      "openai_whisper-1 on fr: WER=0.0588, Avg Time=1.31s, Samples=2\n",
      "openai_whisper-1 on fr: WER=0.0588, Avg Time=1.31s, Samples=2\n",
      "openai_whisper-1 on fr: WER=0.0588, Avg Time=1.31s, Samples=2\n",
      "groq_whisper-large-v3 on fr: WER=0.1176, Avg Time=0.38s, Samples=2\n",
      "groq_whisper-large-v3 on fr: WER=0.1176, Avg Time=0.38s, Samples=2\n",
      "groq_whisper-large-v3 on fr: WER=0.1176, Avg Time=0.38s, Samples=2\n",
      "groq_whisper-large-v3 on fr: WER=0.1176, Avg Time=0.38s, Samples=2\n",
      "groq_whisper-large-v3 on fr: WER=0.1176, Avg Time=0.38s, Samples=2\n",
      "groq_whisper-large-v3 on fr: WER=0.1176, Avg Time=0.38s, Samples=2\n",
      "groq_whisper-large-v3-turbo on fr: WER=0.1176, Avg Time=0.32s, Samples=2\n",
      "groq_whisper-large-v3-turbo on fr: WER=0.1176, Avg Time=0.32s, Samples=2\n",
      "groq_whisper-large-v3-turbo on fr: WER=0.1176, Avg Time=0.32s, Samples=2\n",
      "groq_whisper-large-v3-turbo on fr: WER=0.1176, Avg Time=0.32s, Samples=2\n",
      "groq_whisper-large-v3-turbo on fr: WER=0.1176, Avg Time=0.32s, Samples=2\n",
      "groq_whisper-large-v3-turbo on fr: WER=0.1176, Avg Time=0.32s, Samples=2\n",
      "elevenlabs_scribe_v1 on fr: WER=0.3529, Avg Time=0.81s, Samples=2\n",
      "elevenlabs_scribe_v1 on fr: WER=0.3529, Avg Time=0.81s, Samples=2\n",
      "elevenlabs_scribe_v1 on fr: WER=0.3529, Avg Time=0.81s, Samples=2\n",
      "openai_whisper-1 on fr: WER=0.0588, Avg Time=1.31s, Samples=2\n",
      "openai_whisper-1 on fr: WER=0.0588, Avg Time=1.31s, Samples=2\n",
      "groq_whisper-large-v3 on fr: WER=0.1176, Avg Time=0.38s, Samples=2\n",
      "groq_whisper-large-v3 on fr: WER=0.1176, Avg Time=0.38s, Samples=2\n",
      "groq_whisper-large-v3 on fr: WER=0.1176, Avg Time=0.38s, Samples=2\n",
      "groq_whisper-large-v3 on fr: WER=0.1176, Avg Time=0.38s, Samples=2\n",
      "groq_whisper-large-v3 on fr: WER=0.1176, Avg Time=0.38s, Samples=2\n",
      "groq_whisper-large-v3 on fr: WER=0.1176, Avg Time=0.38s, Samples=2\n",
      "groq_whisper-large-v3-turbo on fr: WER=0.1176, Avg Time=0.32s, Samples=2\n",
      "groq_whisper-large-v3-turbo on fr: WER=0.1176, Avg Time=0.32s, Samples=2\n",
      "groq_whisper-large-v3-turbo on fr: WER=0.1176, Avg Time=0.32s, Samples=2\n",
      "groq_whisper-large-v3-turbo on fr: WER=0.1176, Avg Time=0.32s, Samples=2\n",
      "groq_whisper-large-v3-turbo on fr: WER=0.1176, Avg Time=0.32s, Samples=2\n",
      "groq_whisper-large-v3-turbo on fr: WER=0.1176, Avg Time=0.32s, Samples=2\n",
      "elevenlabs_scribe_v1 on fr: WER=0.3529, Avg Time=0.81s, Samples=2\n",
      "elevenlabs_scribe_v1 on fr: WER=0.3529, Avg Time=0.81s, Samples=2\n",
      "elevenlabs_scribe_v1 on fr: WER=0.3529, Avg Time=0.81s, Samples=2\n",
      "elevenlabs_scribe_v1 on fr: WER=0.3529, Avg Time=0.81s, Samples=2\n",
      "elevenlabs_scribe_v1 on fr: WER=0.3529, Avg Time=0.81s, Samples=2\n",
      "elevenlabs_scribe_v1 on fr: WER=0.3529, Avg Time=0.81s, Samples=2\n",
      "speechmatics_en on fr: WER=0.8235, Avg Time=3.12s, Samples=2\n",
      "speechmatics_en on fr: WER=0.8235, Avg Time=3.12s, Samples=2\n",
      "speechmatics_en on fr: WER=0.8235, Avg Time=3.12s, Samples=2\n",
      "speechmatics_en on fr: WER=0.8235, Avg Time=3.12s, Samples=2\n",
      "speechmatics_en on fr: WER=0.8235, Avg Time=3.12s, Samples=2\n",
      "speechmatics_en on fr: WER=0.8235, Avg Time=3.12s, Samples=2\n",
      "gladia_pre-recorded on fr: WER=0.1176, Avg Time=7.84s, Samples=2\n",
      "gladia_pre-recorded on fr: WER=0.1176, Avg Time=7.84s, Samples=2\n",
      "gladia_pre-recorded on fr: WER=0.1176, Avg Time=7.84s, Samples=2\n",
      "gladia_pre-recorded on fr: WER=0.1176, Avg Time=7.84s, Samples=2\n",
      "gladia_pre-recorded on fr: WER=0.1176, Avg Time=7.84s, Samples=2\n",
      "gladia_pre-recorded on fr: WER=0.1176, Avg Time=7.84s, Samples=2\n",
      "elevenlabs_scribe_v1 on fr: WER=0.3529, Avg Time=0.81s, Samples=2\n",
      "elevenlabs_scribe_v1 on fr: WER=0.3529, Avg Time=0.81s, Samples=2\n",
      "elevenlabs_scribe_v1 on fr: WER=0.3529, Avg Time=0.81s, Samples=2\n",
      "speechmatics_en on fr: WER=0.8235, Avg Time=3.12s, Samples=2\n",
      "speechmatics_en on fr: WER=0.8235, Avg Time=3.12s, Samples=2\n",
      "speechmatics_en on fr: WER=0.8235, Avg Time=3.12s, Samples=2\n",
      "speechmatics_en on fr: WER=0.8235, Avg Time=3.12s, Samples=2\n",
      "speechmatics_en on fr: WER=0.8235, Avg Time=3.12s, Samples=2\n",
      "speechmatics_en on fr: WER=0.8235, Avg Time=3.12s, Samples=2\n",
      "gladia_pre-recorded on fr: WER=0.1176, Avg Time=7.84s, Samples=2\n",
      "gladia_pre-recorded on fr: WER=0.1176, Avg Time=7.84s, Samples=2\n",
      "gladia_pre-recorded on fr: WER=0.1176, Avg Time=7.84s, Samples=2\n",
      "gladia_pre-recorded on fr: WER=0.1176, Avg Time=7.84s, Samples=2\n",
      "gladia_pre-recorded on fr: WER=0.1176, Avg Time=7.84s, Samples=2\n",
      "gladia_pre-recorded on fr: WER=0.1176, Avg Time=7.84s, Samples=2\n",
      "Processing language: zh-CN\n",
      "Processing language: zh-CN\n",
      "Processing language: zh-CN\n",
      "Processing language: zh-CN\n",
      "Processing language: zh-CN\n",
      "Processing language: zh-CN\n",
      "Processing language: zh-CN\n",
      "Processing language: zh-CN\n",
      "Processing language: zh-CN\n",
      "Processing language: zh-CN\n",
      "Processing language: zh-CN\n",
      "Processing language: zh-CN\n",
      "Evaluating 8 services on zh-CN\n",
      "Evaluating 8 services on zh-CN\n",
      "Evaluating 8 services on zh-CN\n",
      "Evaluating 8 services on zh-CN\n",
      "Evaluating 8 services on zh-CN\n",
      "Evaluating 8 services on zh-CN\n",
      "Evaluating 8 services on zh-CN\n",
      "Evaluating 8 services on zh-CN\n",
      "Evaluating 8 services on zh-CN\n",
      "Evaluating 8 services on zh-CN\n",
      "Evaluating 8 services on zh-CN\n",
      "Evaluating 8 services on zh-CN\n",
      "openai_gpt-4o-transcribe on zh-CN: WER=1.0000, Avg Time=1.27s, Samples=2\n",
      "openai_gpt-4o-transcribe on zh-CN: WER=1.0000, Avg Time=1.27s, Samples=2\n",
      "openai_gpt-4o-transcribe on zh-CN: WER=1.0000, Avg Time=1.27s, Samples=2\n",
      "openai_gpt-4o-transcribe on zh-CN: WER=1.0000, Avg Time=1.27s, Samples=2\n",
      "openai_gpt-4o-transcribe on zh-CN: WER=1.0000, Avg Time=1.27s, Samples=2\n",
      "openai_gpt-4o-transcribe on zh-CN: WER=1.0000, Avg Time=1.27s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on zh-CN: WER=0.5000, Avg Time=0.93s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on zh-CN: WER=0.5000, Avg Time=0.93s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on zh-CN: WER=0.5000, Avg Time=0.93s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on zh-CN: WER=0.5000, Avg Time=0.93s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on zh-CN: WER=0.5000, Avg Time=0.93s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on zh-CN: WER=0.5000, Avg Time=0.93s, Samples=2\n",
      "openai_whisper-1 on zh-CN: WER=1.0000, Avg Time=1.46s, Samples=2\n",
      "openai_whisper-1 on zh-CN: WER=1.0000, Avg Time=1.46s, Samples=2\n",
      "openai_whisper-1 on zh-CN: WER=1.0000, Avg Time=1.46s, Samples=2\n",
      "openai_whisper-1 on zh-CN: WER=1.0000, Avg Time=1.46s, Samples=2\n",
      "openai_whisper-1 on zh-CN: WER=1.0000, Avg Time=1.46s, Samples=2\n",
      "openai_whisper-1 on zh-CN: WER=1.0000, Avg Time=1.46s, Samples=2\n",
      "groq_whisper-large-v3 on zh-CN: WER=1.0000, Avg Time=0.31s, Samples=2\n",
      "groq_whisper-large-v3 on zh-CN: WER=1.0000, Avg Time=0.31s, Samples=2\n",
      "groq_whisper-large-v3 on zh-CN: WER=1.0000, Avg Time=0.31s, Samples=2\n",
      "groq_whisper-large-v3 on zh-CN: WER=1.0000, Avg Time=0.31s, Samples=2\n",
      "groq_whisper-large-v3 on zh-CN: WER=1.0000, Avg Time=0.31s, Samples=2\n",
      "groq_whisper-large-v3 on zh-CN: WER=1.0000, Avg Time=0.31s, Samples=2\n",
      "groq_whisper-large-v3-turbo on zh-CN: WER=1.0000, Avg Time=0.29s, Samples=2\n",
      "groq_whisper-large-v3-turbo on zh-CN: WER=1.0000, Avg Time=0.29s, Samples=2\n",
      "groq_whisper-large-v3-turbo on zh-CN: WER=1.0000, Avg Time=0.29s, Samples=2\n",
      "groq_whisper-large-v3-turbo on zh-CN: WER=1.0000, Avg Time=0.29s, Samples=2\n",
      "groq_whisper-large-v3-turbo on zh-CN: WER=1.0000, Avg Time=0.29s, Samples=2\n",
      "groq_whisper-large-v3-turbo on zh-CN: WER=1.0000, Avg Time=0.29s, Samples=2\n",
      "elevenlabs_scribe_v1 on zh-CN: WER=1.0000, Avg Time=0.72s, Samples=2\n",
      "elevenlabs_scribe_v1 on zh-CN: WER=1.0000, Avg Time=0.72s, Samples=2\n",
      "elevenlabs_scribe_v1 on zh-CN: WER=1.0000, Avg Time=0.72s, Samples=2\n",
      "elevenlabs_scribe_v1 on zh-CN: WER=1.0000, Avg Time=0.72s, Samples=2\n",
      "elevenlabs_scribe_v1 on zh-CN: WER=1.0000, Avg Time=0.72s, Samples=2\n",
      "elevenlabs_scribe_v1 on zh-CN: WER=1.0000, Avg Time=0.72s, Samples=2\n",
      "openai_gpt-4o-transcribe on zh-CN: WER=1.0000, Avg Time=1.27s, Samples=2\n",
      "openai_gpt-4o-transcribe on zh-CN: WER=1.0000, Avg Time=1.27s, Samples=2\n",
      "openai_gpt-4o-transcribe on zh-CN: WER=1.0000, Avg Time=1.27s, Samples=2\n",
      "openai_gpt-4o-transcribe on zh-CN: WER=1.0000, Avg Time=1.27s, Samples=2\n",
      "openai_gpt-4o-transcribe on zh-CN: WER=1.0000, Avg Time=1.27s, Samples=2\n",
      "openai_gpt-4o-transcribe on zh-CN: WER=1.0000, Avg Time=1.27s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on zh-CN: WER=0.5000, Avg Time=0.93s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on zh-CN: WER=0.5000, Avg Time=0.93s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on zh-CN: WER=0.5000, Avg Time=0.93s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on zh-CN: WER=0.5000, Avg Time=0.93s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on zh-CN: WER=0.5000, Avg Time=0.93s, Samples=2\n",
      "openai_gpt-4o-mini-transcribe on zh-CN: WER=0.5000, Avg Time=0.93s, Samples=2\n",
      "openai_whisper-1 on zh-CN: WER=1.0000, Avg Time=1.46s, Samples=2\n",
      "openai_whisper-1 on zh-CN: WER=1.0000, Avg Time=1.46s, Samples=2\n",
      "openai_whisper-1 on zh-CN: WER=1.0000, Avg Time=1.46s, Samples=2\n",
      "openai_whisper-1 on zh-CN: WER=1.0000, Avg Time=1.46s, Samples=2\n",
      "openai_whisper-1 on zh-CN: WER=1.0000, Avg Time=1.46s, Samples=2\n",
      "openai_whisper-1 on zh-CN: WER=1.0000, Avg Time=1.46s, Samples=2\n",
      "groq_whisper-large-v3 on zh-CN: WER=1.0000, Avg Time=0.31s, Samples=2\n",
      "groq_whisper-large-v3 on zh-CN: WER=1.0000, Avg Time=0.31s, Samples=2\n",
      "groq_whisper-large-v3 on zh-CN: WER=1.0000, Avg Time=0.31s, Samples=2\n",
      "groq_whisper-large-v3 on zh-CN: WER=1.0000, Avg Time=0.31s, Samples=2\n",
      "groq_whisper-large-v3 on zh-CN: WER=1.0000, Avg Time=0.31s, Samples=2\n",
      "groq_whisper-large-v3 on zh-CN: WER=1.0000, Avg Time=0.31s, Samples=2\n",
      "groq_whisper-large-v3-turbo on zh-CN: WER=1.0000, Avg Time=0.29s, Samples=2\n",
      "groq_whisper-large-v3-turbo on zh-CN: WER=1.0000, Avg Time=0.29s, Samples=2\n",
      "groq_whisper-large-v3-turbo on zh-CN: WER=1.0000, Avg Time=0.29s, Samples=2\n",
      "groq_whisper-large-v3-turbo on zh-CN: WER=1.0000, Avg Time=0.29s, Samples=2\n",
      "groq_whisper-large-v3-turbo on zh-CN: WER=1.0000, Avg Time=0.29s, Samples=2\n",
      "groq_whisper-large-v3-turbo on zh-CN: WER=1.0000, Avg Time=0.29s, Samples=2\n",
      "elevenlabs_scribe_v1 on zh-CN: WER=1.0000, Avg Time=0.72s, Samples=2\n",
      "elevenlabs_scribe_v1 on zh-CN: WER=1.0000, Avg Time=0.72s, Samples=2\n",
      "elevenlabs_scribe_v1 on zh-CN: WER=1.0000, Avg Time=0.72s, Samples=2\n",
      "elevenlabs_scribe_v1 on zh-CN: WER=1.0000, Avg Time=0.72s, Samples=2\n",
      "elevenlabs_scribe_v1 on zh-CN: WER=1.0000, Avg Time=0.72s, Samples=2\n",
      "elevenlabs_scribe_v1 on zh-CN: WER=1.0000, Avg Time=0.72s, Samples=2\n",
      "speechmatics_en on zh-CN: WER=4.0000, Avg Time=2.62s, Samples=2\n",
      "speechmatics_en on zh-CN: WER=4.0000, Avg Time=2.62s, Samples=2\n",
      "speechmatics_en on zh-CN: WER=4.0000, Avg Time=2.62s, Samples=2\n",
      "speechmatics_en on zh-CN: WER=4.0000, Avg Time=2.62s, Samples=2\n",
      "speechmatics_en on zh-CN: WER=4.0000, Avg Time=2.62s, Samples=2\n",
      "speechmatics_en on zh-CN: WER=4.0000, Avg Time=2.62s, Samples=2\n",
      "gladia_pre-recorded on zh-CN: WER=1.0000, Avg Time=6.31s, Samples=2\n",
      "gladia_pre-recorded on zh-CN: WER=1.0000, Avg Time=6.31s, Samples=2\n",
      "gladia_pre-recorded on zh-CN: WER=1.0000, Avg Time=6.31s, Samples=2\n",
      "gladia_pre-recorded on zh-CN: WER=1.0000, Avg Time=6.31s, Samples=2\n",
      "gladia_pre-recorded on zh-CN: WER=1.0000, Avg Time=6.31s, Samples=2\n",
      "gladia_pre-recorded on zh-CN: WER=1.0000, Avg Time=6.31s, Samples=2\n",
      "Evaluation completed\n",
      "Evaluation completed\n",
      "Evaluation completed\n",
      "Evaluation completed\n",
      "speechmatics_en on zh-CN: WER=4.0000, Avg Time=2.62s, Samples=2\n",
      "speechmatics_en on zh-CN: WER=4.0000, Avg Time=2.62s, Samples=2\n",
      "speechmatics_en on zh-CN: WER=4.0000, Avg Time=2.62s, Samples=2\n",
      "speechmatics_en on zh-CN: WER=4.0000, Avg Time=2.62s, Samples=2\n",
      "speechmatics_en on zh-CN: WER=4.0000, Avg Time=2.62s, Samples=2\n",
      "speechmatics_en on zh-CN: WER=4.0000, Avg Time=2.62s, Samples=2\n",
      "gladia_pre-recorded on zh-CN: WER=1.0000, Avg Time=6.31s, Samples=2\n",
      "gladia_pre-recorded on zh-CN: WER=1.0000, Avg Time=6.31s, Samples=2\n",
      "gladia_pre-recorded on zh-CN: WER=1.0000, Avg Time=6.31s, Samples=2\n",
      "gladia_pre-recorded on zh-CN: WER=1.0000, Avg Time=6.31s, Samples=2\n",
      "gladia_pre-recorded on zh-CN: WER=1.0000, Avg Time=6.31s, Samples=2\n",
      "gladia_pre-recorded on zh-CN: WER=1.0000, Avg Time=6.31s, Samples=2\n",
      "Evaluation completed\n",
      "Evaluation completed\n",
      "Evaluation completed\n",
      "Evaluation completed\n",
      "Evaluation completed\n",
      "Evaluation completed\n",
      "Evaluation completed\n",
      "Evaluation completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final results:\n",
      "\n",
      "RU:\n",
      "  openai_gpt-4o-transcribe: WER=0.5000, Avg Time=1.09s, Samples=2\n",
      "  openai_gpt-4o-mini-transcribe: WER=0.3750, Avg Time=1.53s, Samples=2\n",
      "  openai_whisper-1: WER=0.6250, Avg Time=1.46s, Samples=2\n",
      "  groq_whisper-large-v3: WER=0.6250, Avg Time=0.27s, Samples=2\n",
      "  groq_whisper-large-v3-turbo: WER=0.5000, Avg Time=0.26s, Samples=2\n",
      "  elevenlabs_scribe_v1: WER=2.0000, Avg Time=0.76s, Samples=2\n",
      "  speechmatics_en: WER=1.1250, Avg Time=2.26s, Samples=2\n",
      "  gladia_pre-recorded: WER=0.1250, Avg Time=6.35s, Samples=2\n",
      "\n",
      "IT:\n",
      "  openai_gpt-4o-transcribe: WER=0.1053, Avg Time=1.17s, Samples=2\n",
      "  openai_gpt-4o-mini-transcribe: WER=0.1053, Avg Time=0.88s, Samples=2\n",
      "  openai_whisper-1: WER=0.0000, Avg Time=1.82s, Samples=2\n",
      "  groq_whisper-large-v3: WER=0.0000, Avg Time=0.39s, Samples=2\n",
      "  groq_whisper-large-v3-turbo: WER=0.0000, Avg Time=0.27s, Samples=2\n",
      "  elevenlabs_scribe_v1: WER=0.0000, Avg Time=0.81s, Samples=2\n",
      "  speechmatics_en: WER=0.9474, Avg Time=2.67s, Samples=2\n",
      "  gladia_pre-recorded: WER=0.0000, Avg Time=6.43s, Samples=2\n",
      "\n",
      "EN:\n",
      "  openai_gpt-4o-transcribe: WER=0.0588, Avg Time=1.14s, Samples=2\n",
      "  openai_gpt-4o-mini-transcribe: WER=0.0588, Avg Time=0.91s, Samples=2\n",
      "  openai_whisper-1: WER=0.0588, Avg Time=1.23s, Samples=2\n",
      "  groq_whisper-large-v3: WER=0.0588, Avg Time=0.29s, Samples=2\n",
      "  groq_whisper-large-v3-turbo: WER=0.0588, Avg Time=0.31s, Samples=2\n",
      "  elevenlabs_scribe_v1: WER=0.0588, Avg Time=0.73s, Samples=2\n",
      "  speechmatics_en: WER=0.0588, Avg Time=2.99s, Samples=2\n",
      "  gladia_pre-recorded: WER=0.0588, Avg Time=6.38s, Samples=2\n",
      "\n",
      "ES:\n",
      "  openai_gpt-4o-transcribe: WER=0.0000, Avg Time=0.88s, Samples=2\n",
      "  openai_gpt-4o-mini-transcribe: WER=0.0000, Avg Time=0.81s, Samples=2\n",
      "  openai_whisper-1: WER=0.0000, Avg Time=1.48s, Samples=2\n",
      "  groq_whisper-large-v3: WER=0.0000, Avg Time=0.28s, Samples=2\n",
      "  groq_whisper-large-v3-turbo: WER=0.0000, Avg Time=0.28s, Samples=2\n",
      "  elevenlabs_scribe_v1: WER=0.0000, Avg Time=0.78s, Samples=2\n",
      "  speechmatics_en: WER=0.6471, Avg Time=2.58s, Samples=2\n",
      "  gladia_pre-recorded: WER=0.0000, Avg Time=6.50s, Samples=2\n",
      "\n",
      "JA:\n",
      "  openai_gpt-4o-transcribe: WER=0.5000, Avg Time=1.08s, Samples=2\n",
      "  openai_gpt-4o-mini-transcribe: WER=0.5000, Avg Time=1.25s, Samples=2\n",
      "  openai_whisper-1: WER=0.5000, Avg Time=0.89s, Samples=2\n",
      "  groq_whisper-large-v3: WER=0.5000, Avg Time=0.29s, Samples=2\n",
      "  groq_whisper-large-v3-turbo: WER=2.0000, Avg Time=0.26s, Samples=2\n",
      "  elevenlabs_scribe_v1: WER=5.0000, Avg Time=0.75s, Samples=2\n",
      "  speechmatics_en: WER=2.5000, Avg Time=2.31s, Samples=2\n",
      "  gladia_pre-recorded: WER=2.0000, Avg Time=6.33s, Samples=2\n",
      "\n",
      "DE:\n",
      "  openai_gpt-4o-transcribe: WER=0.0000, Avg Time=1.07s, Samples=2\n",
      "  openai_gpt-4o-mini-transcribe: WER=0.0000, Avg Time=1.21s, Samples=2\n",
      "  openai_whisper-1: WER=0.0800, Avg Time=1.10s, Samples=2\n",
      "  groq_whisper-large-v3: WER=0.0400, Avg Time=0.33s, Samples=2\n",
      "  groq_whisper-large-v3-turbo: WER=0.0800, Avg Time=0.29s, Samples=2\n",
      "  elevenlabs_scribe_v1: WER=0.0400, Avg Time=0.89s, Samples=2\n",
      "  speechmatics_en: WER=0.8000, Avg Time=2.62s, Samples=2\n",
      "  gladia_pre-recorded: WER=0.0000, Avg Time=6.23s, Samples=2\n",
      "\n",
      "PL:\n",
      "  openai_gpt-4o-transcribe: WER=0.0000, Avg Time=1.02s, Samples=2\n",
      "  openai_gpt-4o-mini-transcribe: WER=0.0000, Avg Time=1.47s, Samples=2\n",
      "  openai_whisper-1: WER=0.0000, Avg Time=1.95s, Samples=2\n",
      "  groq_whisper-large-v3: WER=0.0000, Avg Time=0.29s, Samples=2\n",
      "  groq_whisper-large-v3-turbo: WER=0.0000, Avg Time=0.30s, Samples=2\n",
      "  elevenlabs_scribe_v1: WER=0.0000, Avg Time=0.88s, Samples=2\n",
      "  speechmatics_en: WER=1.0000, Avg Time=3.03s, Samples=2\n",
      "  gladia_pre-recorded: WER=0.0000, Avg Time=6.28s, Samples=2\n",
      "\n",
      "PT:\n",
      "  openai_gpt-4o-transcribe: WER=0.1667, Avg Time=1.34s, Samples=2\n",
      "  openai_gpt-4o-mini-transcribe: WER=0.1667, Avg Time=1.04s, Samples=2\n",
      "  openai_whisper-1: WER=0.1667, Avg Time=0.97s, Samples=2\n",
      "  groq_whisper-large-v3: WER=0.1667, Avg Time=0.29s, Samples=2\n",
      "  groq_whisper-large-v3-turbo: WER=0.1667, Avg Time=0.25s, Samples=2\n",
      "  elevenlabs_scribe_v1: WER=0.0833, Avg Time=0.75s, Samples=2\n",
      "  speechmatics_en: WER=1.0000, Avg Time=2.35s, Samples=2\n",
      "  gladia_pre-recorded: WER=0.1667, Avg Time=6.11s, Samples=2\n",
      "\n",
      "FR:\n",
      "  openai_gpt-4o-transcribe: WER=0.0000, Avg Time=1.83s, Samples=2\n",
      "  openai_gpt-4o-mini-transcribe: WER=0.1176, Avg Time=1.11s, Samples=2\n",
      "  openai_whisper-1: WER=0.0588, Avg Time=1.31s, Samples=2\n",
      "  groq_whisper-large-v3: WER=0.1176, Avg Time=0.38s, Samples=2\n",
      "  groq_whisper-large-v3-turbo: WER=0.1176, Avg Time=0.32s, Samples=2\n",
      "  elevenlabs_scribe_v1: WER=0.3529, Avg Time=0.81s, Samples=2\n",
      "  speechmatics_en: WER=0.8235, Avg Time=3.12s, Samples=2\n",
      "  gladia_pre-recorded: WER=0.1176, Avg Time=7.84s, Samples=2\n",
      "\n",
      "ZH-CN:\n",
      "  openai_gpt-4o-transcribe: WER=1.0000, Avg Time=1.27s, Samples=2\n",
      "  openai_gpt-4o-mini-transcribe: WER=0.5000, Avg Time=0.93s, Samples=2\n",
      "  openai_whisper-1: WER=1.0000, Avg Time=1.46s, Samples=2\n",
      "  groq_whisper-large-v3: WER=1.0000, Avg Time=0.31s, Samples=2\n",
      "  groq_whisper-large-v3-turbo: WER=1.0000, Avg Time=0.29s, Samples=2\n",
      "  elevenlabs_scribe_v1: WER=1.0000, Avg Time=0.72s, Samples=2\n",
      "  speechmatics_en: WER=4.0000, Avg Time=2.62s, Samples=2\n",
      "  gladia_pre-recorded: WER=1.0000, Avg Time=6.31s, Samples=2\n"
     ]
    }
   ],
   "source": [
    "# Run evaluation for ALL languages, 2 samples each, all services (PARALLEL VERSION)\n",
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Get all available languages\n",
    "cv_dataset = CommonVoiceDataset(os.getenv(\"CV22_PATH\"))\n",
    "all_languages = cv_dataset.languages\n",
    "print(f\"Found {len(all_languages)} languages: {all_languages}\")\n",
    "\n",
    "print(f\"Number of services: {len(SERVICES)}\")\n",
    "print(f\"Services: {SERVICES}\")\n",
    "\n",
    "results = run_wer_evaluation_parallel(\n",
    "    dataset_path=os.getenv(\"CV22_PATH\"),\n",
    "    services=SERVICES,  # All services\n",
    "    service_funcs=SERVICE_FUNCS,\n",
    "    languages=all_languages,  # All languages\n",
    "    results_file=f\"results/wer_results_all_{timestamp}.json\",\n",
    "    checkpoint_file=f\"results/wer_checkpoint_all_{timestamp}.json\",\n",
    "    log_file=f\"logs/wer_eval_all_{timestamp}.log\",\n",
    "    max_retries=2,  # Fewer retries for testing\n",
    "    n_samples=2,  # 2 samples per language\n",
    "    max_workers=8,  # Run up to 8 services in parallel per sample\n",
    ")\n",
    "\n",
    "print(\"\\nFinal results:\")\n",
    "for lang, data in results.items():\n",
    "    print(f\"\\n{lang.upper()}:\")\n",
    "    for service, metrics in data.items():\n",
    "        print(\n",
    "            f\"  {service}: WER={metrics['wer']:.4f}, Avg Time={metrics['timing']:.2f}s, Samples={metrics['n_samples']}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "01c60ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing text normalizer:\n",
      "Original: He was thinking about omens, and someone had appeared.\n",
      "Normalized: he was thinking about omens and someone had appeared (CHANGED)\n",
      "\n",
      "Original: Almost all species have some known economic value.\n",
      "Normalized: almost all species have some known economic value (CHANGED)\n",
      "\n",
      "Original: The fruit of a fig tree is apple shaped.\n",
      "Normalized: the fruit of a fig tree is apple shaped (CHANGED)\n",
      "\n",
      "Original: A knockout on all levels.\n",
      "Normalized: a knockout on all levels (CHANGED)\n",
      "\n",
      "Original: This applies to adding new scripture.\n",
      "Normalized: this applies to adding new scripture (CHANGED)\n",
      "\n",
      "Original: Reading or not, it's to you, really.\n",
      "Normalized: reading or not it is to you really (CHANGED)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the text normalizer\n",
    "test_texts = [\n",
    "    \"He was thinking about omens, and someone had appeared.\",\n",
    "    \"Almost all species have some known economic value.\",\n",
    "    \"The fruit of a fig tree is apple shaped.\",\n",
    "    \"A knockout on all levels.\",\n",
    "    \"This applies to adding new scripture.\",\n",
    "    \"Reading or not, it's to you, really.\",\n",
    "]\n",
    "\n",
    "print(\"Testing text normalizer:\")\n",
    "for text in test_texts:\n",
    "    normalized = text_normalizer(text)\n",
    "    changed = \" (CHANGED)\" if normalized != text.lower() else \"\"\n",
    "    print(f\"Original: {text}\")\n",
    "    print(f\"Normalized: {normalized}{changed}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8f2f5999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing language-specific normalizers:\n",
      "EN: He was thinking about omens, and someone had appeared.\n",
      "       -> he was thinking about omens and someone had appeared\n",
      "\n",
      "ES: Él estaba pensando en augurios, y alguien había aparecido.\n",
      "       -> el estaba pensando en augurios y alguien habia aparecido\n",
      "\n",
      "FR: Il pensait aux présages, et quelqu'un était apparu.\n",
      "       -> il pensait aux presages et quelqu un etait apparu\n",
      "\n",
      "DE: Er dachte an Omen, und jemand war erschienen.\n",
      "       -> er dachte an omen und jemand war erschienen\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test language-specific normalizers\n",
    "test_texts = {\n",
    "    \"en\": \"He was thinking about omens, and someone had appeared.\",\n",
    "    \"es\": \"Él estaba pensando en augurios, y alguien había aparecido.\",\n",
    "    \"fr\": \"Il pensait aux présages, et quelqu'un était apparu.\",\n",
    "    \"de\": \"Er dachte an Omen, und jemand war erschienen.\",\n",
    "}\n",
    "\n",
    "print(\"Testing language-specific normalizers:\")\n",
    "for lang, text in test_texts.items():\n",
    "    normalizer = get_text_normalizer(lang)\n",
    "    normalized = normalizer(text)\n",
    "    print(f\"{lang.upper()}: {text}\")\n",
    "    print(f\"       -> {normalized}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2186e433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global logging suppression configured\n"
     ]
    }
   ],
   "source": [
    "# Global logging suppression for ASR services\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# Set logging levels to ERROR for all HTTP-related libraries before any imports\n",
    "logging.getLogger(\"httpx\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"urllib3\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"requests\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"openai\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"groq\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"elevenlabs\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"speechmatics\").setLevel(logging.ERROR)\n",
    "\n",
    "# Also disable propagation for these loggers to prevent duplicate messages\n",
    "for logger_name in [\"httpx\", \"urllib3\", \"requests\", \"openai\", \"groq\", \"elevenlabs\", \"speechmatics\"]:\n",
    "    logger = logging.getLogger(logger_name)\n",
    "    logger.propagate = False\n",
    "\n",
    "print(\"Global logging suppression configured\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jan-incub-multimodal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
